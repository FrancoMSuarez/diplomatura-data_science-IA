{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diplodatos Kaggle Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import CategoricalNB, GaussianNB, MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from utils import display_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(train_data_fname, test_data_fname):\n",
    "    df_train = pd.read_csv(train_data_fname)\n",
    "    df_train['is_train_set'] = 1\n",
    "    df_test = pd.read_csv(test_data_fname)\n",
    "    df_test['is_train_set'] = 0\n",
    "\n",
    "    # we  get the TripType for the train set. To do that, we group by VisitNumber and\n",
    "    # then we get the max (or min or avg)\n",
    "    y = df_train.groupby([\"VisitNumber\", \"Weekday\"], as_index=False).max().TripType\n",
    "\n",
    "    # we remove the TripType now, and concat training and testing data\n",
    "    # the concat is done so that we have the same columns for both datasets\n",
    "    # after one-hot encoding\n",
    "    df_train = df_train.drop(\"TripType\", axis=1)\n",
    "    df = pd.concat([df_train, df_test])\n",
    "    \n",
    "    # the next three operations are the ones we have just presented in the previous lines\n",
    "    \n",
    "    # drop the columns we won't use (it may be good to use them somehow)\n",
    "    df = df.drop([\"Upc\", \"FinelineNumber\"], axis=1)\n",
    "\n",
    "    # one-hot encoding for the DepartmentDescription\n",
    "    df = pd.get_dummies(df, columns=[\"DepartmentDescription\"], dummy_na=True)\n",
    "\n",
    "    # now we add the groupby values\n",
    "    df = df.groupby([\"VisitNumber\", \"Weekday\"], as_index=False).sum()\n",
    "    \n",
    "    # finally, we do one-hot encoding for the Weekday\n",
    "    df = pd.get_dummies(df, columns=[\"Weekday\"], dummy_na=True)\n",
    "\n",
    "    # get train and test back\n",
    "    df_train = df[df.is_train_set != 0]\n",
    "    df_test = df[df.is_train_set == 0]\n",
    "    \n",
    "    X = df_train.drop([\"is_train_set\"], axis=1)\n",
    "    yy = None\n",
    "    XX = df_test.drop([\"is_train_set\"], axis=1)\n",
    "\n",
    "    return X, y, XX, yy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, XX, yy = transform_data(\"./data/train.csv\", \"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VisitNumber</th>\n",
       "      <th>ScanCount</th>\n",
       "      <th>DepartmentDescription_1-HR PHOTO</th>\n",
       "      <th>DepartmentDescription_ACCESSORIES</th>\n",
       "      <th>DepartmentDescription_AUTOMOTIVE</th>\n",
       "      <th>DepartmentDescription_BAKERY</th>\n",
       "      <th>DepartmentDescription_BATH AND SHOWER</th>\n",
       "      <th>DepartmentDescription_BEAUTY</th>\n",
       "      <th>DepartmentDescription_BEDDING</th>\n",
       "      <th>DepartmentDescription_BOOKS AND MAGAZINES</th>\n",
       "      <th>...</th>\n",
       "      <th>DepartmentDescription_WIRELESS</th>\n",
       "      <th>DepartmentDescription_nan</th>\n",
       "      <th>Weekday_Friday</th>\n",
       "      <th>Weekday_Monday</th>\n",
       "      <th>Weekday_Saturday</th>\n",
       "      <th>Weekday_Sunday</th>\n",
       "      <th>Weekday_Thursday</th>\n",
       "      <th>Weekday_Tuesday</th>\n",
       "      <th>Weekday_Wednesday</th>\n",
       "      <th>Weekday_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81954</th>\n",
       "      <td>163907</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32578</th>\n",
       "      <td>65166</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86578</th>\n",
       "      <td>173052</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85079</th>\n",
       "      <td>170137</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9788</th>\n",
       "      <td>19404</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52950</th>\n",
       "      <td>106158</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8809</th>\n",
       "      <td>17442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78302</th>\n",
       "      <td>156542</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>2404</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22577</th>\n",
       "      <td>45320</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46920 rows Ã— 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       VisitNumber  ScanCount  DepartmentDescription_1-HR PHOTO  \\\n",
       "81954       163907          2                                 0   \n",
       "32578        65166          1                                 0   \n",
       "86578       173052          5                                 0   \n",
       "85079       170137          7                                 0   \n",
       "9788         19404         32                                 0   \n",
       "...            ...        ...                               ...   \n",
       "52950       106158          1                                 0   \n",
       "8809         17442          1                                 0   \n",
       "78302       156542          5                                 0   \n",
       "1229          2404          8                                 0   \n",
       "22577        45320         19                                 0   \n",
       "\n",
       "       DepartmentDescription_ACCESSORIES  DepartmentDescription_AUTOMOTIVE  \\\n",
       "81954                                  0                                 0   \n",
       "32578                                  0                                 0   \n",
       "86578                                  0                                 0   \n",
       "85079                                  0                                 0   \n",
       "9788                                   0                                 0   \n",
       "...                                  ...                               ...   \n",
       "52950                                  0                                 0   \n",
       "8809                                   0                                 0   \n",
       "78302                                  0                                 0   \n",
       "1229                                   0                                 0   \n",
       "22577                                  0                                 0   \n",
       "\n",
       "       DepartmentDescription_BAKERY  DepartmentDescription_BATH AND SHOWER  \\\n",
       "81954                             0                                      0   \n",
       "32578                             0                                      0   \n",
       "86578                             0                                      0   \n",
       "85079                             0                                      0   \n",
       "9788                              0                                      0   \n",
       "...                             ...                                    ...   \n",
       "52950                             0                                      0   \n",
       "8809                              0                                      0   \n",
       "78302                             0                                      0   \n",
       "1229                              0                                      0   \n",
       "22577                             0                                      0   \n",
       "\n",
       "       DepartmentDescription_BEAUTY  DepartmentDescription_BEDDING  \\\n",
       "81954                             0                              0   \n",
       "32578                             0                              0   \n",
       "86578                             0                              0   \n",
       "85079                             0                              0   \n",
       "9788                              0                              0   \n",
       "...                             ...                            ...   \n",
       "52950                             0                              0   \n",
       "8809                              0                              0   \n",
       "78302                             0                              0   \n",
       "1229                              0                              0   \n",
       "22577                             0                              0   \n",
       "\n",
       "       DepartmentDescription_BOOKS AND MAGAZINES  ...  \\\n",
       "81954                                          0  ...   \n",
       "32578                                          0  ...   \n",
       "86578                                          0  ...   \n",
       "85079                                          0  ...   \n",
       "9788                                           0  ...   \n",
       "...                                          ...  ...   \n",
       "52950                                          0  ...   \n",
       "8809                                           0  ...   \n",
       "78302                                          0  ...   \n",
       "1229                                           0  ...   \n",
       "22577                                          0  ...   \n",
       "\n",
       "       DepartmentDescription_WIRELESS  DepartmentDescription_nan  \\\n",
       "81954                               0                          0   \n",
       "32578                               0                          0   \n",
       "86578                               0                          0   \n",
       "85079                               0                          0   \n",
       "9788                                0                          0   \n",
       "...                               ...                        ...   \n",
       "52950                               0                          0   \n",
       "8809                                0                          0   \n",
       "78302                               0                          0   \n",
       "1229                                0                          0   \n",
       "22577                               0                          0   \n",
       "\n",
       "       Weekday_Friday  Weekday_Monday  Weekday_Saturday  Weekday_Sunday  \\\n",
       "81954               0               0                 0               0   \n",
       "32578               0               1                 0               0   \n",
       "86578               1               0                 0               0   \n",
       "85079               0               0                 0               0   \n",
       "9788                0               0                 0               1   \n",
       "...               ...             ...               ...             ...   \n",
       "52950               0               0                 0               1   \n",
       "8809                0               0                 0               1   \n",
       "78302               0               0                 0               0   \n",
       "1229                1               0                 0               0   \n",
       "22577               1               0                 0               0   \n",
       "\n",
       "       Weekday_Thursday  Weekday_Tuesday  Weekday_Wednesday  Weekday_nan  \n",
       "81954                 0                0                  1            0  \n",
       "32578                 0                0                  0            0  \n",
       "86578                 0                0                  0            0  \n",
       "85079                 1                0                  0            0  \n",
       "9788                  0                0                  0            0  \n",
       "...                 ...              ...                ...          ...  \n",
       "52950                 0                0                  0            0  \n",
       "8809                  0                0                  0            0  \n",
       "78302                 0                1                  0            0  \n",
       "1229                  0                0                  0            0  \n",
       "22577                 0                0                  0            0  \n",
       "\n",
       "[46920 rows x 79 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split training dataset into train and \"validation\" \n",
    "# (we won't be using validation set in this example, because of the cross-validation;\n",
    "# but it could be useful for you depending on your approach)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results dataframe is used to store the computed results\n",
    "results = pd.DataFrame(columns=('clf', 'best_acc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.28674052\n",
      "Iteration 2, loss = 1.31093222\n",
      "Iteration 3, loss = 1.15790133\n",
      "Iteration 4, loss = 1.09663572\n",
      "Iteration 5, loss = 1.05953026\n",
      "Iteration 6, loss = 1.03445582\n",
      "Iteration 7, loss = 1.01706251\n",
      "Iteration 8, loss = 1.00225217\n",
      "Iteration 9, loss = 0.98997892\n",
      "Iteration 10, loss = 0.97956292\n",
      "Iteration 11, loss = 0.97005799\n",
      "Iteration 12, loss = 0.96265076\n",
      "Iteration 13, loss = 0.95459246\n",
      "Iteration 14, loss = 0.94900034\n",
      "Iteration 15, loss = 0.94254913\n",
      "Iteration 16, loss = 0.93711596\n",
      "Iteration 17, loss = 0.93326171\n",
      "Iteration 18, loss = 0.92768156\n",
      "Iteration 19, loss = 0.92412697\n",
      "Iteration 20, loss = 0.92019768\n",
      "Iteration 21, loss = 0.91722915\n",
      "Iteration 22, loss = 0.91399503\n",
      "Iteration 23, loss = 0.91063932\n",
      "Iteration 24, loss = 0.90832256\n",
      "Iteration 25, loss = 0.90624154\n",
      "Iteration 26, loss = 0.90331874\n",
      "Iteration 27, loss = 0.90168567\n",
      "Iteration 28, loss = 0.89933657\n",
      "Iteration 29, loss = 0.89743939\n",
      "Iteration 30, loss = 0.89601150\n",
      "Iteration 31, loss = 0.89432808\n",
      "Iteration 32, loss = 0.89211607\n",
      "Iteration 33, loss = 0.89175962\n",
      "Iteration 34, loss = 0.88886193\n",
      "Iteration 35, loss = 0.88730078\n",
      "Iteration 36, loss = 0.88596334\n",
      "Iteration 37, loss = 0.88492523\n",
      "Iteration 38, loss = 0.88367627\n",
      "Iteration 39, loss = 0.88252233\n",
      "Iteration 40, loss = 0.88032590\n",
      "Iteration 41, loss = 0.88000584\n",
      "Iteration 42, loss = 0.87846831\n",
      "Iteration 43, loss = 0.87780911\n",
      "Iteration 44, loss = 0.87598118\n",
      "Iteration 45, loss = 0.87518815\n",
      "Iteration 46, loss = 0.87445008\n",
      "Iteration 47, loss = 0.87381019\n",
      "Iteration 48, loss = 0.87265605\n",
      "Iteration 49, loss = 0.87106247\n",
      "Iteration 50, loss = 0.87058126\n",
      "Iteration 51, loss = 0.86980524\n",
      "Iteration 52, loss = 0.86847901\n",
      "Iteration 53, loss = 0.86838454\n",
      "Iteration 54, loss = 0.86775322\n",
      "Iteration 55, loss = 0.86675752\n",
      "Iteration 56, loss = 0.86582220\n",
      "Iteration 57, loss = 0.86489709\n",
      "Iteration 58, loss = 0.86401121\n",
      "Iteration 59, loss = 0.86326798\n",
      "Iteration 60, loss = 0.86266679\n",
      "Iteration 61, loss = 0.86211152\n",
      "Iteration 62, loss = 0.86187062\n",
      "Iteration 63, loss = 0.86068708\n",
      "Iteration 64, loss = 0.85996338\n",
      "Iteration 65, loss = 0.85969344\n",
      "Iteration 66, loss = 0.85867782\n",
      "Iteration 67, loss = 0.85827014\n",
      "Iteration 68, loss = 0.85835977\n",
      "Iteration 69, loss = 0.85722387\n",
      "Iteration 70, loss = 0.85655597\n",
      "Iteration 71, loss = 0.85639630\n",
      "Iteration 72, loss = 0.85600770\n",
      "Iteration 73, loss = 0.85564247\n",
      "Iteration 74, loss = 0.85501218\n",
      "Iteration 75, loss = 0.85345673\n",
      "Iteration 76, loss = 0.85288911\n",
      "Iteration 77, loss = 0.85219082\n",
      "Iteration 78, loss = 0.85287460\n",
      "Iteration 79, loss = 0.85221575\n",
      "Iteration 80, loss = 0.85156543\n",
      "Iteration 81, loss = 0.85116230\n",
      "Iteration 82, loss = 0.85042841\n",
      "Iteration 83, loss = 0.85026141\n",
      "Iteration 84, loss = 0.84952022\n",
      "Iteration 85, loss = 0.84932445\n",
      "Iteration 86, loss = 0.84922138\n",
      "Iteration 87, loss = 0.84873487\n",
      "Iteration 88, loss = 0.84800806\n",
      "Iteration 89, loss = 0.84733014\n",
      "Iteration 90, loss = 0.84641063\n",
      "Iteration 91, loss = 0.84720434\n",
      "Iteration 92, loss = 0.84683943\n",
      "Iteration 93, loss = 0.84575519\n",
      "Iteration 94, loss = 0.84645080\n",
      "Iteration 95, loss = 0.84579449\n",
      "Iteration 96, loss = 0.84502050\n",
      "Iteration 97, loss = 0.84418950\n",
      "Iteration 98, loss = 0.84476361\n",
      "Iteration 99, loss = 0.84397264\n",
      "Iteration 100, loss = 0.84341139\n",
      "Iteration 101, loss = 0.84406270\n",
      "Iteration 102, loss = 0.84368467\n",
      "Iteration 103, loss = 0.84304353\n",
      "Iteration 104, loss = 0.84252718\n",
      "Iteration 105, loss = 0.84218069\n",
      "Iteration 106, loss = 0.84153000\n",
      "Iteration 107, loss = 0.84158955\n",
      "Iteration 108, loss = 0.84079139\n",
      "Iteration 109, loss = 0.84200396\n",
      "Iteration 110, loss = 0.84045444\n",
      "Iteration 111, loss = 0.84022048\n",
      "Iteration 112, loss = 0.83932002\n",
      "Iteration 113, loss = 0.83969449\n",
      "Iteration 114, loss = 0.83956234\n",
      "Iteration 115, loss = 0.83979869\n",
      "Iteration 116, loss = 0.83875116\n",
      "Iteration 117, loss = 0.83887916\n",
      "Iteration 118, loss = 0.83838789\n",
      "Iteration 119, loss = 0.83822849\n",
      "Iteration 120, loss = 0.83736162\n",
      "Iteration 121, loss = 0.83744945\n",
      "Iteration 122, loss = 0.83766109\n",
      "Iteration 123, loss = 0.83582619\n",
      "Iteration 124, loss = 0.83698282\n",
      "Iteration 125, loss = 0.83658176\n",
      "Iteration 126, loss = 0.83576986\n",
      "Iteration 127, loss = 0.83572987\n",
      "Iteration 128, loss = 0.83574961\n",
      "Iteration 129, loss = 0.83509604\n",
      "Iteration 130, loss = 0.83497293\n",
      "Iteration 131, loss = 0.83531862\n",
      "Iteration 132, loss = 0.83518918\n",
      "Iteration 133, loss = 0.83474340\n",
      "Iteration 134, loss = 0.83466105\n",
      "Iteration 135, loss = 0.83349415\n",
      "Iteration 136, loss = 0.83379815\n",
      "Iteration 137, loss = 0.83442883\n",
      "Iteration 138, loss = 0.83380746\n",
      "Iteration 139, loss = 0.83309223\n",
      "Iteration 140, loss = 0.83279248\n",
      "Iteration 141, loss = 0.83304159\n",
      "Iteration 142, loss = 0.83273346\n",
      "Iteration 143, loss = 0.83299592\n",
      "Iteration 144, loss = 0.83166723\n",
      "Iteration 145, loss = 0.83243734\n",
      "Iteration 146, loss = 0.83187869\n",
      "Iteration 147, loss = 0.83068519\n",
      "Iteration 148, loss = 0.83133341\n",
      "Iteration 149, loss = 0.83002379\n",
      "Iteration 150, loss = 0.83035694\n",
      "Iteration 151, loss = 0.83020087\n",
      "Iteration 152, loss = 0.83017605\n",
      "Iteration 153, loss = 0.83028689\n",
      "Iteration 154, loss = 0.82991235\n",
      "Iteration 155, loss = 0.82991297\n",
      "Iteration 156, loss = 0.82928859\n",
      "Iteration 157, loss = 0.82926566\n",
      "Iteration 158, loss = 0.82947647\n",
      "Iteration 159, loss = 0.82870436\n",
      "Iteration 160, loss = 0.82826341\n",
      "Iteration 161, loss = 0.82848908\n",
      "Iteration 162, loss = 0.82799118\n",
      "Iteration 163, loss = 0.82895719\n",
      "Iteration 164, loss = 0.82804278\n",
      "Iteration 165, loss = 0.82841128\n",
      "Iteration 166, loss = 0.82662139\n",
      "Iteration 167, loss = 0.82749549\n",
      "Iteration 168, loss = 0.82756644\n",
      "Iteration 169, loss = 0.82748889\n",
      "Iteration 170, loss = 0.82724991\n",
      "Iteration 171, loss = 0.82773686\n",
      "Iteration 172, loss = 0.82645995\n",
      "Iteration 173, loss = 0.82708502\n",
      "Iteration 174, loss = 0.82593208\n",
      "Iteration 175, loss = 0.82557891\n",
      "Iteration 176, loss = 0.82600623\n",
      "Iteration 177, loss = 0.82561031\n",
      "Iteration 178, loss = 0.82547358\n",
      "Iteration 179, loss = 0.82589943\n",
      "Iteration 180, loss = 0.82586931\n",
      "Iteration 181, loss = 0.82580051\n",
      "Iteration 182, loss = 0.82533956\n",
      "Iteration 183, loss = 0.82461239\n",
      "Iteration 184, loss = 0.82309384\n",
      "Iteration 185, loss = 0.82436720\n",
      "Iteration 186, loss = 0.82352138\n",
      "Iteration 187, loss = 0.82438923\n",
      "Iteration 188, loss = 0.82411222\n",
      "Iteration 189, loss = 0.82373279\n",
      "Iteration 190, loss = 0.82389414\n",
      "Iteration 191, loss = 0.82362389\n",
      "Iteration 192, loss = 0.82376352\n",
      "Iteration 193, loss = 0.82241172\n",
      "Iteration 194, loss = 0.82242853\n",
      "Iteration 195, loss = 0.82263976\n",
      "Iteration 196, loss = 0.82266481\n",
      "Iteration 197, loss = 0.82269948\n",
      "Iteration 198, loss = 0.82266409\n",
      "Iteration 199, loss = 0.82203791\n",
      "Iteration 200, loss = 0.82264645\n",
      "Iteration 201, loss = 0.82103177\n",
      "Iteration 202, loss = 0.82249538\n",
      "Iteration 203, loss = 0.82129292\n",
      "Iteration 204, loss = 0.82175302\n",
      "Iteration 205, loss = 0.82130527\n",
      "Iteration 206, loss = 0.82038640\n",
      "Iteration 207, loss = 0.82079571\n",
      "Iteration 208, loss = 0.82072542\n",
      "Iteration 209, loss = 0.82105667\n",
      "Iteration 210, loss = 0.82026170\n",
      "Iteration 211, loss = 0.82067957\n",
      "Iteration 212, loss = 0.81926385\n",
      "Iteration 213, loss = 0.82031804\n",
      "Iteration 214, loss = 0.82023113\n",
      "Iteration 215, loss = 0.82043442\n",
      "Iteration 216, loss = 0.81904694\n",
      "Iteration 217, loss = 0.81984987\n",
      "Iteration 218, loss = 0.81991343\n",
      "Iteration 219, loss = 0.81898055\n",
      "Iteration 220, loss = 0.81967993\n",
      "Iteration 221, loss = 0.81867397\n",
      "Iteration 222, loss = 0.81890681\n",
      "Iteration 223, loss = 0.81894516\n",
      "Iteration 224, loss = 0.81844741\n",
      "Iteration 225, loss = 0.81989899\n",
      "Iteration 226, loss = 0.81916075\n",
      "Iteration 227, loss = 0.81810257\n",
      "Iteration 228, loss = 0.81882282\n",
      "Iteration 229, loss = 0.81790080\n",
      "Iteration 230, loss = 0.81733695\n",
      "Iteration 231, loss = 0.81710761\n",
      "Iteration 232, loss = 0.81812758\n",
      "Iteration 233, loss = 0.81694238\n",
      "Iteration 234, loss = 0.81752355\n",
      "Iteration 235, loss = 0.81694520\n",
      "Iteration 236, loss = 0.81777146\n",
      "Iteration 237, loss = 0.81705734\n",
      "Iteration 238, loss = 0.81683760\n",
      "Iteration 239, loss = 0.81717239\n",
      "Iteration 240, loss = 0.81711205\n",
      "Iteration 241, loss = 0.81663070\n",
      "Iteration 242, loss = 0.81673175\n",
      "Iteration 243, loss = 0.81704742\n",
      "Iteration 244, loss = 0.81571913\n",
      "Iteration 245, loss = 0.81596960\n",
      "Iteration 246, loss = 0.81547159\n",
      "Iteration 247, loss = 0.81550930\n",
      "Iteration 248, loss = 0.81621196\n",
      "Iteration 249, loss = 0.81545484\n",
      "Iteration 250, loss = 0.81622156\n",
      "Iteration 251, loss = 0.81571569\n",
      "Iteration 252, loss = 0.81571463\n",
      "Iteration 253, loss = 0.81568614\n",
      "Iteration 254, loss = 0.81491381\n",
      "Iteration 255, loss = 0.81551391\n",
      "Iteration 256, loss = 0.81536392\n",
      "Iteration 257, loss = 0.81445327\n",
      "Iteration 258, loss = 0.81526686\n",
      "Iteration 259, loss = 0.81449326\n",
      "Iteration 260, loss = 0.81531063\n",
      "Iteration 261, loss = 0.81582935\n",
      "Iteration 262, loss = 0.81404529\n",
      "Iteration 263, loss = 0.81445353\n",
      "Iteration 264, loss = 0.81442386\n",
      "Iteration 265, loss = 0.81421969\n",
      "Iteration 266, loss = 0.81383662\n",
      "Iteration 267, loss = 0.81353825\n",
      "Iteration 268, loss = 0.81305631\n",
      "Iteration 269, loss = 0.81379122\n",
      "Iteration 270, loss = 0.81306601\n",
      "Iteration 271, loss = 0.81420257\n",
      "Iteration 272, loss = 0.81281228\n",
      "Iteration 273, loss = 0.81281045\n",
      "Iteration 274, loss = 0.81338595\n",
      "Iteration 275, loss = 0.81318721\n",
      "Iteration 276, loss = 0.81356929\n",
      "Iteration 277, loss = 0.81391658\n",
      "Iteration 278, loss = 0.81266954\n",
      "Iteration 279, loss = 0.81377497\n",
      "Iteration 280, loss = 0.81272077\n",
      "Iteration 281, loss = 0.81217737\n",
      "Iteration 282, loss = 0.81354664\n",
      "Iteration 283, loss = 0.81226128\n",
      "Iteration 284, loss = 0.81327555\n",
      "Iteration 285, loss = 0.81281392\n",
      "Iteration 286, loss = 0.81323614\n",
      "Iteration 287, loss = 0.81240700\n",
      "Iteration 288, loss = 0.81119628\n",
      "Iteration 289, loss = 0.81186186\n",
      "Iteration 290, loss = 0.81202229\n",
      "Iteration 291, loss = 0.81177988\n",
      "Iteration 292, loss = 0.81125715\n",
      "Iteration 293, loss = 0.81124843\n",
      "Iteration 294, loss = 0.81228934\n",
      "Iteration 295, loss = 0.81167785\n",
      "Iteration 296, loss = 0.81189714\n",
      "Iteration 297, loss = 0.81090395\n",
      "Iteration 298, loss = 0.81146055\n",
      "Iteration 299, loss = 0.81128289\n",
      "Iteration 300, loss = 0.81202858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiergallo/miniconda3/envs/diplodatos/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Original:\n",
    "model_param = {\n",
    "    'criterion': ('gini', 'entropy'),\n",
    "    'min_samples_leaf': (1, 2, 5),\n",
    "    'min_samples_split': (2, 3, 5, 10, 50, 100)\n",
    "}\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Best models so far (from better to worse):\n",
    "\n",
    "# No enviado!\n",
    "MLPClassifier(\n",
    "    alpha=0.03,\n",
    "    beta_1=0.8,\n",
    "    beta_2=0.99999,\n",
    "    hidden_layer_sizes=(96,),\n",
    "    max_iter=300,\n",
    "    n_iter_no_change=30,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "MLPClassifier(\n",
    "    alpha=0.03,\n",
    "    hidden_layer_sizes=(96,),\n",
    "    max_iter=300,\n",
    "    n_iter_no_change=30,\n",
    "    random_state=42\n",
    ")\n",
    "MLPClassifier(\n",
    "    alpha=0.02,\n",
    "    hidden_layer_sizes=(64,),\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=101,\n",
    "    max_depth=59,\n",
    "    class_weight={999: 0.49}\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "model_param = {\n",
    "    'beta_1': (0.8,),\n",
    "    'beta_2': (0.99999,),\n",
    "}\n",
    "model = MLPClassifier(random_state=42,\n",
    "                      verbose=True,\n",
    "                      max_iter=300,\n",
    "                      alpha=0.03,\n",
    "                      n_iter_no_change=30,\n",
    "                      hidden_layer_sizes=(96,))\n",
    "\n",
    "search = GridSearchCV(model,\n",
    "                      model_param,\n",
    "                      cv=3,\n",
    "                      n_jobs=-1,  # Use all processors.\n",
    "                      scoring='accuracy',\n",
    "                      verbose=1)  # scoring='balanced_accuracy'\n",
    "\n",
    "# NOTICE we exclude visit number; it's just an index,\n",
    "# it messes up the training.\n",
    "search.fit(X_train.drop(columns=['VisitNumber']), y_train)\n",
    "\n",
    "best_found_clf = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(best_found_clf.loss_curve_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Best classifier found**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(alpha=0.03, beta_1=0.8, beta_2=0.99999, hidden_layer_sizes=(96,),\n",
      "              max_iter=300, n_iter_no_change=30, random_state=42, verbose=True)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Accuracy**: 0.6916879795396419"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown('**Best classifier found**:')\n",
    "print(best_found_clf)\n",
    "display_markdown('**Accuracy**: {}'.format(search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.append(\n",
    "    {'clf': best_found_clf, 'best_acc': search.best_score_},\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clf</th>\n",
       "      <th>best_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLPClassifier(alpha=0.03, beta_1=0.8, beta_2=0...</td>\n",
       "      <td>0.691688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 clf  best_acc\n",
       "0  MLPClassifier(alpha=0.03, beta_1=0.8, beta_2=0...  0.691688"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**The best classifier so far is:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(alpha=0.03, beta_1=0.8, beta_2=0.99999, hidden_layer_sizes=(96,),\n",
      "              max_iter=300, n_iter_no_change=30, random_state=42, verbose=True)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Accuracy**: 0.6916879795396419"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_clf_idx = results['best_acc'].idxmax()\n",
    "best_clf = results.loc[best_clf_idx]['clf']\n",
    "best_acc = results.loc[best_clf_idx]['best_acc']\n",
    "display_markdown('**The best classifier so far is:**')\n",
    "print(best_clf)\n",
    "display_markdown('**Accuracy**: {}'.format(search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And finally**, we predict the unknown label for the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67029, 79), (28645, 79))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTICE we exclude visit number because we didn't use it for training.\n",
    "yy = best_clf.predict(XX.drop(columns=['VisitNumber']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we do is generating a file that should be *submitted* on kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(list(zip(XX.VisitNumber, yy)), columns=[\"VisitNumber\", \"TripType\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"./data/submission.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
