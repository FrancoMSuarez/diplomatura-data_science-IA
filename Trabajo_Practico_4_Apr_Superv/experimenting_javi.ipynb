{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diplodatos Kaggle Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import CategoricalNB, GaussianNB, MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from utils import display_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(train_data_fname, test_data_fname):\n",
    "    df_train = pd.read_csv(train_data_fname)\n",
    "    df_train['is_train_set'] = 1\n",
    "    df_test = pd.read_csv(test_data_fname)\n",
    "    df_test['is_train_set'] = 0\n",
    "\n",
    "    # we  get the TripType for the train set. To do that, we group by VisitNumber and\n",
    "    # then we get the max (or min or avg)\n",
    "    y = df_train.groupby([\"VisitNumber\", \"Weekday\"], as_index=False).max().TripType\n",
    "\n",
    "    # we remove the TripType now, and concat training and testing data\n",
    "    # the concat is done so that we have the same columns for both datasets\n",
    "    # after one-hot encoding\n",
    "    df_train = df_train.drop(\"TripType\", axis=1)\n",
    "    df = pd.concat([df_train, df_test])\n",
    "    \n",
    "    # the next three operations are the ones we have just presented in the previous lines\n",
    "    \n",
    "    # drop the columns we won't use (it may be good to use them somehow)\n",
    "    df = df.drop([\"Upc\", \"FinelineNumber\"], axis=1)\n",
    "\n",
    "    # one-hot encoding for the DepartmentDescription\n",
    "    df = pd.get_dummies(df, columns=[\"DepartmentDescription\"], dummy_na=True)\n",
    "\n",
    "    # now we add the groupby values\n",
    "    df = df.groupby([\"VisitNumber\", \"Weekday\"], as_index=False).sum()\n",
    "    \n",
    "    # we do one-hot encoding for the Weekday\n",
    "    df = pd.get_dummies(df, columns=[\"Weekday\"], dummy_na=True)\n",
    "\n",
    "    # get train and test back\n",
    "    df_train = df[df.is_train_set != 0]\n",
    "    df_test = df[df.is_train_set == 0]\n",
    "    \n",
    "    X = df_train.drop([\"is_train_set\"], axis=1)\n",
    "    yy = None\n",
    "    XX = df_test.drop([\"is_train_set\"], axis=1)\n",
    "    visit_n_test = XX.VisitNumber\n",
    "\n",
    "    # we exclude visit number; it's just an index, it messes up all the\n",
    "    # training (added by Javi).\n",
    "    X = X.drop(columns=['VisitNumber'])\n",
    "    XX = XX.drop(columns=['VisitNumber'])\n",
    "    \n",
    "    return X, y, visit_n_test, XX, yy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, visit_n_test, XX, yy = transform_data(\"./data/train.csv\", \"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ScanCount</th>\n",
       "      <th>DepartmentDescription_1-HR PHOTO</th>\n",
       "      <th>DepartmentDescription_ACCESSORIES</th>\n",
       "      <th>DepartmentDescription_AUTOMOTIVE</th>\n",
       "      <th>DepartmentDescription_BAKERY</th>\n",
       "      <th>DepartmentDescription_BATH AND SHOWER</th>\n",
       "      <th>DepartmentDescription_BEAUTY</th>\n",
       "      <th>DepartmentDescription_BEDDING</th>\n",
       "      <th>DepartmentDescription_BOOKS AND MAGAZINES</th>\n",
       "      <th>DepartmentDescription_BOYS WEAR</th>\n",
       "      <th>...</th>\n",
       "      <th>DepartmentDescription_WIRELESS</th>\n",
       "      <th>DepartmentDescription_nan</th>\n",
       "      <th>Weekday_Friday</th>\n",
       "      <th>Weekday_Monday</th>\n",
       "      <th>Weekday_Saturday</th>\n",
       "      <th>Weekday_Sunday</th>\n",
       "      <th>Weekday_Thursday</th>\n",
       "      <th>Weekday_Tuesday</th>\n",
       "      <th>Weekday_Wednesday</th>\n",
       "      <th>Weekday_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81954</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32578</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86578</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85079</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9788</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52950</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8809</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78302</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22577</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46920 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ScanCount  DepartmentDescription_1-HR PHOTO  \\\n",
       "81954          2                                 0   \n",
       "32578          1                                 0   \n",
       "86578          5                                 0   \n",
       "85079          7                                 0   \n",
       "9788          32                                 0   \n",
       "...          ...                               ...   \n",
       "52950          1                                 0   \n",
       "8809           1                                 0   \n",
       "78302          5                                 0   \n",
       "1229           8                                 0   \n",
       "22577         19                                 0   \n",
       "\n",
       "       DepartmentDescription_ACCESSORIES  DepartmentDescription_AUTOMOTIVE  \\\n",
       "81954                                  0                                 0   \n",
       "32578                                  0                                 0   \n",
       "86578                                  0                                 0   \n",
       "85079                                  0                                 0   \n",
       "9788                                   0                                 0   \n",
       "...                                  ...                               ...   \n",
       "52950                                  0                                 0   \n",
       "8809                                   0                                 0   \n",
       "78302                                  0                                 0   \n",
       "1229                                   0                                 0   \n",
       "22577                                  0                                 0   \n",
       "\n",
       "       DepartmentDescription_BAKERY  DepartmentDescription_BATH AND SHOWER  \\\n",
       "81954                             0                                      0   \n",
       "32578                             0                                      0   \n",
       "86578                             0                                      0   \n",
       "85079                             0                                      0   \n",
       "9788                              0                                      0   \n",
       "...                             ...                                    ...   \n",
       "52950                             0                                      0   \n",
       "8809                              0                                      0   \n",
       "78302                             0                                      0   \n",
       "1229                              0                                      0   \n",
       "22577                             0                                      0   \n",
       "\n",
       "       DepartmentDescription_BEAUTY  DepartmentDescription_BEDDING  \\\n",
       "81954                             0                              0   \n",
       "32578                             0                              0   \n",
       "86578                             0                              0   \n",
       "85079                             0                              0   \n",
       "9788                              0                              0   \n",
       "...                             ...                            ...   \n",
       "52950                             0                              0   \n",
       "8809                              0                              0   \n",
       "78302                             0                              0   \n",
       "1229                              0                              0   \n",
       "22577                             0                              0   \n",
       "\n",
       "       DepartmentDescription_BOOKS AND MAGAZINES  \\\n",
       "81954                                          0   \n",
       "32578                                          0   \n",
       "86578                                          0   \n",
       "85079                                          0   \n",
       "9788                                           0   \n",
       "...                                          ...   \n",
       "52950                                          0   \n",
       "8809                                           0   \n",
       "78302                                          0   \n",
       "1229                                           0   \n",
       "22577                                          0   \n",
       "\n",
       "       DepartmentDescription_BOYS WEAR  ...  DepartmentDescription_WIRELESS  \\\n",
       "81954                                0  ...                               0   \n",
       "32578                                0  ...                               0   \n",
       "86578                                0  ...                               0   \n",
       "85079                                0  ...                               0   \n",
       "9788                                 0  ...                               0   \n",
       "...                                ...  ...                             ...   \n",
       "52950                                0  ...                               0   \n",
       "8809                                 0  ...                               0   \n",
       "78302                                0  ...                               0   \n",
       "1229                                 0  ...                               0   \n",
       "22577                                0  ...                               0   \n",
       "\n",
       "       DepartmentDescription_nan  Weekday_Friday  Weekday_Monday  \\\n",
       "81954                          0               0               0   \n",
       "32578                          0               0               1   \n",
       "86578                          0               1               0   \n",
       "85079                          0               0               0   \n",
       "9788                           0               0               0   \n",
       "...                          ...             ...             ...   \n",
       "52950                          0               0               0   \n",
       "8809                           0               0               0   \n",
       "78302                          0               0               0   \n",
       "1229                           0               1               0   \n",
       "22577                          0               1               0   \n",
       "\n",
       "       Weekday_Saturday  Weekday_Sunday  Weekday_Thursday  Weekday_Tuesday  \\\n",
       "81954                 0               0                 0                0   \n",
       "32578                 0               0                 0                0   \n",
       "86578                 0               0                 0                0   \n",
       "85079                 0               0                 1                0   \n",
       "9788                  0               1                 0                0   \n",
       "...                 ...             ...               ...              ...   \n",
       "52950                 0               1                 0                0   \n",
       "8809                  0               1                 0                0   \n",
       "78302                 0               0                 0                1   \n",
       "1229                  0               0                 0                0   \n",
       "22577                 0               0                 0                0   \n",
       "\n",
       "       Weekday_Wednesday  Weekday_nan  \n",
       "81954                  1            0  \n",
       "32578                  0            0  \n",
       "86578                  0            0  \n",
       "85079                  0            0  \n",
       "9788                   0            0  \n",
       "...                  ...          ...  \n",
       "52950                  0            0  \n",
       "8809                   0            0  \n",
       "78302                  0            0  \n",
       "1229                   0            0  \n",
       "22577                  0            0  \n",
       "\n",
       "[46920 rows x 78 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split training dataset into train and \"validation\" \n",
    "# (we won't be using validation set in this example, because of the cross-validation;\n",
    "# but it could be useful for you depending on your approach)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results dataframe is used to store the computed results\n",
    "results = pd.DataFrame(columns=('clf', 'best_acc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.61309642\n",
      "Iteration 2, loss = 1.50575373\n",
      "Iteration 3, loss = 1.25339794\n",
      "Iteration 4, loss = 1.15990872\n",
      "Iteration 5, loss = 1.11010052\n",
      "Iteration 6, loss = 1.07714854\n",
      "Iteration 7, loss = 1.05270407\n",
      "Iteration 8, loss = 1.03491592\n",
      "Iteration 9, loss = 1.01832212\n",
      "Iteration 10, loss = 1.00731881\n",
      "Iteration 11, loss = 0.99616158\n",
      "Iteration 12, loss = 0.98578023\n",
      "Iteration 13, loss = 0.97817940\n",
      "Iteration 14, loss = 0.97035809\n",
      "Iteration 15, loss = 0.96392514\n",
      "Iteration 16, loss = 0.95772222\n",
      "Iteration 17, loss = 0.95085115\n",
      "Iteration 18, loss = 0.94399899\n",
      "Iteration 19, loss = 0.93931794\n",
      "Iteration 20, loss = 0.93593474\n",
      "Iteration 21, loss = 0.93126724\n",
      "Iteration 22, loss = 0.92608372\n",
      "Iteration 23, loss = 0.92182880\n",
      "Iteration 24, loss = 0.91840470\n",
      "Iteration 25, loss = 0.91456487\n",
      "Iteration 26, loss = 0.91189997\n",
      "Iteration 27, loss = 0.90911434\n",
      "Iteration 28, loss = 0.90614362\n",
      "Iteration 29, loss = 0.90326771\n",
      "Iteration 30, loss = 0.90064052\n",
      "Iteration 31, loss = 0.89715576\n",
      "Iteration 32, loss = 0.89559766\n",
      "Iteration 33, loss = 0.89303138\n",
      "Iteration 34, loss = 0.89114809\n",
      "Iteration 35, loss = 0.88860846\n",
      "Iteration 36, loss = 0.88661399\n",
      "Iteration 37, loss = 0.88423590\n",
      "Iteration 38, loss = 0.88307226\n",
      "Iteration 39, loss = 0.88079232\n",
      "Iteration 40, loss = 0.88005896\n",
      "Iteration 41, loss = 0.87648079\n",
      "Iteration 42, loss = 0.87634646\n",
      "Iteration 43, loss = 0.87362388\n",
      "Iteration 44, loss = 0.87230134\n",
      "Iteration 45, loss = 0.87062037\n",
      "Iteration 46, loss = 0.86950805\n",
      "Iteration 47, loss = 0.86823198\n",
      "Iteration 48, loss = 0.86665808\n",
      "Iteration 49, loss = 0.86465595\n",
      "Iteration 50, loss = 0.86485484\n",
      "Iteration 51, loss = 0.86263523\n",
      "Iteration 52, loss = 0.86173816\n",
      "Iteration 53, loss = 0.86006457\n",
      "Iteration 54, loss = 0.85861458\n",
      "Iteration 55, loss = 0.85780266\n",
      "Iteration 56, loss = 0.85699930\n",
      "Iteration 57, loss = 0.85500645\n",
      "Iteration 58, loss = 0.85454661\n",
      "Iteration 59, loss = 0.85301926\n",
      "Iteration 60, loss = 0.85378169\n",
      "Iteration 61, loss = 0.85108934\n",
      "Iteration 62, loss = 0.84974116\n",
      "Iteration 63, loss = 0.84994796\n",
      "Iteration 64, loss = 0.84888135\n",
      "Iteration 65, loss = 0.84729588\n",
      "Iteration 66, loss = 0.84682583\n",
      "Iteration 67, loss = 0.84558625\n",
      "Iteration 68, loss = 0.84514197\n",
      "Iteration 69, loss = 0.84313839\n",
      "Iteration 70, loss = 0.84264860\n",
      "Iteration 71, loss = 0.84228219\n",
      "Iteration 72, loss = 0.84117478\n",
      "Iteration 73, loss = 0.84045971\n",
      "Iteration 74, loss = 0.84006994\n",
      "Iteration 75, loss = 0.83832952\n",
      "Iteration 76, loss = 0.83800415\n",
      "Iteration 77, loss = 0.83717125\n",
      "Iteration 78, loss = 0.83620054\n",
      "Iteration 79, loss = 0.83610741\n",
      "Iteration 80, loss = 0.83449118\n",
      "Iteration 81, loss = 0.83405588\n",
      "Iteration 82, loss = 0.83456207\n",
      "Iteration 83, loss = 0.83222053\n",
      "Iteration 84, loss = 0.83237421\n",
      "Iteration 85, loss = 0.83204944\n",
      "Iteration 86, loss = 0.83087365\n",
      "Iteration 87, loss = 0.83022888\n",
      "Iteration 88, loss = 0.82988200\n",
      "Iteration 89, loss = 0.82869015\n",
      "Iteration 90, loss = 0.82918491\n",
      "Iteration 91, loss = 0.82756178\n",
      "Iteration 92, loss = 0.82616985\n",
      "Iteration 93, loss = 0.82655497\n",
      "Iteration 94, loss = 0.82672702\n",
      "Iteration 95, loss = 0.82596058\n",
      "Iteration 96, loss = 0.82448159\n",
      "Iteration 97, loss = 0.82426757\n",
      "Iteration 98, loss = 0.82453726\n",
      "Iteration 99, loss = 0.82309024\n",
      "Iteration 100, loss = 0.82236797\n",
      "Iteration 101, loss = 0.82209735\n",
      "Iteration 102, loss = 0.82094888\n",
      "Iteration 103, loss = 0.82184101\n",
      "Iteration 104, loss = 0.81978851\n",
      "Iteration 105, loss = 0.81944630\n",
      "Iteration 106, loss = 0.82009567\n",
      "Iteration 107, loss = 0.81922046\n",
      "Iteration 108, loss = 0.81870486\n",
      "Iteration 109, loss = 0.81805211\n",
      "Iteration 110, loss = 0.81795280\n",
      "Iteration 111, loss = 0.81673008\n",
      "Iteration 112, loss = 0.81655683\n",
      "Iteration 113, loss = 0.81610626\n",
      "Iteration 114, loss = 0.81450709\n",
      "Iteration 115, loss = 0.81553913\n",
      "Iteration 116, loss = 0.81435597\n",
      "Iteration 117, loss = 0.81367459\n",
      "Iteration 118, loss = 0.81386850\n",
      "Iteration 119, loss = 0.81335484\n",
      "Iteration 120, loss = 0.81270546\n",
      "Iteration 121, loss = 0.81292085\n",
      "Iteration 122, loss = 0.81257068\n",
      "Iteration 123, loss = 0.81145855\n",
      "Iteration 124, loss = 0.81172151\n",
      "Iteration 125, loss = 0.81090414\n",
      "Iteration 126, loss = 0.81099522\n",
      "Iteration 127, loss = 0.81012716\n",
      "Iteration 128, loss = 0.80899380\n",
      "Iteration 129, loss = 0.80829465\n",
      "Iteration 130, loss = 0.80885737\n",
      "Iteration 131, loss = 0.80913303\n",
      "Iteration 132, loss = 0.80961417\n",
      "Iteration 133, loss = 0.80814826\n",
      "Iteration 134, loss = 0.80689978\n",
      "Iteration 135, loss = 0.80734717\n",
      "Iteration 136, loss = 0.80713994\n",
      "Iteration 137, loss = 0.80703464\n",
      "Iteration 138, loss = 0.80534157\n",
      "Iteration 139, loss = 0.80550321\n",
      "Iteration 140, loss = 0.80541882\n",
      "Iteration 141, loss = 0.80569052\n",
      "Iteration 142, loss = 0.80570788\n",
      "Iteration 143, loss = 0.80445530\n",
      "Iteration 144, loss = 0.80394326\n",
      "Iteration 145, loss = 0.80407898\n",
      "Iteration 146, loss = 0.80328924\n",
      "Iteration 147, loss = 0.80280495\n",
      "Iteration 148, loss = 0.80172614\n",
      "Iteration 149, loss = 0.80231837\n",
      "Iteration 150, loss = 0.80236543\n",
      "Iteration 151, loss = 0.80223333\n",
      "Iteration 152, loss = 0.80148461\n",
      "Iteration 153, loss = 0.80190053\n",
      "Iteration 154, loss = 0.80148677\n",
      "Iteration 155, loss = 0.80169898\n",
      "Iteration 156, loss = 0.80125893\n",
      "Iteration 157, loss = 0.80031836\n",
      "Iteration 158, loss = 0.80016001\n",
      "Iteration 159, loss = 0.80091301\n",
      "Iteration 160, loss = 0.79910718\n",
      "Iteration 161, loss = 0.79998253\n",
      "Iteration 162, loss = 0.79933533\n",
      "Iteration 163, loss = 0.79872129\n",
      "Iteration 164, loss = 0.79864706\n",
      "Iteration 165, loss = 0.79872121\n",
      "Iteration 166, loss = 0.79948880\n",
      "Iteration 167, loss = 0.79834048\n",
      "Iteration 168, loss = 0.79836565\n",
      "Iteration 169, loss = 0.79670664\n",
      "Iteration 170, loss = 0.79750893\n",
      "Iteration 171, loss = 0.79744326\n",
      "Iteration 172, loss = 0.79627970\n",
      "Iteration 173, loss = 0.79719526\n",
      "Iteration 174, loss = 0.79646808\n",
      "Iteration 175, loss = 0.79693038\n",
      "Iteration 176, loss = 0.79583927\n",
      "Iteration 177, loss = 0.79604702\n",
      "Iteration 178, loss = 0.79519390\n",
      "Iteration 179, loss = 0.79544708\n",
      "Iteration 180, loss = 0.79496130\n",
      "Iteration 181, loss = 0.79437636\n",
      "Iteration 182, loss = 0.79470909\n",
      "Iteration 183, loss = 0.79490579\n",
      "Iteration 184, loss = 0.79454939\n",
      "Iteration 185, loss = 0.79480877\n",
      "Iteration 186, loss = 0.79437872\n",
      "Iteration 187, loss = 0.79409975\n",
      "Iteration 188, loss = 0.79393144\n",
      "Iteration 189, loss = 0.79412090\n",
      "Iteration 190, loss = 0.79219943\n",
      "Iteration 191, loss = 0.79380797\n",
      "Iteration 192, loss = 0.79254134\n",
      "Iteration 193, loss = 0.79330922\n",
      "Iteration 194, loss = 0.79229121\n",
      "Iteration 195, loss = 0.79327447\n",
      "Iteration 196, loss = 0.79308697\n",
      "Iteration 197, loss = 0.79232537\n",
      "Iteration 198, loss = 0.79146868\n",
      "Iteration 199, loss = 0.79107163\n",
      "Iteration 200, loss = 0.79124426\n",
      "Iteration 201, loss = 0.79185605\n",
      "Iteration 202, loss = 0.79083717\n",
      "Iteration 203, loss = 0.79182717\n",
      "Iteration 204, loss = 0.79113937\n",
      "Iteration 205, loss = 0.78995240\n",
      "Iteration 206, loss = 0.78997130\n",
      "Iteration 207, loss = 0.78982180\n",
      "Iteration 208, loss = 0.79025448\n",
      "Iteration 209, loss = 0.78911390\n",
      "Iteration 210, loss = 0.78911153\n",
      "Iteration 211, loss = 0.78971268\n",
      "Iteration 212, loss = 0.78944006\n",
      "Iteration 213, loss = 0.78858446\n",
      "Iteration 214, loss = 0.78872576\n",
      "Iteration 215, loss = 0.78908714\n",
      "Iteration 216, loss = 0.78953842\n",
      "Iteration 217, loss = 0.78811951\n",
      "Iteration 218, loss = 0.78945195\n",
      "Iteration 219, loss = 0.78859789\n",
      "Iteration 220, loss = 0.78782787\n",
      "Iteration 221, loss = 0.78919585\n",
      "Iteration 222, loss = 0.78804024\n",
      "Iteration 223, loss = 0.78848610\n",
      "Iteration 224, loss = 0.78780663\n",
      "Iteration 225, loss = 0.78674779\n",
      "Iteration 226, loss = 0.78702497\n",
      "Iteration 227, loss = 0.78716882\n",
      "Iteration 228, loss = 0.78681860\n",
      "Iteration 229, loss = 0.78746990\n",
      "Iteration 230, loss = 0.78640983\n",
      "Iteration 231, loss = 0.78628407\n",
      "Iteration 232, loss = 0.78682753\n",
      "Iteration 233, loss = 0.78644378\n",
      "Iteration 234, loss = 0.78705972\n",
      "Iteration 235, loss = 0.78638038\n",
      "Iteration 236, loss = 0.78542527\n",
      "Iteration 237, loss = 0.78651969\n",
      "Iteration 238, loss = 0.78563816\n",
      "Iteration 239, loss = 0.78582613\n",
      "Iteration 240, loss = 0.78635406\n",
      "Iteration 241, loss = 0.78579966\n",
      "Iteration 242, loss = 0.78566593\n",
      "Iteration 243, loss = 0.78463485\n",
      "Iteration 244, loss = 0.78465214\n",
      "Iteration 245, loss = 0.78517452\n",
      "Iteration 246, loss = 0.78439045\n",
      "Iteration 247, loss = 0.78499489\n",
      "Iteration 248, loss = 0.78565307\n",
      "Iteration 249, loss = 0.78457251\n",
      "Iteration 250, loss = 0.78324434\n",
      "Iteration 251, loss = 0.78364629\n",
      "Iteration 252, loss = 0.78271399\n",
      "Iteration 253, loss = 0.78391253\n",
      "Iteration 254, loss = 0.78305437\n",
      "Iteration 255, loss = 0.78339841\n",
      "Iteration 256, loss = 0.78343422\n",
      "Iteration 257, loss = 0.78271930\n",
      "Iteration 258, loss = 0.78286826\n",
      "Iteration 259, loss = 0.78381229\n",
      "Iteration 260, loss = 0.78183373\n",
      "Iteration 261, loss = 0.78291791\n",
      "Iteration 262, loss = 0.78247820\n",
      "Iteration 263, loss = 0.78237038\n",
      "Iteration 264, loss = 0.78275924\n",
      "Iteration 265, loss = 0.78170762\n",
      "Iteration 266, loss = 0.78153021\n",
      "Iteration 267, loss = 0.78209648\n",
      "Iteration 268, loss = 0.78268688\n",
      "Iteration 269, loss = 0.78137425\n",
      "Iteration 270, loss = 0.78126672\n",
      "Iteration 271, loss = 0.78121672\n",
      "Iteration 272, loss = 0.78067275\n",
      "Iteration 273, loss = 0.78244416\n",
      "Iteration 274, loss = 0.78134330\n",
      "Iteration 275, loss = 0.78045346\n",
      "Iteration 276, loss = 0.78012546\n",
      "Iteration 277, loss = 0.77966258\n",
      "Iteration 278, loss = 0.78043289\n",
      "Iteration 279, loss = 0.78122213\n",
      "Iteration 280, loss = 0.78115997\n",
      "Iteration 281, loss = 0.78066133\n",
      "Iteration 282, loss = 0.77985714\n",
      "Iteration 283, loss = 0.77962392\n",
      "Iteration 284, loss = 0.77977551\n",
      "Iteration 285, loss = 0.77995371\n",
      "Iteration 286, loss = 0.77963334\n",
      "Iteration 287, loss = 0.78003098\n",
      "Iteration 288, loss = 0.78031522\n",
      "Iteration 289, loss = 0.77904808\n",
      "Iteration 290, loss = 0.78022602\n",
      "Iteration 291, loss = 0.77994051\n",
      "Iteration 292, loss = 0.77823332\n",
      "Iteration 293, loss = 0.77908928\n",
      "Iteration 294, loss = 0.77881047\n",
      "Iteration 295, loss = 0.77864824\n",
      "Iteration 296, loss = 0.77913462\n",
      "Iteration 297, loss = 0.77907235\n",
      "Iteration 298, loss = 0.77907038\n",
      "Iteration 299, loss = 0.77810520\n",
      "Iteration 300, loss = 0.77789964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiergallo/miniconda3/envs/diplodatos/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.61742848\n",
      "Iteration 2, loss = 1.50757027\n",
      "Iteration 3, loss = 1.26029338\n",
      "Iteration 4, loss = 1.16518973\n",
      "Iteration 5, loss = 1.11578492\n",
      "Iteration 6, loss = 1.08147117\n",
      "Iteration 7, loss = 1.05700553\n",
      "Iteration 8, loss = 1.03784213\n",
      "Iteration 9, loss = 1.02059157\n",
      "Iteration 10, loss = 1.00922082\n",
      "Iteration 11, loss = 0.99753320\n",
      "Iteration 12, loss = 0.98669686\n",
      "Iteration 13, loss = 0.97776790\n",
      "Iteration 14, loss = 0.97025752\n",
      "Iteration 15, loss = 0.96323410\n",
      "Iteration 16, loss = 0.95756052\n",
      "Iteration 17, loss = 0.95060418\n",
      "Iteration 18, loss = 0.94510227\n",
      "Iteration 19, loss = 0.93973074\n",
      "Iteration 20, loss = 0.93515922\n",
      "Iteration 21, loss = 0.93182112\n",
      "Iteration 22, loss = 0.92679950\n",
      "Iteration 23, loss = 0.92260554\n",
      "Iteration 24, loss = 0.92002115\n",
      "Iteration 25, loss = 0.91615432\n",
      "Iteration 26, loss = 0.91306355\n",
      "Iteration 27, loss = 0.91051500\n",
      "Iteration 28, loss = 0.90743691\n",
      "Iteration 29, loss = 0.90464131\n",
      "Iteration 30, loss = 0.90188324\n",
      "Iteration 31, loss = 0.89947192\n",
      "Iteration 32, loss = 0.89679245\n",
      "Iteration 33, loss = 0.89491976\n",
      "Iteration 34, loss = 0.89388446\n",
      "Iteration 35, loss = 0.88981066\n",
      "Iteration 36, loss = 0.88828553\n",
      "Iteration 37, loss = 0.88785348\n",
      "Iteration 38, loss = 0.88535147\n",
      "Iteration 39, loss = 0.88293384\n",
      "Iteration 40, loss = 0.88188318\n",
      "Iteration 41, loss = 0.87928226\n",
      "Iteration 42, loss = 0.87896988\n",
      "Iteration 43, loss = 0.87733781\n",
      "Iteration 44, loss = 0.87648885\n",
      "Iteration 45, loss = 0.87427689\n",
      "Iteration 46, loss = 0.87304484\n",
      "Iteration 47, loss = 0.87140586\n",
      "Iteration 48, loss = 0.87044711\n",
      "Iteration 49, loss = 0.86920760\n",
      "Iteration 50, loss = 0.86925939\n",
      "Iteration 51, loss = 0.86613576\n",
      "Iteration 52, loss = 0.86613735\n",
      "Iteration 53, loss = 0.86462607\n",
      "Iteration 54, loss = 0.86352678\n",
      "Iteration 55, loss = 0.86316836\n",
      "Iteration 56, loss = 0.86149107\n",
      "Iteration 57, loss = 0.85965154\n",
      "Iteration 58, loss = 0.85988182\n",
      "Iteration 59, loss = 0.85876194\n",
      "Iteration 60, loss = 0.85740522\n",
      "Iteration 61, loss = 0.85675134\n",
      "Iteration 62, loss = 0.85540790\n",
      "Iteration 63, loss = 0.85406180\n",
      "Iteration 64, loss = 0.85291620\n",
      "Iteration 65, loss = 0.85278565\n",
      "Iteration 66, loss = 0.85155461\n",
      "Iteration 67, loss = 0.85085116\n",
      "Iteration 68, loss = 0.85014302\n",
      "Iteration 69, loss = 0.84900912\n",
      "Iteration 70, loss = 0.84798633\n",
      "Iteration 71, loss = 0.84780644\n",
      "Iteration 72, loss = 0.84695229\n",
      "Iteration 73, loss = 0.84558479\n",
      "Iteration 74, loss = 0.84533554\n",
      "Iteration 75, loss = 0.84409398\n",
      "Iteration 76, loss = 0.84384813\n",
      "Iteration 77, loss = 0.84375045\n",
      "Iteration 78, loss = 0.84203758\n",
      "Iteration 79, loss = 0.84134799\n",
      "Iteration 80, loss = 0.84136230\n",
      "Iteration 81, loss = 0.84096246\n",
      "Iteration 82, loss = 0.84110770\n",
      "Iteration 83, loss = 0.83923153\n",
      "Iteration 84, loss = 0.83951467\n",
      "Iteration 85, loss = 0.83902684\n",
      "Iteration 86, loss = 0.83719473\n",
      "Iteration 87, loss = 0.83700851\n",
      "Iteration 88, loss = 0.83689883\n",
      "Iteration 89, loss = 0.83631943\n",
      "Iteration 90, loss = 0.83549963\n",
      "Iteration 91, loss = 0.83429129\n",
      "Iteration 92, loss = 0.83437740\n",
      "Iteration 93, loss = 0.83375882\n",
      "Iteration 94, loss = 0.83407463\n",
      "Iteration 95, loss = 0.83295190\n",
      "Iteration 96, loss = 0.83246051\n",
      "Iteration 97, loss = 0.83177188\n",
      "Iteration 98, loss = 0.83182561\n",
      "Iteration 99, loss = 0.83084462\n",
      "Iteration 100, loss = 0.82950423\n",
      "Iteration 101, loss = 0.83053139\n",
      "Iteration 102, loss = 0.82935643\n",
      "Iteration 103, loss = 0.82900709\n",
      "Iteration 104, loss = 0.82827173\n",
      "Iteration 105, loss = 0.82712941\n",
      "Iteration 106, loss = 0.82713284\n",
      "Iteration 107, loss = 0.82635705\n",
      "Iteration 108, loss = 0.82613180\n",
      "Iteration 109, loss = 0.82708873\n",
      "Iteration 110, loss = 0.82554509\n",
      "Iteration 111, loss = 0.82466982\n",
      "Iteration 112, loss = 0.82546898\n",
      "Iteration 113, loss = 0.82519827\n",
      "Iteration 114, loss = 0.82429977\n",
      "Iteration 115, loss = 0.82337067\n",
      "Iteration 116, loss = 0.82410140\n",
      "Iteration 117, loss = 0.82311878\n",
      "Iteration 118, loss = 0.82220406\n",
      "Iteration 119, loss = 0.82325940\n",
      "Iteration 120, loss = 0.82109520\n",
      "Iteration 121, loss = 0.82102306\n",
      "Iteration 122, loss = 0.82044284\n",
      "Iteration 123, loss = 0.82085990\n",
      "Iteration 124, loss = 0.82032517\n",
      "Iteration 125, loss = 0.81978876\n",
      "Iteration 126, loss = 0.82015405\n",
      "Iteration 127, loss = 0.81878112\n",
      "Iteration 128, loss = 0.81833635\n",
      "Iteration 129, loss = 0.81833360\n",
      "Iteration 130, loss = 0.81826825\n",
      "Iteration 131, loss = 0.81871393\n",
      "Iteration 132, loss = 0.81730979\n",
      "Iteration 133, loss = 0.81781766\n",
      "Iteration 134, loss = 0.81651475\n",
      "Iteration 135, loss = 0.81595708\n",
      "Iteration 136, loss = 0.81642216\n",
      "Iteration 137, loss = 0.81581107\n",
      "Iteration 138, loss = 0.81463445\n",
      "Iteration 139, loss = 0.81516055\n",
      "Iteration 140, loss = 0.81465517\n",
      "Iteration 141, loss = 0.81478088\n",
      "Iteration 142, loss = 0.81438569\n",
      "Iteration 143, loss = 0.81370323\n",
      "Iteration 144, loss = 0.81330179\n",
      "Iteration 145, loss = 0.81283581\n",
      "Iteration 146, loss = 0.81198241\n",
      "Iteration 147, loss = 0.81220385\n",
      "Iteration 148, loss = 0.81190124\n",
      "Iteration 149, loss = 0.81118875\n",
      "Iteration 150, loss = 0.81157253\n",
      "Iteration 151, loss = 0.81261094\n",
      "Iteration 152, loss = 0.81112891\n",
      "Iteration 153, loss = 0.81030661\n",
      "Iteration 154, loss = 0.81109174\n",
      "Iteration 155, loss = 0.81076518\n",
      "Iteration 156, loss = 0.80988474\n",
      "Iteration 157, loss = 0.80885830\n",
      "Iteration 158, loss = 0.80887917\n",
      "Iteration 159, loss = 0.80919549\n",
      "Iteration 160, loss = 0.80901065\n",
      "Iteration 161, loss = 0.80913003\n",
      "Iteration 162, loss = 0.80815953\n",
      "Iteration 163, loss = 0.80868711\n",
      "Iteration 164, loss = 0.80752606\n",
      "Iteration 165, loss = 0.80690589\n",
      "Iteration 166, loss = 0.80751174\n",
      "Iteration 167, loss = 0.80756713\n",
      "Iteration 168, loss = 0.80699716\n",
      "Iteration 169, loss = 0.80565534\n",
      "Iteration 170, loss = 0.80616904\n",
      "Iteration 171, loss = 0.80529670\n",
      "Iteration 172, loss = 0.80502333\n",
      "Iteration 173, loss = 0.80614947\n",
      "Iteration 174, loss = 0.80574539\n",
      "Iteration 175, loss = 0.80496742\n",
      "Iteration 176, loss = 0.80544914\n",
      "Iteration 177, loss = 0.80483662\n",
      "Iteration 178, loss = 0.80508350\n",
      "Iteration 179, loss = 0.80389412\n",
      "Iteration 180, loss = 0.80332733\n",
      "Iteration 181, loss = 0.80374967\n",
      "Iteration 182, loss = 0.80316847\n",
      "Iteration 183, loss = 0.80306329\n",
      "Iteration 184, loss = 0.80209081\n",
      "Iteration 185, loss = 0.80235213\n",
      "Iteration 186, loss = 0.80257250\n",
      "Iteration 187, loss = 0.80182447\n",
      "Iteration 188, loss = 0.80227734\n",
      "Iteration 189, loss = 0.80236652\n",
      "Iteration 190, loss = 0.80155280\n",
      "Iteration 191, loss = 0.80170552\n",
      "Iteration 192, loss = 0.80145336\n",
      "Iteration 193, loss = 0.80199411\n",
      "Iteration 194, loss = 0.79998632\n",
      "Iteration 195, loss = 0.80064067\n",
      "Iteration 196, loss = 0.80003451\n",
      "Iteration 197, loss = 0.79958152\n",
      "Iteration 198, loss = 0.79893943\n",
      "Iteration 199, loss = 0.79881880\n",
      "Iteration 200, loss = 0.79934135\n",
      "Iteration 201, loss = 0.79921036\n",
      "Iteration 202, loss = 0.79985779\n",
      "Iteration 203, loss = 0.79865047\n",
      "Iteration 204, loss = 0.79944017\n",
      "Iteration 205, loss = 0.79868872\n",
      "Iteration 206, loss = 0.79807788\n",
      "Iteration 207, loss = 0.79850247\n",
      "Iteration 208, loss = 0.79750920\n",
      "Iteration 209, loss = 0.79853310\n",
      "Iteration 210, loss = 0.79753416\n",
      "Iteration 211, loss = 0.79727368\n",
      "Iteration 212, loss = 0.79684085\n",
      "Iteration 213, loss = 0.79700646\n",
      "Iteration 214, loss = 0.79692271\n",
      "Iteration 215, loss = 0.79815100\n",
      "Iteration 216, loss = 0.79682958\n",
      "Iteration 217, loss = 0.79575482\n",
      "Iteration 218, loss = 0.79640763\n",
      "Iteration 219, loss = 0.79536436\n",
      "Iteration 220, loss = 0.79516646\n",
      "Iteration 221, loss = 0.79628472\n",
      "Iteration 222, loss = 0.79612196\n",
      "Iteration 223, loss = 0.79495606\n",
      "Iteration 224, loss = 0.79565709\n",
      "Iteration 225, loss = 0.79483399\n",
      "Iteration 226, loss = 0.79440091\n",
      "Iteration 227, loss = 0.79539687\n",
      "Iteration 228, loss = 0.79396635\n",
      "Iteration 229, loss = 0.79428780\n",
      "Iteration 230, loss = 0.79419658\n",
      "Iteration 231, loss = 0.79388346\n",
      "Iteration 232, loss = 0.79438808\n",
      "Iteration 233, loss = 0.79370468\n",
      "Iteration 234, loss = 0.79392749\n",
      "Iteration 235, loss = 0.79219506\n",
      "Iteration 236, loss = 0.79384864\n",
      "Iteration 237, loss = 0.79300795\n",
      "Iteration 238, loss = 0.79184348\n",
      "Iteration 239, loss = 0.79310021\n",
      "Iteration 240, loss = 0.79292731\n",
      "Iteration 241, loss = 0.79264664\n",
      "Iteration 242, loss = 0.79227546\n",
      "Iteration 243, loss = 0.79268141\n",
      "Iteration 244, loss = 0.79197466\n",
      "Iteration 245, loss = 0.79173084\n",
      "Iteration 246, loss = 0.79145344\n",
      "Iteration 247, loss = 0.79167572\n",
      "Iteration 248, loss = 0.79168724\n",
      "Iteration 249, loss = 0.79078108\n",
      "Iteration 250, loss = 0.79069160\n",
      "Iteration 251, loss = 0.79113430\n",
      "Iteration 252, loss = 0.79024517\n",
      "Iteration 253, loss = 0.79089397\n",
      "Iteration 254, loss = 0.79025917\n",
      "Iteration 255, loss = 0.78913897\n",
      "Iteration 256, loss = 0.78985547\n",
      "Iteration 257, loss = 0.79044791\n",
      "Iteration 258, loss = 0.78995418\n",
      "Iteration 259, loss = 0.78909458\n",
      "Iteration 260, loss = 0.78878084\n",
      "Iteration 261, loss = 0.78902818\n",
      "Iteration 262, loss = 0.78930295\n",
      "Iteration 263, loss = 0.78858240\n",
      "Iteration 264, loss = 0.78987039\n",
      "Iteration 265, loss = 0.78888070\n",
      "Iteration 266, loss = 0.78897800\n",
      "Iteration 267, loss = 0.78963085\n",
      "Iteration 268, loss = 0.78918492\n",
      "Iteration 269, loss = 0.78926856\n",
      "Iteration 270, loss = 0.78797236\n",
      "Iteration 271, loss = 0.78889859\n",
      "Iteration 272, loss = 0.78800673\n",
      "Iteration 273, loss = 0.78825540\n",
      "Iteration 274, loss = 0.78736891\n",
      "Iteration 275, loss = 0.78755805\n",
      "Iteration 276, loss = 0.78662364\n",
      "Iteration 277, loss = 0.78665555\n",
      "Iteration 278, loss = 0.78694511\n",
      "Iteration 279, loss = 0.78654374\n",
      "Iteration 280, loss = 0.78585774\n",
      "Iteration 281, loss = 0.78575044\n",
      "Iteration 282, loss = 0.78643334\n",
      "Iteration 283, loss = 0.78623992\n",
      "Iteration 284, loss = 0.78562205\n",
      "Iteration 285, loss = 0.78574701\n",
      "Iteration 286, loss = 0.78606542\n",
      "Iteration 287, loss = 0.78723148\n",
      "Iteration 288, loss = 0.78618706\n",
      "Iteration 289, loss = 0.78494922\n",
      "Iteration 290, loss = 0.78579007\n",
      "Iteration 291, loss = 0.78549553\n",
      "Iteration 292, loss = 0.78561131\n",
      "Iteration 293, loss = 0.78557436\n",
      "Iteration 294, loss = 0.78534972\n",
      "Iteration 295, loss = 0.78576768\n",
      "Iteration 296, loss = 0.78379304\n",
      "Iteration 297, loss = 0.78466248\n",
      "Iteration 298, loss = 0.78444727\n",
      "Iteration 299, loss = 0.78427337\n",
      "Iteration 300, loss = 0.78309807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiergallo/miniconda3/envs/diplodatos/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.61162668\n",
      "Iteration 2, loss = 1.50725433\n",
      "Iteration 3, loss = 1.25420766\n",
      "Iteration 4, loss = 1.16089924\n",
      "Iteration 5, loss = 1.10776228\n",
      "Iteration 6, loss = 1.07353982\n",
      "Iteration 7, loss = 1.04865884\n",
      "Iteration 8, loss = 1.02852855\n",
      "Iteration 9, loss = 1.01260724\n",
      "Iteration 10, loss = 1.00087157\n",
      "Iteration 11, loss = 0.98973708\n",
      "Iteration 12, loss = 0.98029392\n",
      "Iteration 13, loss = 0.97172212\n",
      "Iteration 14, loss = 0.96460516\n",
      "Iteration 15, loss = 0.95858023\n",
      "Iteration 16, loss = 0.95016627\n",
      "Iteration 17, loss = 0.94490350\n",
      "Iteration 18, loss = 0.93909660\n",
      "Iteration 19, loss = 0.93475591\n",
      "Iteration 20, loss = 0.93040314\n",
      "Iteration 21, loss = 0.92460986\n",
      "Iteration 22, loss = 0.92072016\n",
      "Iteration 23, loss = 0.91740571\n",
      "Iteration 24, loss = 0.91312342\n",
      "Iteration 25, loss = 0.91023966\n",
      "Iteration 26, loss = 0.90713775\n",
      "Iteration 27, loss = 0.90514209\n",
      "Iteration 28, loss = 0.90200687\n",
      "Iteration 29, loss = 0.89923559\n",
      "Iteration 30, loss = 0.89709592\n",
      "Iteration 31, loss = 0.89344736\n",
      "Iteration 32, loss = 0.89139236\n",
      "Iteration 33, loss = 0.88964953\n",
      "Iteration 34, loss = 0.88671308\n",
      "Iteration 35, loss = 0.88523472\n",
      "Iteration 36, loss = 0.88309629\n",
      "Iteration 37, loss = 0.88355001\n",
      "Iteration 38, loss = 0.88049513\n",
      "Iteration 39, loss = 0.87747673\n",
      "Iteration 40, loss = 0.87678894\n",
      "Iteration 41, loss = 0.87464675\n",
      "Iteration 42, loss = 0.87402391\n",
      "Iteration 43, loss = 0.87155541\n",
      "Iteration 44, loss = 0.87044307\n",
      "Iteration 45, loss = 0.86887138\n",
      "Iteration 46, loss = 0.86751975\n",
      "Iteration 47, loss = 0.86579050\n",
      "Iteration 48, loss = 0.86599381\n",
      "Iteration 49, loss = 0.86332621\n",
      "Iteration 50, loss = 0.86273020\n",
      "Iteration 51, loss = 0.86131695\n",
      "Iteration 52, loss = 0.86003907\n",
      "Iteration 53, loss = 0.85798552\n",
      "Iteration 54, loss = 0.85818805\n",
      "Iteration 55, loss = 0.85759093\n",
      "Iteration 56, loss = 0.85568531\n",
      "Iteration 57, loss = 0.85519603\n",
      "Iteration 58, loss = 0.85355967\n",
      "Iteration 59, loss = 0.85219385\n",
      "Iteration 60, loss = 0.85189268\n",
      "Iteration 61, loss = 0.85096821\n",
      "Iteration 62, loss = 0.85010279\n",
      "Iteration 63, loss = 0.84937190\n",
      "Iteration 64, loss = 0.84833084\n",
      "Iteration 65, loss = 0.84749737\n",
      "Iteration 66, loss = 0.84667302\n",
      "Iteration 67, loss = 0.84525632\n",
      "Iteration 68, loss = 0.84439915\n",
      "Iteration 69, loss = 0.84399246\n",
      "Iteration 70, loss = 0.84345126\n",
      "Iteration 71, loss = 0.84223511\n",
      "Iteration 72, loss = 0.84143964\n",
      "Iteration 73, loss = 0.84059758\n",
      "Iteration 74, loss = 0.83965394\n",
      "Iteration 75, loss = 0.83874093\n",
      "Iteration 76, loss = 0.83849355\n",
      "Iteration 77, loss = 0.83826714\n",
      "Iteration 78, loss = 0.83696147\n",
      "Iteration 79, loss = 0.83672871\n",
      "Iteration 80, loss = 0.83519665\n",
      "Iteration 81, loss = 0.83449618\n",
      "Iteration 82, loss = 0.83420914\n",
      "Iteration 83, loss = 0.83446570\n",
      "Iteration 84, loss = 0.83250777\n",
      "Iteration 85, loss = 0.83280362\n",
      "Iteration 86, loss = 0.83216360\n",
      "Iteration 87, loss = 0.83135164\n",
      "Iteration 88, loss = 0.83210709\n",
      "Iteration 89, loss = 0.83023219\n",
      "Iteration 90, loss = 0.82986456\n",
      "Iteration 91, loss = 0.82924325\n",
      "Iteration 92, loss = 0.82963169\n",
      "Iteration 93, loss = 0.82854523\n",
      "Iteration 94, loss = 0.82876157\n",
      "Iteration 95, loss = 0.82741280\n",
      "Iteration 96, loss = 0.82766362\n",
      "Iteration 97, loss = 0.82637647\n",
      "Iteration 98, loss = 0.82568096\n",
      "Iteration 99, loss = 0.82578315\n",
      "Iteration 100, loss = 0.82424720\n",
      "Iteration 101, loss = 0.82503207\n",
      "Iteration 102, loss = 0.82480100\n",
      "Iteration 103, loss = 0.82320902\n",
      "Iteration 104, loss = 0.82327385\n",
      "Iteration 105, loss = 0.82296764\n",
      "Iteration 106, loss = 0.82260141\n",
      "Iteration 107, loss = 0.82228441\n",
      "Iteration 108, loss = 0.82143051\n",
      "Iteration 109, loss = 0.82093814\n",
      "Iteration 110, loss = 0.82037209\n",
      "Iteration 111, loss = 0.81959754\n",
      "Iteration 112, loss = 0.82012228\n",
      "Iteration 113, loss = 0.81978543\n",
      "Iteration 114, loss = 0.81973272\n",
      "Iteration 115, loss = 0.81940816\n",
      "Iteration 116, loss = 0.81780697\n",
      "Iteration 117, loss = 0.81720581\n",
      "Iteration 118, loss = 0.81765851\n",
      "Iteration 119, loss = 0.81721437\n",
      "Iteration 120, loss = 0.81635623\n",
      "Iteration 121, loss = 0.81652714\n",
      "Iteration 122, loss = 0.81581938\n",
      "Iteration 123, loss = 0.81563182\n",
      "Iteration 124, loss = 0.81560516\n",
      "Iteration 125, loss = 0.81474183\n",
      "Iteration 126, loss = 0.81511380\n",
      "Iteration 127, loss = 0.81432271\n",
      "Iteration 128, loss = 0.81261720\n",
      "Iteration 129, loss = 0.81353113\n",
      "Iteration 130, loss = 0.81375008\n",
      "Iteration 131, loss = 0.81284441\n",
      "Iteration 132, loss = 0.81157376\n",
      "Iteration 133, loss = 0.81181747\n",
      "Iteration 134, loss = 0.81237830\n",
      "Iteration 135, loss = 0.81211482\n",
      "Iteration 136, loss = 0.81186130\n",
      "Iteration 137, loss = 0.81043472\n",
      "Iteration 138, loss = 0.80951498\n",
      "Iteration 139, loss = 0.80998957\n",
      "Iteration 140, loss = 0.80975132\n",
      "Iteration 141, loss = 0.80913811\n",
      "Iteration 142, loss = 0.80930719\n",
      "Iteration 143, loss = 0.80856031\n",
      "Iteration 144, loss = 0.80894644\n",
      "Iteration 145, loss = 0.80750081\n",
      "Iteration 146, loss = 0.80808145\n",
      "Iteration 147, loss = 0.80741583\n",
      "Iteration 148, loss = 0.80683049\n",
      "Iteration 149, loss = 0.80714943\n",
      "Iteration 150, loss = 0.80713782\n",
      "Iteration 151, loss = 0.80563055\n",
      "Iteration 152, loss = 0.80539814\n",
      "Iteration 153, loss = 0.80484460\n",
      "Iteration 154, loss = 0.80597214\n",
      "Iteration 155, loss = 0.80535279\n",
      "Iteration 156, loss = 0.80577361\n",
      "Iteration 157, loss = 0.80407780\n",
      "Iteration 158, loss = 0.80369463\n",
      "Iteration 159, loss = 0.80341547\n",
      "Iteration 160, loss = 0.80436442\n",
      "Iteration 161, loss = 0.80344528\n",
      "Iteration 162, loss = 0.80300571\n",
      "Iteration 163, loss = 0.80211694\n",
      "Iteration 164, loss = 0.80254830\n",
      "Iteration 165, loss = 0.80265071\n",
      "Iteration 166, loss = 0.80215467\n",
      "Iteration 167, loss = 0.80187449\n",
      "Iteration 168, loss = 0.80242537\n",
      "Iteration 169, loss = 0.80080489\n",
      "Iteration 170, loss = 0.80187658\n",
      "Iteration 171, loss = 0.80129505\n",
      "Iteration 172, loss = 0.80102770\n",
      "Iteration 173, loss = 0.80124100\n",
      "Iteration 174, loss = 0.80025238\n",
      "Iteration 175, loss = 0.80037230\n",
      "Iteration 176, loss = 0.79943871\n",
      "Iteration 177, loss = 0.79905784\n",
      "Iteration 178, loss = 0.79819557\n",
      "Iteration 179, loss = 0.79950373\n",
      "Iteration 180, loss = 0.79890600\n",
      "Iteration 181, loss = 0.79800068\n",
      "Iteration 182, loss = 0.79738217\n",
      "Iteration 183, loss = 0.79765510\n",
      "Iteration 184, loss = 0.79754388\n",
      "Iteration 185, loss = 0.79690527\n",
      "Iteration 186, loss = 0.79744415\n",
      "Iteration 187, loss = 0.79713415\n",
      "Iteration 188, loss = 0.79715403\n",
      "Iteration 189, loss = 0.79601357\n",
      "Iteration 190, loss = 0.79663241\n",
      "Iteration 191, loss = 0.79505538\n",
      "Iteration 192, loss = 0.79533730\n",
      "Iteration 193, loss = 0.79598727\n",
      "Iteration 194, loss = 0.79626868\n",
      "Iteration 195, loss = 0.79500114\n",
      "Iteration 196, loss = 0.79452183\n",
      "Iteration 197, loss = 0.79572835\n",
      "Iteration 198, loss = 0.79415574\n",
      "Iteration 199, loss = 0.79333081\n",
      "Iteration 200, loss = 0.79393952\n",
      "Iteration 201, loss = 0.79494504\n",
      "Iteration 202, loss = 0.79268422\n",
      "Iteration 203, loss = 0.79266281\n",
      "Iteration 204, loss = 0.79343334\n",
      "Iteration 205, loss = 0.79391547\n",
      "Iteration 206, loss = 0.79346976\n",
      "Iteration 207, loss = 0.79273151\n",
      "Iteration 208, loss = 0.79180968\n",
      "Iteration 209, loss = 0.79285600\n",
      "Iteration 210, loss = 0.79290502\n",
      "Iteration 211, loss = 0.79237943\n",
      "Iteration 212, loss = 0.79242387\n",
      "Iteration 213, loss = 0.79114952\n",
      "Iteration 214, loss = 0.79177256\n",
      "Iteration 215, loss = 0.79130201\n",
      "Iteration 216, loss = 0.79082962\n",
      "Iteration 217, loss = 0.79020317\n",
      "Iteration 218, loss = 0.79095401\n",
      "Iteration 219, loss = 0.79054342\n",
      "Iteration 220, loss = 0.79019129\n",
      "Iteration 221, loss = 0.79054206\n",
      "Iteration 222, loss = 0.78968131\n",
      "Iteration 223, loss = 0.78971023\n",
      "Iteration 224, loss = 0.78964469\n",
      "Iteration 225, loss = 0.78993942\n",
      "Iteration 226, loss = 0.78974918\n",
      "Iteration 227, loss = 0.78903168\n",
      "Iteration 228, loss = 0.78877511\n",
      "Iteration 229, loss = 0.78931296\n",
      "Iteration 230, loss = 0.78903151\n",
      "Iteration 231, loss = 0.78853721\n",
      "Iteration 232, loss = 0.78723402\n",
      "Iteration 233, loss = 0.78843375\n",
      "Iteration 234, loss = 0.78848167\n",
      "Iteration 235, loss = 0.78808839\n",
      "Iteration 236, loss = 0.78752885\n",
      "Iteration 237, loss = 0.78716550\n",
      "Iteration 238, loss = 0.78756559\n",
      "Iteration 239, loss = 0.78759142\n",
      "Iteration 240, loss = 0.78722735\n",
      "Iteration 241, loss = 0.78840065\n",
      "Iteration 242, loss = 0.78728134\n",
      "Iteration 243, loss = 0.78709462\n",
      "Iteration 244, loss = 0.78587994\n",
      "Iteration 245, loss = 0.78577662\n",
      "Iteration 246, loss = 0.78668744\n",
      "Iteration 247, loss = 0.78544438\n",
      "Iteration 248, loss = 0.78568090\n",
      "Iteration 249, loss = 0.78582582\n",
      "Iteration 250, loss = 0.78517386\n",
      "Iteration 251, loss = 0.78552532\n",
      "Iteration 252, loss = 0.78444797\n",
      "Iteration 253, loss = 0.78510894\n",
      "Iteration 254, loss = 0.78432749\n",
      "Iteration 255, loss = 0.78468022\n",
      "Iteration 256, loss = 0.78463012\n",
      "Iteration 257, loss = 0.78447005\n",
      "Iteration 258, loss = 0.78483894\n",
      "Iteration 259, loss = 0.78425158\n",
      "Iteration 260, loss = 0.78375872\n",
      "Iteration 261, loss = 0.78389101\n",
      "Iteration 262, loss = 0.78387228\n",
      "Iteration 263, loss = 0.78456393\n",
      "Iteration 264, loss = 0.78428201\n",
      "Iteration 265, loss = 0.78335125\n",
      "Iteration 266, loss = 0.78289114\n",
      "Iteration 267, loss = 0.78230597\n",
      "Iteration 268, loss = 0.78325562\n",
      "Iteration 269, loss = 0.78306730\n",
      "Iteration 270, loss = 0.78352828\n",
      "Iteration 271, loss = 0.78252586\n",
      "Iteration 272, loss = 0.78206276\n",
      "Iteration 273, loss = 0.78228938\n",
      "Iteration 274, loss = 0.78187791\n",
      "Iteration 275, loss = 0.78184179\n",
      "Iteration 276, loss = 0.78192996\n",
      "Iteration 277, loss = 0.78228766\n",
      "Iteration 278, loss = 0.78257770\n",
      "Iteration 279, loss = 0.78048480\n",
      "Iteration 280, loss = 0.78108727\n",
      "Iteration 281, loss = 0.78139437\n",
      "Iteration 282, loss = 0.78151925\n",
      "Iteration 283, loss = 0.78119709\n",
      "Iteration 284, loss = 0.78073635\n",
      "Iteration 285, loss = 0.77991035\n",
      "Iteration 286, loss = 0.77969680\n",
      "Iteration 287, loss = 0.78040586\n",
      "Iteration 288, loss = 0.77996030\n",
      "Iteration 289, loss = 0.78025341\n",
      "Iteration 290, loss = 0.78040811\n",
      "Iteration 291, loss = 0.78040882\n",
      "Iteration 292, loss = 0.77974417\n",
      "Iteration 293, loss = 0.77986288\n",
      "Iteration 294, loss = 0.77866580\n",
      "Iteration 295, loss = 0.77921115\n",
      "Iteration 296, loss = 0.77937540\n",
      "Iteration 297, loss = 0.77941497\n",
      "Iteration 298, loss = 0.77883361\n",
      "Iteration 299, loss = 0.77957954\n",
      "Iteration 300, loss = 0.77849604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiergallo/miniconda3/envs/diplodatos/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.28674052\n",
      "Iteration 2, loss = 1.31093222\n",
      "Iteration 3, loss = 1.15790133\n",
      "Iteration 4, loss = 1.09663572\n",
      "Iteration 5, loss = 1.05953026\n",
      "Iteration 6, loss = 1.03445582\n",
      "Iteration 7, loss = 1.01706251\n",
      "Iteration 8, loss = 1.00225217\n",
      "Iteration 9, loss = 0.98997892\n",
      "Iteration 10, loss = 0.97956292\n",
      "Iteration 11, loss = 0.97005799\n",
      "Iteration 12, loss = 0.96265076\n",
      "Iteration 13, loss = 0.95459246\n",
      "Iteration 14, loss = 0.94900034\n",
      "Iteration 15, loss = 0.94254913\n",
      "Iteration 16, loss = 0.93711596\n",
      "Iteration 17, loss = 0.93326171\n",
      "Iteration 18, loss = 0.92768156\n",
      "Iteration 19, loss = 0.92412697\n",
      "Iteration 20, loss = 0.92019768\n",
      "Iteration 21, loss = 0.91722915\n",
      "Iteration 22, loss = 0.91399503\n",
      "Iteration 23, loss = 0.91063932\n",
      "Iteration 24, loss = 0.90832256\n",
      "Iteration 25, loss = 0.90624154\n",
      "Iteration 26, loss = 0.90331874\n",
      "Iteration 27, loss = 0.90168567\n",
      "Iteration 28, loss = 0.89933657\n",
      "Iteration 29, loss = 0.89743939\n",
      "Iteration 30, loss = 0.89601150\n",
      "Iteration 31, loss = 0.89432808\n",
      "Iteration 32, loss = 0.89211607\n",
      "Iteration 33, loss = 0.89175962\n",
      "Iteration 34, loss = 0.88886193\n",
      "Iteration 35, loss = 0.88730078\n",
      "Iteration 36, loss = 0.88596334\n",
      "Iteration 37, loss = 0.88492523\n",
      "Iteration 38, loss = 0.88367627\n",
      "Iteration 39, loss = 0.88252233\n",
      "Iteration 40, loss = 0.88032590\n",
      "Iteration 41, loss = 0.88000584\n",
      "Iteration 42, loss = 0.87846831\n",
      "Iteration 43, loss = 0.87780911\n",
      "Iteration 44, loss = 0.87598118\n",
      "Iteration 45, loss = 0.87518815\n",
      "Iteration 46, loss = 0.87445008\n",
      "Iteration 47, loss = 0.87381019\n",
      "Iteration 48, loss = 0.87265605\n",
      "Iteration 49, loss = 0.87106247\n",
      "Iteration 50, loss = 0.87058126\n",
      "Iteration 51, loss = 0.86980524\n",
      "Iteration 52, loss = 0.86847901\n",
      "Iteration 53, loss = 0.86838454\n",
      "Iteration 54, loss = 0.86775322\n",
      "Iteration 55, loss = 0.86675752\n",
      "Iteration 56, loss = 0.86582220\n",
      "Iteration 57, loss = 0.86489709\n",
      "Iteration 58, loss = 0.86401121\n",
      "Iteration 59, loss = 0.86326798\n",
      "Iteration 60, loss = 0.86266679\n",
      "Iteration 61, loss = 0.86211152\n",
      "Iteration 62, loss = 0.86187062\n",
      "Iteration 63, loss = 0.86068708\n",
      "Iteration 64, loss = 0.85996338\n",
      "Iteration 65, loss = 0.85969344\n",
      "Iteration 66, loss = 0.85867782\n",
      "Iteration 67, loss = 0.85827014\n",
      "Iteration 68, loss = 0.85835977\n",
      "Iteration 69, loss = 0.85722387\n",
      "Iteration 70, loss = 0.85655597\n",
      "Iteration 71, loss = 0.85639630\n",
      "Iteration 72, loss = 0.85600770\n",
      "Iteration 73, loss = 0.85564247\n",
      "Iteration 74, loss = 0.85501218\n",
      "Iteration 75, loss = 0.85345673\n",
      "Iteration 76, loss = 0.85288911\n",
      "Iteration 77, loss = 0.85219082\n",
      "Iteration 78, loss = 0.85287460\n",
      "Iteration 79, loss = 0.85221575\n",
      "Iteration 80, loss = 0.85156543\n",
      "Iteration 81, loss = 0.85116230\n",
      "Iteration 82, loss = 0.85042841\n",
      "Iteration 83, loss = 0.85026141\n",
      "Iteration 84, loss = 0.84952022\n",
      "Iteration 85, loss = 0.84932445\n",
      "Iteration 86, loss = 0.84922138\n",
      "Iteration 87, loss = 0.84873487\n",
      "Iteration 88, loss = 0.84800806\n",
      "Iteration 89, loss = 0.84733014\n",
      "Iteration 90, loss = 0.84641063\n",
      "Iteration 91, loss = 0.84720434\n",
      "Iteration 92, loss = 0.84683943\n",
      "Iteration 93, loss = 0.84575519\n",
      "Iteration 94, loss = 0.84645080\n",
      "Iteration 95, loss = 0.84579449\n",
      "Iteration 96, loss = 0.84502050\n",
      "Iteration 97, loss = 0.84418950\n",
      "Iteration 98, loss = 0.84476361\n",
      "Iteration 99, loss = 0.84397264\n",
      "Iteration 100, loss = 0.84341139\n",
      "Iteration 101, loss = 0.84406270\n",
      "Iteration 102, loss = 0.84368467\n",
      "Iteration 103, loss = 0.84304353\n",
      "Iteration 104, loss = 0.84252718\n",
      "Iteration 105, loss = 0.84218069\n",
      "Iteration 106, loss = 0.84153000\n",
      "Iteration 107, loss = 0.84158955\n",
      "Iteration 108, loss = 0.84079139\n",
      "Iteration 109, loss = 0.84200396\n",
      "Iteration 110, loss = 0.84045444\n",
      "Iteration 111, loss = 0.84022048\n",
      "Iteration 112, loss = 0.83932002\n",
      "Iteration 113, loss = 0.83969449\n",
      "Iteration 114, loss = 0.83956234\n",
      "Iteration 115, loss = 0.83979869\n",
      "Iteration 116, loss = 0.83875116\n",
      "Iteration 117, loss = 0.83887916\n",
      "Iteration 118, loss = 0.83838789\n",
      "Iteration 119, loss = 0.83822849\n",
      "Iteration 120, loss = 0.83736162\n",
      "Iteration 121, loss = 0.83744945\n",
      "Iteration 122, loss = 0.83766109\n",
      "Iteration 123, loss = 0.83582619\n",
      "Iteration 124, loss = 0.83698282\n",
      "Iteration 125, loss = 0.83658176\n",
      "Iteration 126, loss = 0.83576986\n",
      "Iteration 127, loss = 0.83572987\n",
      "Iteration 128, loss = 0.83574961\n",
      "Iteration 129, loss = 0.83509604\n",
      "Iteration 130, loss = 0.83497293\n",
      "Iteration 131, loss = 0.83531862\n",
      "Iteration 132, loss = 0.83518918\n",
      "Iteration 133, loss = 0.83474340\n",
      "Iteration 134, loss = 0.83466105\n",
      "Iteration 135, loss = 0.83349415\n",
      "Iteration 136, loss = 0.83379815\n",
      "Iteration 137, loss = 0.83442883\n",
      "Iteration 138, loss = 0.83380746\n",
      "Iteration 139, loss = 0.83309223\n",
      "Iteration 140, loss = 0.83279248\n",
      "Iteration 141, loss = 0.83304159\n",
      "Iteration 142, loss = 0.83273346\n",
      "Iteration 143, loss = 0.83299592\n",
      "Iteration 144, loss = 0.83166723\n",
      "Iteration 145, loss = 0.83243734\n",
      "Iteration 146, loss = 0.83187869\n",
      "Iteration 147, loss = 0.83068519\n",
      "Iteration 148, loss = 0.83133341\n",
      "Iteration 149, loss = 0.83002379\n",
      "Iteration 150, loss = 0.83035694\n",
      "Iteration 151, loss = 0.83020087\n",
      "Iteration 152, loss = 0.83017605\n",
      "Iteration 153, loss = 0.83028689\n",
      "Iteration 154, loss = 0.82991235\n",
      "Iteration 155, loss = 0.82991297\n",
      "Iteration 156, loss = 0.82928859\n",
      "Iteration 157, loss = 0.82926566\n",
      "Iteration 158, loss = 0.82947647\n",
      "Iteration 159, loss = 0.82870436\n",
      "Iteration 160, loss = 0.82826341\n",
      "Iteration 161, loss = 0.82848908\n",
      "Iteration 162, loss = 0.82799118\n",
      "Iteration 163, loss = 0.82895719\n",
      "Iteration 164, loss = 0.82804278\n",
      "Iteration 165, loss = 0.82841128\n",
      "Iteration 166, loss = 0.82662139\n",
      "Iteration 167, loss = 0.82749549\n",
      "Iteration 168, loss = 0.82756644\n",
      "Iteration 169, loss = 0.82748889\n",
      "Iteration 170, loss = 0.82724991\n",
      "Iteration 171, loss = 0.82773686\n",
      "Iteration 172, loss = 0.82645995\n",
      "Iteration 173, loss = 0.82708502\n",
      "Iteration 174, loss = 0.82593208\n",
      "Iteration 175, loss = 0.82557891\n",
      "Iteration 176, loss = 0.82600623\n",
      "Iteration 177, loss = 0.82561031\n",
      "Iteration 178, loss = 0.82547358\n",
      "Iteration 179, loss = 0.82589943\n",
      "Iteration 180, loss = 0.82586931\n",
      "Iteration 181, loss = 0.82580051\n",
      "Iteration 182, loss = 0.82533956\n",
      "Iteration 183, loss = 0.82461239\n",
      "Iteration 184, loss = 0.82309384\n",
      "Iteration 185, loss = 0.82436720\n",
      "Iteration 186, loss = 0.82352138\n",
      "Iteration 187, loss = 0.82438923\n",
      "Iteration 188, loss = 0.82411222\n",
      "Iteration 189, loss = 0.82373279\n",
      "Iteration 190, loss = 0.82389414\n",
      "Iteration 191, loss = 0.82362389\n",
      "Iteration 192, loss = 0.82376352\n",
      "Iteration 193, loss = 0.82241172\n",
      "Iteration 194, loss = 0.82242853\n",
      "Iteration 195, loss = 0.82263976\n",
      "Iteration 196, loss = 0.82266481\n",
      "Iteration 197, loss = 0.82269948\n",
      "Iteration 198, loss = 0.82266409\n",
      "Iteration 199, loss = 0.82203791\n",
      "Iteration 200, loss = 0.82264645\n",
      "Iteration 201, loss = 0.82103177\n",
      "Iteration 202, loss = 0.82249538\n",
      "Iteration 203, loss = 0.82129292\n",
      "Iteration 204, loss = 0.82175302\n",
      "Iteration 205, loss = 0.82130527\n",
      "Iteration 206, loss = 0.82038640\n",
      "Iteration 207, loss = 0.82079571\n",
      "Iteration 208, loss = 0.82072542\n",
      "Iteration 209, loss = 0.82105667\n",
      "Iteration 210, loss = 0.82026170\n",
      "Iteration 211, loss = 0.82067957\n",
      "Iteration 212, loss = 0.81926385\n",
      "Iteration 213, loss = 0.82031804\n",
      "Iteration 214, loss = 0.82023113\n",
      "Iteration 215, loss = 0.82043442\n",
      "Iteration 216, loss = 0.81904694\n",
      "Iteration 217, loss = 0.81984987\n",
      "Iteration 218, loss = 0.81991343\n",
      "Iteration 219, loss = 0.81898055\n",
      "Iteration 220, loss = 0.81967993\n",
      "Iteration 221, loss = 0.81867397\n",
      "Iteration 222, loss = 0.81890681\n",
      "Iteration 223, loss = 0.81894516\n",
      "Iteration 224, loss = 0.81844741\n",
      "Iteration 225, loss = 0.81989899\n",
      "Iteration 226, loss = 0.81916075\n",
      "Iteration 227, loss = 0.81810257\n",
      "Iteration 228, loss = 0.81882282\n",
      "Iteration 229, loss = 0.81790080\n",
      "Iteration 230, loss = 0.81733695\n",
      "Iteration 231, loss = 0.81710761\n",
      "Iteration 232, loss = 0.81812758\n",
      "Iteration 233, loss = 0.81694238\n",
      "Iteration 234, loss = 0.81752355\n",
      "Iteration 235, loss = 0.81694520\n",
      "Iteration 236, loss = 0.81777146\n",
      "Iteration 237, loss = 0.81705734\n",
      "Iteration 238, loss = 0.81683760\n",
      "Iteration 239, loss = 0.81717239\n",
      "Iteration 240, loss = 0.81711205\n",
      "Iteration 241, loss = 0.81663070\n",
      "Iteration 242, loss = 0.81673175\n",
      "Iteration 243, loss = 0.81704742\n",
      "Iteration 244, loss = 0.81571913\n",
      "Iteration 245, loss = 0.81596960\n",
      "Iteration 246, loss = 0.81547159\n",
      "Iteration 247, loss = 0.81550930\n",
      "Iteration 248, loss = 0.81621196\n",
      "Iteration 249, loss = 0.81545484\n",
      "Iteration 250, loss = 0.81622156\n",
      "Iteration 251, loss = 0.81571569\n",
      "Iteration 252, loss = 0.81571463\n",
      "Iteration 253, loss = 0.81568614\n",
      "Iteration 254, loss = 0.81491381\n",
      "Iteration 255, loss = 0.81551391\n",
      "Iteration 256, loss = 0.81536392\n",
      "Iteration 257, loss = 0.81445327\n",
      "Iteration 258, loss = 0.81526686\n",
      "Iteration 259, loss = 0.81449326\n",
      "Iteration 260, loss = 0.81531063\n",
      "Iteration 261, loss = 0.81582935\n",
      "Iteration 262, loss = 0.81404529\n",
      "Iteration 263, loss = 0.81445353\n",
      "Iteration 264, loss = 0.81442386\n",
      "Iteration 265, loss = 0.81421969\n",
      "Iteration 266, loss = 0.81383662\n",
      "Iteration 267, loss = 0.81353825\n",
      "Iteration 268, loss = 0.81305631\n",
      "Iteration 269, loss = 0.81379122\n",
      "Iteration 270, loss = 0.81306601\n",
      "Iteration 271, loss = 0.81420257\n",
      "Iteration 272, loss = 0.81281228\n",
      "Iteration 273, loss = 0.81281045\n",
      "Iteration 274, loss = 0.81338595\n",
      "Iteration 275, loss = 0.81318721\n",
      "Iteration 276, loss = 0.81356929\n",
      "Iteration 277, loss = 0.81391658\n",
      "Iteration 278, loss = 0.81266954\n",
      "Iteration 279, loss = 0.81377497\n",
      "Iteration 280, loss = 0.81272077\n",
      "Iteration 281, loss = 0.81217737\n",
      "Iteration 282, loss = 0.81354664\n",
      "Iteration 283, loss = 0.81226128\n",
      "Iteration 284, loss = 0.81327555\n",
      "Iteration 285, loss = 0.81281392\n",
      "Iteration 286, loss = 0.81323614\n",
      "Iteration 287, loss = 0.81240700\n",
      "Iteration 288, loss = 0.81119628\n",
      "Iteration 289, loss = 0.81186186\n",
      "Iteration 290, loss = 0.81202229\n",
      "Iteration 291, loss = 0.81177988\n",
      "Iteration 292, loss = 0.81125715\n",
      "Iteration 293, loss = 0.81124843\n",
      "Iteration 294, loss = 0.81228934\n",
      "Iteration 295, loss = 0.81167785\n",
      "Iteration 296, loss = 0.81189714\n",
      "Iteration 297, loss = 0.81090395\n",
      "Iteration 298, loss = 0.81146055\n",
      "Iteration 299, loss = 0.81128289\n",
      "Iteration 300, loss = 0.81202858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiergallo/miniconda3/envs/diplodatos/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Original:\n",
    "model_param = {\n",
    "    'criterion': ('gini', 'entropy'),\n",
    "    'min_samples_leaf': (1, 2, 5),\n",
    "    'min_samples_split': (2, 3, 5, 10, 50, 100)\n",
    "}\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Best models so far (from better to worse):\n",
    "\n",
    "MLPClassifier(\n",
    "    alpha=0.03,\n",
    "    beta_1=0.8,\n",
    "    beta_2=0.99999,\n",
    "    hidden_layer_sizes=(96,),\n",
    "    max_iter=300,\n",
    "    n_iter_no_change=30,\n",
    "    random_state=42\n",
    ")\n",
    "MLPClassifier(\n",
    "    alpha=0.03,\n",
    "    hidden_layer_sizes=(96,),\n",
    "    max_iter=300,\n",
    "    n_iter_no_change=30,\n",
    "    random_state=42\n",
    ")\n",
    "MLPClassifier(\n",
    "    alpha=0.02,\n",
    "    hidden_layer_sizes=(64,),\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=101,\n",
    "    max_depth=59,\n",
    "    class_weight={999: 0.49}\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "model_params = {\n",
    "    'beta_1': (0.8,),\n",
    "    'beta_2': (0.99999,),\n",
    "}\n",
    "model = MLPClassifier(random_state=42,\n",
    "                      verbose=True,\n",
    "                      max_iter=300,\n",
    "                      alpha=0.03,\n",
    "                      n_iter_no_change=30,\n",
    "                      hidden_layer_sizes=(96,))\n",
    "\n",
    "search = GridSearchCV(model,\n",
    "                      model_params,\n",
    "                      cv=3,\n",
    "                      # n_jobs=-1,  # Use all processors.\n",
    "                      scoring='accuracy',\n",
    "                      verbose=1)  # scoring='balanced_accuracy'\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "best_found_clf = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(best_found_clf.loss_curve_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Best classifier found**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(alpha=0.03, beta_1=0.8, beta_2=0.99999, hidden_layer_sizes=(96,),\n",
      "              max_iter=300, n_iter_no_change=30, random_state=42, verbose=True)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Accuracy**: 0.6916879795396419"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown('**Best classifier found**:')\n",
    "print(best_found_clf)\n",
    "display_markdown('**Accuracy**: {}'.format(search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.append(\n",
    "    {'clf': best_found_clf, 'best_acc': search.best_score_},\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clf</th>\n",
       "      <th>best_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLPClassifier(alpha=0.03, beta_1=0.8, beta_2=0...</td>\n",
       "      <td>0.691688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 clf  best_acc\n",
       "0  MLPClassifier(alpha=0.03, beta_1=0.8, beta_2=0...  0.691688"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**The best classifier so far is:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(alpha=0.03, beta_1=0.8, beta_2=0.99999, hidden_layer_sizes=(96,),\n",
      "              max_iter=300, n_iter_no_change=30, random_state=42, verbose=True)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Accuracy**: 0.6916879795396419"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_clf_idx = results['best_acc'].idxmax()\n",
    "best_clf = results.loc[best_clf_idx]['clf']\n",
    "best_acc = results.loc[best_clf_idx]['best_acc']\n",
    "display_markdown('**The best classifier so far is:**')\n",
    "print(best_clf)\n",
    "display_markdown('**Accuracy**: {}'.format(search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And finally**, we predict the unknown label for the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67029, 78), (28645, 78))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = best_clf.predict(XX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we do is generating a file that should be *submitted* on kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(list(zip(visit_n_test, yy)), columns=[\"VisitNumber\", \"TripType\"])\n",
    "submission.to_csv('./data/submission.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
