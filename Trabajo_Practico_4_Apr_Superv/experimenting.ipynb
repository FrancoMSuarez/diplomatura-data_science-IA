{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diplodatos Kaggle Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import CategoricalNB, GaussianNB, MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(train_data_fname, test_data_fname):\n",
    "    df_train = pd.read_csv(train_data_fname)\n",
    "    df_train['is_train_set'] = 1\n",
    "    df_test = pd.read_csv(test_data_fname)\n",
    "    df_test['is_train_set'] = 0\n",
    "\n",
    "    # we  get the TripType for the train set. To do that, we group by VisitNumber and\n",
    "    # then we get the max (or min or avg)\n",
    "    y = df_train.groupby([\"VisitNumber\", \"Weekday\"], as_index=False).max().TripType\n",
    "\n",
    "    # we remove the TripType now, and concat training and testing data\n",
    "    # the concat is done so that we have the same columns for both datasets\n",
    "    # after one-hot encoding\n",
    "    df_train = df_train.drop(\"TripType\", axis=1)\n",
    "    df = pd.concat([df_train, df_test])\n",
    "    \n",
    "    # the next three operations are the ones we have just presented in the previous lines\n",
    "    \n",
    "    # drop the columns we won't use (it may be good to use them somehow)\n",
    "    df = df.drop([\"Upc\", \"FinelineNumber\"], axis=1)\n",
    "\n",
    "    # one-hot encoding for the DepartmentDescription\n",
    "    df = pd.get_dummies(df, columns=[\"DepartmentDescription\"], dummy_na=True)\n",
    "\n",
    "    # now we add the groupby values\n",
    "    df = df.groupby([\"VisitNumber\", \"Weekday\"], as_index=False).sum()\n",
    "    \n",
    "    # finally, we do one-hot encoding for the Weekday\n",
    "    df = pd.get_dummies(df, columns=[\"Weekday\"], dummy_na=True)\n",
    "\n",
    "    # get train and test back\n",
    "    df_train = df[df.is_train_set != 0]\n",
    "    df_test = df[df.is_train_set == 0]\n",
    "    \n",
    "    X = df_train.drop([\"is_train_set\"], axis=1)\n",
    "    yy = None\n",
    "    XX = df_test.drop([\"is_train_set\"], axis=1)\n",
    "\n",
    "    return X, y, XX, yy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, XX, yy = transform_data(\"./data/train.csv\", \"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VisitNumber</th>\n",
       "      <th>ScanCount</th>\n",
       "      <th>DepartmentDescription_1-HR PHOTO</th>\n",
       "      <th>DepartmentDescription_ACCESSORIES</th>\n",
       "      <th>DepartmentDescription_AUTOMOTIVE</th>\n",
       "      <th>DepartmentDescription_BAKERY</th>\n",
       "      <th>DepartmentDescription_BATH AND SHOWER</th>\n",
       "      <th>DepartmentDescription_BEAUTY</th>\n",
       "      <th>DepartmentDescription_BEDDING</th>\n",
       "      <th>DepartmentDescription_BOOKS AND MAGAZINES</th>\n",
       "      <th>...</th>\n",
       "      <th>DepartmentDescription_WIRELESS</th>\n",
       "      <th>DepartmentDescription_nan</th>\n",
       "      <th>Weekday_Friday</th>\n",
       "      <th>Weekday_Monday</th>\n",
       "      <th>Weekday_Saturday</th>\n",
       "      <th>Weekday_Sunday</th>\n",
       "      <th>Weekday_Thursday</th>\n",
       "      <th>Weekday_Tuesday</th>\n",
       "      <th>Weekday_Wednesday</th>\n",
       "      <th>Weekday_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81954</th>\n",
       "      <td>163907</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32578</th>\n",
       "      <td>65166</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86578</th>\n",
       "      <td>173052</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85079</th>\n",
       "      <td>170137</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9788</th>\n",
       "      <td>19404</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52950</th>\n",
       "      <td>106158</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8809</th>\n",
       "      <td>17442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78302</th>\n",
       "      <td>156542</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>2404</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22577</th>\n",
       "      <td>45320</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46920 rows Ã— 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       VisitNumber  ScanCount  DepartmentDescription_1-HR PHOTO  \\\n",
       "81954       163907          2                                 0   \n",
       "32578        65166          1                                 0   \n",
       "86578       173052          5                                 0   \n",
       "85079       170137          7                                 0   \n",
       "9788         19404         32                                 0   \n",
       "...            ...        ...                               ...   \n",
       "52950       106158          1                                 0   \n",
       "8809         17442          1                                 0   \n",
       "78302       156542          5                                 0   \n",
       "1229          2404          8                                 0   \n",
       "22577        45320         19                                 0   \n",
       "\n",
       "       DepartmentDescription_ACCESSORIES  DepartmentDescription_AUTOMOTIVE  \\\n",
       "81954                                  0                                 0   \n",
       "32578                                  0                                 0   \n",
       "86578                                  0                                 0   \n",
       "85079                                  0                                 0   \n",
       "9788                                   0                                 0   \n",
       "...                                  ...                               ...   \n",
       "52950                                  0                                 0   \n",
       "8809                                   0                                 0   \n",
       "78302                                  0                                 0   \n",
       "1229                                   0                                 0   \n",
       "22577                                  0                                 0   \n",
       "\n",
       "       DepartmentDescription_BAKERY  DepartmentDescription_BATH AND SHOWER  \\\n",
       "81954                             0                                      0   \n",
       "32578                             0                                      0   \n",
       "86578                             0                                      0   \n",
       "85079                             0                                      0   \n",
       "9788                              0                                      0   \n",
       "...                             ...                                    ...   \n",
       "52950                             0                                      0   \n",
       "8809                              0                                      0   \n",
       "78302                             0                                      0   \n",
       "1229                              0                                      0   \n",
       "22577                             0                                      0   \n",
       "\n",
       "       DepartmentDescription_BEAUTY  DepartmentDescription_BEDDING  \\\n",
       "81954                             0                              0   \n",
       "32578                             0                              0   \n",
       "86578                             0                              0   \n",
       "85079                             0                              0   \n",
       "9788                              0                              0   \n",
       "...                             ...                            ...   \n",
       "52950                             0                              0   \n",
       "8809                              0                              0   \n",
       "78302                             0                              0   \n",
       "1229                              0                              0   \n",
       "22577                             0                              0   \n",
       "\n",
       "       DepartmentDescription_BOOKS AND MAGAZINES  ...  \\\n",
       "81954                                          0  ...   \n",
       "32578                                          0  ...   \n",
       "86578                                          0  ...   \n",
       "85079                                          0  ...   \n",
       "9788                                           0  ...   \n",
       "...                                          ...  ...   \n",
       "52950                                          0  ...   \n",
       "8809                                           0  ...   \n",
       "78302                                          0  ...   \n",
       "1229                                           0  ...   \n",
       "22577                                          0  ...   \n",
       "\n",
       "       DepartmentDescription_WIRELESS  DepartmentDescription_nan  \\\n",
       "81954                               0                          0   \n",
       "32578                               0                          0   \n",
       "86578                               0                          0   \n",
       "85079                               0                          0   \n",
       "9788                                0                          0   \n",
       "...                               ...                        ...   \n",
       "52950                               0                          0   \n",
       "8809                                0                          0   \n",
       "78302                               0                          0   \n",
       "1229                                0                          0   \n",
       "22577                               0                          0   \n",
       "\n",
       "       Weekday_Friday  Weekday_Monday  Weekday_Saturday  Weekday_Sunday  \\\n",
       "81954               0               0                 0               0   \n",
       "32578               0               1                 0               0   \n",
       "86578               1               0                 0               0   \n",
       "85079               0               0                 0               0   \n",
       "9788                0               0                 0               1   \n",
       "...               ...             ...               ...             ...   \n",
       "52950               0               0                 0               1   \n",
       "8809                0               0                 0               1   \n",
       "78302               0               0                 0               0   \n",
       "1229                1               0                 0               0   \n",
       "22577               1               0                 0               0   \n",
       "\n",
       "       Weekday_Thursday  Weekday_Tuesday  Weekday_Wednesday  Weekday_nan  \n",
       "81954                 0                0                  1            0  \n",
       "32578                 0                0                  0            0  \n",
       "86578                 0                0                  0            0  \n",
       "85079                 1                0                  0            0  \n",
       "9788                  0                0                  0            0  \n",
       "...                 ...              ...                ...          ...  \n",
       "52950                 0                0                  0            0  \n",
       "8809                  0                0                  0            0  \n",
       "78302                 0                1                  0            0  \n",
       "1229                  0                0                  0            0  \n",
       "22577                 0                0                  0            0  \n",
       "\n",
       "[46920 rows x 79 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split training dataset into train and \"validation\" \n",
    "# (we won't be using validation set in this example, because of the cross-validation;\n",
    "# but it could be useful for you depending on your approach)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results dataframe is used to store the computed results\n",
    "results = pd.DataFrame(columns=('clf', 'best_acc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.64678491\n",
      "Iteration 2, loss = 1.52249391\n",
      "Iteration 3, loss = 1.25002378\n",
      "Iteration 4, loss = 1.15240720\n",
      "Iteration 5, loss = 1.10134120\n",
      "Iteration 6, loss = 1.06640185\n",
      "Iteration 7, loss = 1.04016634\n",
      "Iteration 8, loss = 1.02234745\n",
      "Iteration 9, loss = 1.00483227\n",
      "Iteration 10, loss = 0.99190553\n",
      "Iteration 11, loss = 0.97980417\n",
      "Iteration 12, loss = 0.96981355\n",
      "Iteration 13, loss = 0.96115705\n",
      "Iteration 14, loss = 0.95219038\n",
      "Iteration 15, loss = 0.94573773\n",
      "Iteration 16, loss = 0.93911502\n",
      "Iteration 17, loss = 0.93142012\n",
      "Iteration 18, loss = 0.92576354\n",
      "Iteration 19, loss = 0.92098344\n",
      "Iteration 20, loss = 0.91574826\n",
      "Iteration 21, loss = 0.91091674\n",
      "Iteration 22, loss = 0.90511703\n",
      "Iteration 23, loss = 0.90086184\n",
      "Iteration 24, loss = 0.89763433\n",
      "Iteration 25, loss = 0.89300720\n",
      "Iteration 26, loss = 0.89038250\n",
      "Iteration 27, loss = 0.88723421\n",
      "Iteration 28, loss = 0.88525967\n",
      "Iteration 29, loss = 0.88041988\n",
      "Iteration 30, loss = 0.87724221\n",
      "Iteration 31, loss = 0.87373057\n",
      "Iteration 32, loss = 0.87225547\n",
      "Iteration 33, loss = 0.87035424\n",
      "Iteration 34, loss = 0.86712513\n",
      "Iteration 35, loss = 0.86453444\n",
      "Iteration 36, loss = 0.86185591\n",
      "Iteration 37, loss = 0.85955445\n",
      "Iteration 38, loss = 0.85896240\n",
      "Iteration 39, loss = 0.85677180\n",
      "Iteration 40, loss = 0.85377665\n",
      "Iteration 41, loss = 0.85085088\n",
      "Iteration 42, loss = 0.84954478\n",
      "Iteration 43, loss = 0.84629808\n",
      "Iteration 44, loss = 0.84486013\n",
      "Iteration 45, loss = 0.84355941\n",
      "Iteration 46, loss = 0.84185747\n",
      "Iteration 47, loss = 0.84095148\n",
      "Iteration 48, loss = 0.83954118\n",
      "Iteration 49, loss = 0.83728576\n",
      "Iteration 50, loss = 0.83639461\n",
      "Iteration 51, loss = 0.83565456\n",
      "Iteration 52, loss = 0.83264940\n",
      "Iteration 53, loss = 0.83132196\n",
      "Iteration 54, loss = 0.82962939\n",
      "Iteration 55, loss = 0.82842517\n",
      "Iteration 56, loss = 0.82807591\n",
      "Iteration 57, loss = 0.82578568\n",
      "Iteration 58, loss = 0.82555077\n",
      "Iteration 59, loss = 0.82319305\n",
      "Iteration 60, loss = 0.82431480\n",
      "Iteration 61, loss = 0.82250555\n",
      "Iteration 62, loss = 0.82012832\n",
      "Iteration 63, loss = 0.82016193\n",
      "Iteration 64, loss = 0.81916883\n",
      "Iteration 65, loss = 0.81750494\n",
      "Iteration 66, loss = 0.81594792\n",
      "Iteration 67, loss = 0.81467700\n",
      "Iteration 68, loss = 0.81441211\n",
      "Iteration 69, loss = 0.81209755\n",
      "Iteration 70, loss = 0.81249991\n",
      "Iteration 71, loss = 0.81113606\n",
      "Iteration 72, loss = 0.81035745\n",
      "Iteration 73, loss = 0.80937362\n",
      "Iteration 74, loss = 0.80943801\n",
      "Iteration 75, loss = 0.80756854\n",
      "Iteration 76, loss = 0.80657530\n",
      "Iteration 77, loss = 0.80599669\n",
      "Iteration 78, loss = 0.80526321\n",
      "Iteration 79, loss = 0.80417039\n",
      "Iteration 80, loss = 0.80264156\n",
      "Iteration 81, loss = 0.80307521\n",
      "Iteration 82, loss = 0.80175040\n",
      "Iteration 83, loss = 0.80151410\n",
      "Iteration 84, loss = 0.80059710\n",
      "Iteration 85, loss = 0.79998463\n",
      "Iteration 86, loss = 0.79903357\n",
      "Iteration 87, loss = 0.79800210\n",
      "Iteration 88, loss = 0.79715374\n",
      "Iteration 89, loss = 0.79610394\n",
      "Iteration 90, loss = 0.79622188\n",
      "Iteration 91, loss = 0.79547573\n",
      "Iteration 92, loss = 0.79383685\n",
      "Iteration 93, loss = 0.79356585\n",
      "Iteration 94, loss = 0.79337705\n",
      "Iteration 95, loss = 0.79378935\n",
      "Iteration 96, loss = 0.79173176\n",
      "Iteration 97, loss = 0.79176670\n",
      "Iteration 98, loss = 0.79151826\n",
      "Iteration 99, loss = 0.79044284\n",
      "Iteration 100, loss = 0.78972544\n",
      "Iteration 101, loss = 0.78961399\n",
      "Iteration 102, loss = 0.78911453\n",
      "Iteration 103, loss = 0.78930085\n",
      "Iteration 104, loss = 0.78730923\n",
      "Iteration 105, loss = 0.78686340\n",
      "Iteration 106, loss = 0.78647841\n",
      "Iteration 107, loss = 0.78624518\n",
      "Iteration 108, loss = 0.78560053\n",
      "Iteration 109, loss = 0.78517432\n",
      "Iteration 110, loss = 0.78482610\n",
      "Iteration 111, loss = 0.78376458\n",
      "Iteration 112, loss = 0.78287137\n",
      "Iteration 113, loss = 0.78348963\n",
      "Iteration 114, loss = 0.78154107\n",
      "Iteration 115, loss = 0.78275231\n",
      "Iteration 116, loss = 0.78160320\n",
      "Iteration 117, loss = 0.78089898\n",
      "Iteration 118, loss = 0.78125334\n",
      "Iteration 119, loss = 0.78095839\n",
      "Iteration 120, loss = 0.77960681\n",
      "Iteration 121, loss = 0.78094381\n",
      "Iteration 122, loss = 0.78048567\n",
      "Iteration 123, loss = 0.77900760\n",
      "Iteration 124, loss = 0.77821766\n",
      "Iteration 125, loss = 0.77780154\n",
      "Iteration 126, loss = 0.77803309\n",
      "Iteration 127, loss = 0.77698226\n",
      "Iteration 128, loss = 0.77629222\n",
      "Iteration 129, loss = 0.77566361\n",
      "Iteration 130, loss = 0.77546074\n",
      "Iteration 131, loss = 0.77560666\n",
      "Iteration 132, loss = 0.77639599\n",
      "Iteration 133, loss = 0.77442299\n",
      "Iteration 134, loss = 0.77446879\n",
      "Iteration 135, loss = 0.77467845\n",
      "Iteration 136, loss = 0.77397520\n",
      "Iteration 137, loss = 0.77537335\n",
      "Iteration 138, loss = 0.77251417\n",
      "Iteration 139, loss = 0.77287769\n",
      "Iteration 140, loss = 0.77321164\n",
      "Iteration 141, loss = 0.77296779\n",
      "Iteration 142, loss = 0.77128659\n",
      "Iteration 143, loss = 0.77168499\n",
      "Iteration 144, loss = 0.77148219\n",
      "Iteration 145, loss = 0.77143736\n",
      "Iteration 146, loss = 0.77063036\n",
      "Iteration 147, loss = 0.76957219\n",
      "Iteration 148, loss = 0.77003706\n",
      "Iteration 149, loss = 0.76997722\n",
      "Iteration 150, loss = 0.76930242\n",
      "Iteration 151, loss = 0.76946312\n",
      "Iteration 152, loss = 0.76882649\n",
      "Iteration 153, loss = 0.76901749\n",
      "Iteration 154, loss = 0.76855944\n",
      "Iteration 155, loss = 0.76795750\n",
      "Iteration 156, loss = 0.76797238\n",
      "Iteration 157, loss = 0.76680864\n",
      "Iteration 158, loss = 0.76682995\n",
      "Iteration 159, loss = 0.76823104\n",
      "Iteration 160, loss = 0.76723296\n",
      "Iteration 161, loss = 0.76610146\n",
      "Iteration 162, loss = 0.76648176\n",
      "Iteration 163, loss = 0.76516316\n",
      "Iteration 164, loss = 0.76570475\n",
      "Iteration 165, loss = 0.76537837\n",
      "Iteration 166, loss = 0.76551412\n",
      "Iteration 167, loss = 0.76439695\n",
      "Iteration 168, loss = 0.76495787\n",
      "Iteration 169, loss = 0.76312362\n",
      "Iteration 170, loss = 0.76380161\n",
      "Iteration 171, loss = 0.76377772\n",
      "Iteration 172, loss = 0.76286120\n",
      "Iteration 173, loss = 0.76398681\n",
      "Iteration 174, loss = 0.76252108\n",
      "Iteration 175, loss = 0.76319551\n",
      "Iteration 176, loss = 0.76283620\n",
      "Iteration 177, loss = 0.76344221\n",
      "Iteration 178, loss = 0.76089065\n",
      "Iteration 179, loss = 0.76165049\n",
      "Iteration 180, loss = 0.76138390\n",
      "Iteration 181, loss = 0.76060666\n",
      "Iteration 182, loss = 0.76048853\n",
      "Iteration 183, loss = 0.76117338\n",
      "Iteration 184, loss = 0.76033383\n",
      "Iteration 185, loss = 0.76142273\n",
      "Iteration 186, loss = 0.76024350\n",
      "Iteration 187, loss = 0.76011159\n",
      "Iteration 188, loss = 0.76055783\n",
      "Iteration 189, loss = 0.75939473\n",
      "Iteration 190, loss = 0.76007750\n",
      "Iteration 191, loss = 0.75936587\n",
      "Iteration 192, loss = 0.75852979\n",
      "Iteration 193, loss = 0.75914358\n",
      "Iteration 194, loss = 0.75797625\n",
      "Iteration 195, loss = 0.75855128\n",
      "Iteration 196, loss = 0.75852953\n",
      "Iteration 197, loss = 0.75872152\n",
      "Iteration 198, loss = 0.75729741\n",
      "Iteration 199, loss = 0.75660008\n",
      "Iteration 200, loss = 0.75694840\n",
      "Iteration 201, loss = 0.75643821\n",
      "Iteration 202, loss = 0.75662508\n",
      "Iteration 203, loss = 0.75632638\n",
      "Iteration 204, loss = 0.75615866\n",
      "Iteration 205, loss = 0.75566420\n",
      "Iteration 206, loss = 0.75510941\n",
      "Iteration 207, loss = 0.75562418\n",
      "Iteration 208, loss = 0.75527421\n",
      "Iteration 209, loss = 0.75501413\n",
      "Iteration 210, loss = 0.75659331\n",
      "Iteration 211, loss = 0.75535243\n",
      "Iteration 212, loss = 0.75484417\n",
      "Iteration 213, loss = 0.75466227\n",
      "Iteration 214, loss = 0.75351388\n",
      "Iteration 215, loss = 0.75418113\n",
      "Iteration 216, loss = 0.75447697\n",
      "Iteration 217, loss = 0.75336161\n",
      "Iteration 218, loss = 0.75400968\n",
      "Iteration 219, loss = 0.75351947\n",
      "Iteration 220, loss = 0.75312966\n",
      "Iteration 221, loss = 0.75447169\n",
      "Iteration 222, loss = 0.75279135\n",
      "Iteration 223, loss = 0.75381302\n",
      "Iteration 224, loss = 0.75336866\n",
      "Iteration 225, loss = 0.75175002\n",
      "Iteration 226, loss = 0.75206736\n",
      "Iteration 227, loss = 0.75179411\n",
      "Iteration 228, loss = 0.75221747\n",
      "Iteration 229, loss = 0.75173064\n",
      "Iteration 230, loss = 0.75161204\n",
      "Iteration 231, loss = 0.75153945\n",
      "Iteration 232, loss = 0.75251762\n",
      "Iteration 233, loss = 0.75127324\n",
      "Iteration 234, loss = 0.75117287\n",
      "Iteration 235, loss = 0.75130555\n",
      "Iteration 236, loss = 0.74940244\n",
      "Iteration 237, loss = 0.75156310\n",
      "Iteration 238, loss = 0.75013072\n",
      "Iteration 239, loss = 0.75114583\n",
      "Iteration 240, loss = 0.75144163\n",
      "Iteration 241, loss = 0.74934784\n",
      "Iteration 242, loss = 0.74958305\n",
      "Iteration 243, loss = 0.74975567\n",
      "Iteration 244, loss = 0.74922787\n",
      "Iteration 245, loss = 0.74973056\n",
      "Iteration 246, loss = 0.74872220\n",
      "Iteration 247, loss = 0.74924999\n",
      "Iteration 248, loss = 0.75010682\n",
      "Iteration 249, loss = 0.74783666\n",
      "Iteration 250, loss = 0.74778746\n",
      "Iteration 251, loss = 0.74734584\n",
      "Iteration 252, loss = 0.74663716\n",
      "Iteration 253, loss = 0.74756329\n",
      "Iteration 254, loss = 0.74781564\n",
      "Iteration 255, loss = 0.74706842\n",
      "Iteration 256, loss = 0.74714346\n",
      "Iteration 257, loss = 0.74745131\n",
      "Iteration 258, loss = 0.74700624\n",
      "Iteration 259, loss = 0.74688748\n",
      "Iteration 260, loss = 0.74634218\n",
      "Iteration 261, loss = 0.74648550\n",
      "Iteration 262, loss = 0.74593267\n",
      "Iteration 263, loss = 0.74642012\n",
      "Iteration 264, loss = 0.74757877\n",
      "Iteration 265, loss = 0.74589752\n",
      "Iteration 266, loss = 0.74587933\n",
      "Iteration 267, loss = 0.74627245\n",
      "Iteration 268, loss = 0.74519582\n",
      "Iteration 269, loss = 0.74546037\n",
      "Iteration 270, loss = 0.74456892\n",
      "Iteration 271, loss = 0.74463330\n",
      "Iteration 272, loss = 0.74564241\n",
      "Iteration 273, loss = 0.74614141\n",
      "Iteration 274, loss = 0.74590033\n",
      "Iteration 275, loss = 0.74470325\n",
      "Iteration 276, loss = 0.74474376\n",
      "Iteration 277, loss = 0.74389235\n",
      "Iteration 278, loss = 0.74389472\n",
      "Iteration 279, loss = 0.74393750\n",
      "Iteration 280, loss = 0.74348479\n",
      "Iteration 281, loss = 0.74330076\n",
      "Iteration 282, loss = 0.74278674\n",
      "Iteration 283, loss = 0.74333452\n",
      "Iteration 284, loss = 0.74313744\n",
      "Iteration 285, loss = 0.74240998\n",
      "Iteration 286, loss = 0.74245305\n",
      "Iteration 287, loss = 0.74327899\n",
      "Iteration 288, loss = 0.74337044\n",
      "Iteration 289, loss = 0.74235213\n",
      "Iteration 290, loss = 0.74336001\n",
      "Iteration 291, loss = 0.74212514\n",
      "Iteration 292, loss = 0.74187750\n",
      "Iteration 293, loss = 0.74264144\n",
      "Iteration 294, loss = 0.74200790\n",
      "Iteration 295, loss = 0.74213606\n",
      "Iteration 296, loss = 0.74269445\n",
      "Iteration 297, loss = 0.74194955\n",
      "Iteration 298, loss = 0.74111271\n",
      "Iteration 299, loss = 0.74127385\n",
      "Iteration 300, loss = 0.74074961\n",
      "Iteration 1, loss = 2.64789726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiergallo/miniconda3/envs/diplodatos/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.52370257\n",
      "Iteration 3, loss = 1.25849906\n",
      "Iteration 4, loss = 1.15986531\n",
      "Iteration 5, loss = 1.10779738\n",
      "Iteration 6, loss = 1.07327832\n",
      "Iteration 7, loss = 1.04511953\n",
      "Iteration 8, loss = 1.02593724\n",
      "Iteration 9, loss = 1.00725642\n",
      "Iteration 10, loss = 0.99493582\n",
      "Iteration 11, loss = 0.98207310\n",
      "Iteration 12, loss = 0.97169291\n",
      "Iteration 13, loss = 0.96170225\n",
      "Iteration 14, loss = 0.95301906\n",
      "Iteration 15, loss = 0.94568219\n",
      "Iteration 16, loss = 0.93876658\n",
      "Iteration 17, loss = 0.93195829\n",
      "Iteration 18, loss = 0.92642300\n",
      "Iteration 19, loss = 0.92144374\n",
      "Iteration 20, loss = 0.91521536\n",
      "Iteration 21, loss = 0.91184347\n",
      "Iteration 22, loss = 0.90656117\n",
      "Iteration 23, loss = 0.90272134\n",
      "Iteration 24, loss = 0.89992510\n",
      "Iteration 25, loss = 0.89490605\n",
      "Iteration 26, loss = 0.89131546\n",
      "Iteration 27, loss = 0.88804385\n",
      "Iteration 28, loss = 0.88579580\n",
      "Iteration 29, loss = 0.88247492\n",
      "Iteration 30, loss = 0.87872466\n",
      "Iteration 31, loss = 0.87622254\n",
      "Iteration 32, loss = 0.87432970\n",
      "Iteration 33, loss = 0.87224507\n",
      "Iteration 34, loss = 0.87026729\n",
      "Iteration 35, loss = 0.86599003\n",
      "Iteration 36, loss = 0.86431799\n",
      "Iteration 37, loss = 0.86275821\n",
      "Iteration 38, loss = 0.86169232\n",
      "Iteration 39, loss = 0.85929666\n",
      "Iteration 40, loss = 0.85822372\n",
      "Iteration 41, loss = 0.85469677\n",
      "Iteration 42, loss = 0.85379507\n",
      "Iteration 43, loss = 0.85229449\n",
      "Iteration 44, loss = 0.85075230\n",
      "Iteration 45, loss = 0.84851198\n",
      "Iteration 46, loss = 0.84709839\n",
      "Iteration 47, loss = 0.84534780\n",
      "Iteration 48, loss = 0.84492497\n",
      "Iteration 49, loss = 0.84314327\n",
      "Iteration 50, loss = 0.84332033\n",
      "Iteration 51, loss = 0.83958352\n",
      "Iteration 52, loss = 0.84022926\n",
      "Iteration 53, loss = 0.83863368\n",
      "Iteration 54, loss = 0.83630025\n",
      "Iteration 55, loss = 0.83650670\n",
      "Iteration 56, loss = 0.83527425\n",
      "Iteration 57, loss = 0.83248274\n",
      "Iteration 58, loss = 0.83195229\n",
      "Iteration 59, loss = 0.83166617\n",
      "Iteration 60, loss = 0.82991037\n",
      "Iteration 61, loss = 0.83008864\n",
      "Iteration 62, loss = 0.82737825\n",
      "Iteration 63, loss = 0.82633783\n",
      "Iteration 64, loss = 0.82560022\n",
      "Iteration 65, loss = 0.82556106\n",
      "Iteration 66, loss = 0.82359890\n",
      "Iteration 67, loss = 0.82231899\n",
      "Iteration 68, loss = 0.82116800\n",
      "Iteration 69, loss = 0.82031798\n",
      "Iteration 70, loss = 0.82018112\n",
      "Iteration 71, loss = 0.81960021\n",
      "Iteration 72, loss = 0.81794759\n",
      "Iteration 73, loss = 0.81710109\n",
      "Iteration 74, loss = 0.81729355\n",
      "Iteration 75, loss = 0.81648452\n",
      "Iteration 76, loss = 0.81517032\n",
      "Iteration 77, loss = 0.81436786\n",
      "Iteration 78, loss = 0.81419155\n",
      "Iteration 79, loss = 0.81301472\n",
      "Iteration 80, loss = 0.81270446\n",
      "Iteration 81, loss = 0.81131470\n",
      "Iteration 82, loss = 0.81056608\n",
      "Iteration 83, loss = 0.80959107\n",
      "Iteration 84, loss = 0.80973871\n",
      "Iteration 85, loss = 0.80873341\n",
      "Iteration 86, loss = 0.80757950\n",
      "Iteration 87, loss = 0.80692610\n",
      "Iteration 88, loss = 0.80681649\n",
      "Iteration 89, loss = 0.80577974\n",
      "Iteration 90, loss = 0.80500224\n",
      "Iteration 91, loss = 0.80493326\n",
      "Iteration 92, loss = 0.80461955\n",
      "Iteration 93, loss = 0.80315216\n",
      "Iteration 94, loss = 0.80423615\n",
      "Iteration 95, loss = 0.80198606\n",
      "Iteration 96, loss = 0.80276200\n",
      "Iteration 97, loss = 0.80199926\n",
      "Iteration 98, loss = 0.80224870\n",
      "Iteration 99, loss = 0.80039928\n",
      "Iteration 100, loss = 0.79941534\n",
      "Iteration 101, loss = 0.80071100\n",
      "Iteration 102, loss = 0.79885719\n",
      "Iteration 103, loss = 0.79865955\n",
      "Iteration 104, loss = 0.79749071\n",
      "Iteration 105, loss = 0.79686326\n",
      "Iteration 106, loss = 0.79714063\n",
      "Iteration 107, loss = 0.79523936\n",
      "Iteration 108, loss = 0.79542286\n",
      "Iteration 109, loss = 0.79637858\n",
      "Iteration 110, loss = 0.79499766\n",
      "Iteration 111, loss = 0.79368735\n",
      "Iteration 112, loss = 0.79459986\n",
      "Iteration 113, loss = 0.79329367\n",
      "Iteration 114, loss = 0.79318609\n",
      "Iteration 115, loss = 0.79245938\n",
      "Iteration 116, loss = 0.79251796\n",
      "Iteration 117, loss = 0.79144477\n",
      "Iteration 118, loss = 0.79032243\n",
      "Iteration 119, loss = 0.79185168\n",
      "Iteration 120, loss = 0.78947967\n",
      "Iteration 121, loss = 0.78945787\n",
      "Iteration 122, loss = 0.79082487\n",
      "Iteration 123, loss = 0.78832855\n",
      "Iteration 124, loss = 0.78987717\n",
      "Iteration 125, loss = 0.78756925\n",
      "Iteration 126, loss = 0.78787373\n",
      "Iteration 127, loss = 0.78680267\n",
      "Iteration 128, loss = 0.78563736\n",
      "Iteration 129, loss = 0.78630457\n",
      "Iteration 130, loss = 0.78560762\n",
      "Iteration 131, loss = 0.78706747\n",
      "Iteration 132, loss = 0.78511852\n",
      "Iteration 133, loss = 0.78489959\n",
      "Iteration 134, loss = 0.78458745\n",
      "Iteration 135, loss = 0.78474932\n",
      "Iteration 136, loss = 0.78358805\n",
      "Iteration 137, loss = 0.78344917\n",
      "Iteration 138, loss = 0.78151890\n",
      "Iteration 139, loss = 0.78294472\n",
      "Iteration 140, loss = 0.78159629\n",
      "Iteration 141, loss = 0.78149376\n",
      "Iteration 142, loss = 0.78251576\n",
      "Iteration 143, loss = 0.78146921\n",
      "Iteration 144, loss = 0.78091454\n",
      "Iteration 145, loss = 0.78027506\n",
      "Iteration 146, loss = 0.77821212\n",
      "Iteration 147, loss = 0.77893999\n",
      "Iteration 148, loss = 0.77897648\n",
      "Iteration 149, loss = 0.77839010\n",
      "Iteration 150, loss = 0.77793234\n",
      "Iteration 151, loss = 0.77890322\n",
      "Iteration 152, loss = 0.77687909\n",
      "Iteration 153, loss = 0.77760069\n",
      "Iteration 154, loss = 0.77723636\n",
      "Iteration 155, loss = 0.77735095\n",
      "Iteration 156, loss = 0.77579854\n",
      "Iteration 157, loss = 0.77515052\n",
      "Iteration 158, loss = 0.77648261\n",
      "Iteration 159, loss = 0.77591479\n",
      "Iteration 160, loss = 0.77590897\n",
      "Iteration 161, loss = 0.77425406\n",
      "Iteration 162, loss = 0.77459814\n",
      "Iteration 163, loss = 0.77415134\n",
      "Iteration 164, loss = 0.77428629\n",
      "Iteration 165, loss = 0.77353257\n",
      "Iteration 166, loss = 0.77310486\n",
      "Iteration 167, loss = 0.77212906\n",
      "Iteration 168, loss = 0.77280562\n",
      "Iteration 169, loss = 0.77165753\n",
      "Iteration 170, loss = 0.77207003\n",
      "Iteration 171, loss = 0.77108041\n",
      "Iteration 172, loss = 0.76998348\n",
      "Iteration 173, loss = 0.77112929\n",
      "Iteration 174, loss = 0.77000623\n",
      "Iteration 175, loss = 0.77021205\n",
      "Iteration 176, loss = 0.77063209\n",
      "Iteration 177, loss = 0.76978590\n",
      "Iteration 178, loss = 0.76954422\n",
      "Iteration 179, loss = 0.76896667\n",
      "Iteration 180, loss = 0.76844614\n",
      "Iteration 181, loss = 0.76892654\n",
      "Iteration 182, loss = 0.76802278\n",
      "Iteration 183, loss = 0.76818882\n",
      "Iteration 184, loss = 0.76832379\n",
      "Iteration 185, loss = 0.76757855\n",
      "Iteration 186, loss = 0.76758237\n",
      "Iteration 187, loss = 0.76656678\n",
      "Iteration 188, loss = 0.76803540\n",
      "Iteration 189, loss = 0.76813568\n",
      "Iteration 190, loss = 0.76633007\n",
      "Iteration 191, loss = 0.76589727\n",
      "Iteration 192, loss = 0.76528931\n",
      "Iteration 193, loss = 0.76670821\n",
      "Iteration 194, loss = 0.76562479\n",
      "Iteration 195, loss = 0.76487023\n",
      "Iteration 196, loss = 0.76495075\n",
      "Iteration 197, loss = 0.76422278\n",
      "Iteration 198, loss = 0.76402155\n",
      "Iteration 199, loss = 0.76280943\n",
      "Iteration 200, loss = 0.76367715\n",
      "Iteration 201, loss = 0.76325277\n",
      "Iteration 202, loss = 0.76349434\n",
      "Iteration 203, loss = 0.76247513\n",
      "Iteration 204, loss = 0.76347210\n",
      "Iteration 205, loss = 0.76202664\n",
      "Iteration 206, loss = 0.76277727\n",
      "Iteration 207, loss = 0.76181502\n",
      "Iteration 208, loss = 0.76143204\n",
      "Iteration 209, loss = 0.76224314\n",
      "Iteration 210, loss = 0.76181704\n",
      "Iteration 211, loss = 0.76107274\n",
      "Iteration 212, loss = 0.76064780\n",
      "Iteration 213, loss = 0.76229522\n",
      "Iteration 214, loss = 0.76097637\n",
      "Iteration 215, loss = 0.76103763\n",
      "Iteration 216, loss = 0.76063532\n",
      "Iteration 217, loss = 0.75840645\n",
      "Iteration 218, loss = 0.75938194\n",
      "Iteration 219, loss = 0.75870056\n",
      "Iteration 220, loss = 0.75917652\n",
      "Iteration 221, loss = 0.75934387\n",
      "Iteration 222, loss = 0.75779282\n",
      "Iteration 223, loss = 0.75825452\n",
      "Iteration 224, loss = 0.75910003\n",
      "Iteration 225, loss = 0.75798883\n",
      "Iteration 226, loss = 0.75760422\n",
      "Iteration 227, loss = 0.75800561\n",
      "Iteration 228, loss = 0.75746448\n",
      "Iteration 229, loss = 0.75663480\n",
      "Iteration 230, loss = 0.75786687\n",
      "Iteration 231, loss = 0.75726819\n",
      "Iteration 232, loss = 0.75808095\n",
      "Iteration 233, loss = 0.75634974\n",
      "Iteration 234, loss = 0.75676490\n",
      "Iteration 235, loss = 0.75495927\n",
      "Iteration 236, loss = 0.75545094\n",
      "Iteration 237, loss = 0.75723606\n",
      "Iteration 238, loss = 0.75517295\n",
      "Iteration 239, loss = 0.75618635\n",
      "Iteration 240, loss = 0.75536216\n",
      "Iteration 241, loss = 0.75515578\n",
      "Iteration 242, loss = 0.75584392\n",
      "Iteration 243, loss = 0.75547931\n",
      "Iteration 244, loss = 0.75507978\n",
      "Iteration 245, loss = 0.75405228\n",
      "Iteration 246, loss = 0.75446886\n",
      "Iteration 247, loss = 0.75484019\n",
      "Iteration 248, loss = 0.75410282\n",
      "Iteration 249, loss = 0.75261676\n",
      "Iteration 250, loss = 0.75421975\n",
      "Iteration 251, loss = 0.75353418\n",
      "Iteration 252, loss = 0.75258853\n",
      "Iteration 253, loss = 0.75347245\n",
      "Iteration 254, loss = 0.75251621\n",
      "Iteration 255, loss = 0.75216949\n",
      "Iteration 256, loss = 0.75297944\n",
      "Iteration 257, loss = 0.75342218\n",
      "Iteration 258, loss = 0.75224040\n",
      "Iteration 259, loss = 0.75095177\n",
      "Iteration 260, loss = 0.75131410\n",
      "Iteration 261, loss = 0.75114749\n",
      "Iteration 262, loss = 0.75209624\n",
      "Iteration 263, loss = 0.75067298\n",
      "Iteration 264, loss = 0.75158881\n",
      "Iteration 265, loss = 0.75081121\n",
      "Iteration 266, loss = 0.75116554\n",
      "Iteration 267, loss = 0.75106073\n",
      "Iteration 268, loss = 0.75145917\n",
      "Iteration 269, loss = 0.75010173\n",
      "Iteration 270, loss = 0.74989991\n",
      "Iteration 271, loss = 0.75045600\n",
      "Iteration 272, loss = 0.74921443\n",
      "Iteration 273, loss = 0.75035142\n",
      "Iteration 274, loss = 0.74947404\n",
      "Iteration 275, loss = 0.74905105\n",
      "Iteration 276, loss = 0.74843216\n",
      "Iteration 277, loss = 0.74878893\n",
      "Iteration 278, loss = 0.74890081\n",
      "Iteration 279, loss = 0.74845676\n",
      "Iteration 280, loss = 0.74837635\n",
      "Iteration 281, loss = 0.74840513\n",
      "Iteration 282, loss = 0.74843901\n",
      "Iteration 283, loss = 0.74812974\n",
      "Iteration 284, loss = 0.74758693\n",
      "Iteration 285, loss = 0.74747356\n",
      "Iteration 286, loss = 0.74762760\n",
      "Iteration 287, loss = 0.74838434\n",
      "Iteration 288, loss = 0.74707834\n",
      "Iteration 289, loss = 0.74667525\n",
      "Iteration 290, loss = 0.74731631\n",
      "Iteration 291, loss = 0.74649623\n",
      "Iteration 292, loss = 0.74739285\n",
      "Iteration 293, loss = 0.74692762\n",
      "Iteration 294, loss = 0.74666105\n",
      "Iteration 295, loss = 0.74645753\n",
      "Iteration 296, loss = 0.74583709\n",
      "Iteration 297, loss = 0.74601943\n",
      "Iteration 298, loss = 0.74625442\n",
      "Iteration 299, loss = 0.74639839\n",
      "Iteration 300, loss = 0.74420086\n",
      "Iteration 1, loss = 2.64233277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiergallo/miniconda3/envs/diplodatos/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.52314244\n",
      "Iteration 3, loss = 1.25286155\n",
      "Iteration 4, loss = 1.15471672\n",
      "Iteration 5, loss = 1.09829962\n",
      "Iteration 6, loss = 1.06227865\n",
      "Iteration 7, loss = 1.03650481\n",
      "Iteration 8, loss = 1.01622374\n",
      "Iteration 9, loss = 0.99895769\n",
      "Iteration 10, loss = 0.98658810\n",
      "Iteration 11, loss = 0.97392359\n",
      "Iteration 12, loss = 0.96557719\n",
      "Iteration 13, loss = 0.95532473\n",
      "Iteration 14, loss = 0.94693293\n",
      "Iteration 15, loss = 0.93947167\n",
      "Iteration 16, loss = 0.93214560\n",
      "Iteration 17, loss = 0.92523559\n",
      "Iteration 18, loss = 0.91964124\n",
      "Iteration 19, loss = 0.91451842\n",
      "Iteration 20, loss = 0.90981585\n",
      "Iteration 21, loss = 0.90462148\n",
      "Iteration 22, loss = 0.89982537\n",
      "Iteration 23, loss = 0.89584216\n",
      "Iteration 24, loss = 0.89165468\n",
      "Iteration 25, loss = 0.88841322\n",
      "Iteration 26, loss = 0.88655738\n",
      "Iteration 27, loss = 0.88227292\n",
      "Iteration 28, loss = 0.87819707\n",
      "Iteration 29, loss = 0.87723628\n",
      "Iteration 30, loss = 0.87269610\n",
      "Iteration 31, loss = 0.87041769\n",
      "Iteration 32, loss = 0.86753779\n",
      "Iteration 33, loss = 0.86434075\n",
      "Iteration 34, loss = 0.86201552\n",
      "Iteration 35, loss = 0.86056157\n",
      "Iteration 36, loss = 0.85738617\n",
      "Iteration 37, loss = 0.85801806\n",
      "Iteration 38, loss = 0.85455109\n",
      "Iteration 39, loss = 0.85217735\n",
      "Iteration 40, loss = 0.85088637\n",
      "Iteration 41, loss = 0.84923102\n",
      "Iteration 42, loss = 0.84738084\n",
      "Iteration 43, loss = 0.84517972\n",
      "Iteration 44, loss = 0.84417003\n",
      "Iteration 45, loss = 0.84225538\n",
      "Iteration 46, loss = 0.84118285\n",
      "Iteration 47, loss = 0.84007313\n",
      "Iteration 48, loss = 0.83948456\n",
      "Iteration 49, loss = 0.83772078\n",
      "Iteration 50, loss = 0.83617651\n",
      "Iteration 51, loss = 0.83441935\n",
      "Iteration 52, loss = 0.83255692\n",
      "Iteration 53, loss = 0.83197579\n",
      "Iteration 54, loss = 0.83177046\n",
      "Iteration 55, loss = 0.83130711\n",
      "Iteration 56, loss = 0.82811501\n",
      "Iteration 57, loss = 0.82766610\n",
      "Iteration 58, loss = 0.82651025\n",
      "Iteration 59, loss = 0.82430289\n",
      "Iteration 60, loss = 0.82562227\n",
      "Iteration 61, loss = 0.82352998\n",
      "Iteration 62, loss = 0.82176012\n",
      "Iteration 63, loss = 0.82253604\n",
      "Iteration 64, loss = 0.82141305\n",
      "Iteration 65, loss = 0.81962971\n",
      "Iteration 66, loss = 0.81854239\n",
      "Iteration 67, loss = 0.81735109\n",
      "Iteration 68, loss = 0.81752368\n",
      "Iteration 69, loss = 0.81677215\n",
      "Iteration 70, loss = 0.81602275\n",
      "Iteration 71, loss = 0.81552984\n",
      "Iteration 72, loss = 0.81388044\n",
      "Iteration 73, loss = 0.81326891\n",
      "Iteration 74, loss = 0.81124416\n",
      "Iteration 75, loss = 0.81179373\n",
      "Iteration 76, loss = 0.81112978\n",
      "Iteration 77, loss = 0.81043299\n",
      "Iteration 78, loss = 0.80916600\n",
      "Iteration 79, loss = 0.80935311\n",
      "Iteration 80, loss = 0.80811718\n",
      "Iteration 81, loss = 0.80752499\n",
      "Iteration 82, loss = 0.80588913\n",
      "Iteration 83, loss = 0.80734497\n",
      "Iteration 84, loss = 0.80572814\n",
      "Iteration 85, loss = 0.80600340\n",
      "Iteration 86, loss = 0.80510041\n",
      "Iteration 87, loss = 0.80339650\n",
      "Iteration 88, loss = 0.80379174\n",
      "Iteration 89, loss = 0.80397071\n",
      "Iteration 90, loss = 0.80267173\n",
      "Iteration 91, loss = 0.80128076\n",
      "Iteration 92, loss = 0.80154803\n",
      "Iteration 93, loss = 0.80034179\n",
      "Iteration 94, loss = 0.80153828\n",
      "Iteration 95, loss = 0.79929901\n",
      "Iteration 96, loss = 0.79994654\n",
      "Iteration 97, loss = 0.79883536\n",
      "Iteration 98, loss = 0.79725248\n",
      "Iteration 99, loss = 0.79814435\n",
      "Iteration 100, loss = 0.79634968\n",
      "Iteration 101, loss = 0.79629566\n",
      "Iteration 102, loss = 0.79550885\n",
      "Iteration 103, loss = 0.79542702\n",
      "Iteration 104, loss = 0.79540687\n",
      "Iteration 105, loss = 0.79464838\n",
      "Iteration 106, loss = 0.79449128\n",
      "Iteration 107, loss = 0.79366924\n",
      "Iteration 108, loss = 0.79289044\n",
      "Iteration 109, loss = 0.79285339\n",
      "Iteration 110, loss = 0.79167162\n",
      "Iteration 111, loss = 0.79106327\n",
      "Iteration 112, loss = 0.79153872\n",
      "Iteration 113, loss = 0.79053639\n",
      "Iteration 114, loss = 0.79088675\n",
      "Iteration 115, loss = 0.79026347\n",
      "Iteration 116, loss = 0.78976352\n",
      "Iteration 117, loss = 0.78868404\n",
      "Iteration 118, loss = 0.78849072\n",
      "Iteration 119, loss = 0.78765198\n",
      "Iteration 120, loss = 0.78755457\n",
      "Iteration 121, loss = 0.78767891\n",
      "Iteration 122, loss = 0.78642847\n",
      "Iteration 123, loss = 0.78623624\n",
      "Iteration 124, loss = 0.78776359\n",
      "Iteration 125, loss = 0.78577783\n",
      "Iteration 126, loss = 0.78537690\n",
      "Iteration 127, loss = 0.78478506\n",
      "Iteration 128, loss = 0.78319975\n",
      "Iteration 129, loss = 0.78456180\n",
      "Iteration 130, loss = 0.78298437\n",
      "Iteration 131, loss = 0.78343966\n",
      "Iteration 132, loss = 0.78228345\n",
      "Iteration 133, loss = 0.78219721\n",
      "Iteration 134, loss = 0.78243706\n",
      "Iteration 135, loss = 0.78324921\n",
      "Iteration 136, loss = 0.78169400\n",
      "Iteration 137, loss = 0.78086378\n",
      "Iteration 138, loss = 0.77974495\n",
      "Iteration 139, loss = 0.78065822\n",
      "Iteration 140, loss = 0.77936179\n",
      "Iteration 141, loss = 0.77961912\n",
      "Iteration 142, loss = 0.77982659\n",
      "Iteration 143, loss = 0.77833288\n",
      "Iteration 144, loss = 0.77964089\n",
      "Iteration 145, loss = 0.77836073\n",
      "Iteration 146, loss = 0.77746048\n",
      "Iteration 147, loss = 0.77710036\n",
      "Iteration 148, loss = 0.77713030\n",
      "Iteration 149, loss = 0.77753554\n",
      "Iteration 150, loss = 0.77579184\n",
      "Iteration 151, loss = 0.77569551\n",
      "Iteration 152, loss = 0.77524766\n",
      "Iteration 153, loss = 0.77512223\n",
      "Iteration 154, loss = 0.77509436\n",
      "Iteration 155, loss = 0.77505879\n",
      "Iteration 156, loss = 0.77446572\n",
      "Iteration 157, loss = 0.77253505\n",
      "Iteration 158, loss = 0.77318460\n",
      "Iteration 159, loss = 0.77342820\n",
      "Iteration 160, loss = 0.77385895\n",
      "Iteration 161, loss = 0.77272195\n",
      "Iteration 162, loss = 0.77220523\n",
      "Iteration 163, loss = 0.77122055\n",
      "Iteration 164, loss = 0.77172001\n",
      "Iteration 165, loss = 0.77132789\n",
      "Iteration 166, loss = 0.77126345\n",
      "Iteration 167, loss = 0.77086956\n",
      "Iteration 168, loss = 0.77094058\n",
      "Iteration 169, loss = 0.77063792\n",
      "Iteration 170, loss = 0.77011755\n",
      "Iteration 171, loss = 0.76958954\n",
      "Iteration 172, loss = 0.76945179\n",
      "Iteration 173, loss = 0.76979767\n",
      "Iteration 174, loss = 0.76903580\n",
      "Iteration 175, loss = 0.76769811\n",
      "Iteration 176, loss = 0.76794246\n",
      "Iteration 177, loss = 0.76796244\n",
      "Iteration 178, loss = 0.76734863\n",
      "Iteration 179, loss = 0.76767282\n",
      "Iteration 180, loss = 0.76750269\n",
      "Iteration 181, loss = 0.76633902\n",
      "Iteration 182, loss = 0.76602208\n",
      "Iteration 183, loss = 0.76586544\n",
      "Iteration 184, loss = 0.76602422\n",
      "Iteration 185, loss = 0.76484254\n",
      "Iteration 186, loss = 0.76552007\n",
      "Iteration 187, loss = 0.76528829\n",
      "Iteration 188, loss = 0.76634958\n",
      "Iteration 189, loss = 0.76446060\n",
      "Iteration 190, loss = 0.76487398\n",
      "Iteration 191, loss = 0.76240117\n",
      "Iteration 192, loss = 0.76354158\n",
      "Iteration 193, loss = 0.76428395\n",
      "Iteration 194, loss = 0.76399401\n",
      "Iteration 195, loss = 0.76286929\n",
      "Iteration 196, loss = 0.76213108\n",
      "Iteration 197, loss = 0.76359792\n",
      "Iteration 198, loss = 0.76174254\n",
      "Iteration 199, loss = 0.76094238\n",
      "Iteration 200, loss = 0.76185432\n",
      "Iteration 201, loss = 0.76146215\n",
      "Iteration 202, loss = 0.76108819\n",
      "Iteration 203, loss = 0.76055555\n",
      "Iteration 204, loss = 0.76140378\n",
      "Iteration 205, loss = 0.76133633\n",
      "Iteration 206, loss = 0.76039064\n",
      "Iteration 207, loss = 0.76013224\n",
      "Iteration 208, loss = 0.75976306\n",
      "Iteration 209, loss = 0.76048182\n",
      "Iteration 210, loss = 0.76014827\n",
      "Iteration 211, loss = 0.75962834\n",
      "Iteration 212, loss = 0.75881474\n",
      "Iteration 213, loss = 0.75866596\n",
      "Iteration 214, loss = 0.75855386\n",
      "Iteration 215, loss = 0.75863621\n",
      "Iteration 216, loss = 0.75871471\n",
      "Iteration 217, loss = 0.75776952\n",
      "Iteration 218, loss = 0.75802985\n",
      "Iteration 219, loss = 0.75654867\n",
      "Iteration 220, loss = 0.75740438\n",
      "Iteration 221, loss = 0.75709972\n",
      "Iteration 222, loss = 0.75651816\n",
      "Iteration 223, loss = 0.75591541\n",
      "Iteration 224, loss = 0.75638201\n",
      "Iteration 225, loss = 0.75630786\n",
      "Iteration 226, loss = 0.75552797\n",
      "Iteration 227, loss = 0.75580787\n",
      "Iteration 228, loss = 0.75543241\n",
      "Iteration 229, loss = 0.75517272\n",
      "Iteration 230, loss = 0.75500658\n",
      "Iteration 231, loss = 0.75510972\n",
      "Iteration 232, loss = 0.75386681\n",
      "Iteration 233, loss = 0.75471631\n",
      "Iteration 234, loss = 0.75504871\n",
      "Iteration 235, loss = 0.75375468\n",
      "Iteration 236, loss = 0.75340171\n",
      "Iteration 237, loss = 0.75402191\n",
      "Iteration 238, loss = 0.75435684\n",
      "Iteration 239, loss = 0.75476089\n",
      "Iteration 240, loss = 0.75314248\n",
      "Iteration 241, loss = 0.75399942\n",
      "Iteration 242, loss = 0.75395241\n",
      "Iteration 243, loss = 0.75312166\n",
      "Iteration 244, loss = 0.75298109\n",
      "Iteration 245, loss = 0.75158355\n",
      "Iteration 246, loss = 0.75204661\n",
      "Iteration 247, loss = 0.75232589\n",
      "Iteration 248, loss = 0.75125865\n",
      "Iteration 249, loss = 0.75155929\n",
      "Iteration 250, loss = 0.75156581\n",
      "Iteration 251, loss = 0.75168606\n",
      "Iteration 252, loss = 0.75098167\n",
      "Iteration 253, loss = 0.75039084\n",
      "Iteration 254, loss = 0.75022141\n",
      "Iteration 255, loss = 0.75014003\n",
      "Iteration 256, loss = 0.75036137\n",
      "Iteration 257, loss = 0.75008209\n",
      "Iteration 258, loss = 0.75033184\n",
      "Iteration 259, loss = 0.74958557\n",
      "Iteration 260, loss = 0.74923635\n",
      "Iteration 261, loss = 0.74928918\n",
      "Iteration 262, loss = 0.74921849\n",
      "Iteration 263, loss = 0.74924476\n",
      "Iteration 264, loss = 0.74945684\n",
      "Iteration 265, loss = 0.74893306\n",
      "Iteration 266, loss = 0.74899532\n",
      "Iteration 267, loss = 0.74856570\n",
      "Iteration 268, loss = 0.74907715\n",
      "Iteration 269, loss = 0.74816453\n",
      "Iteration 270, loss = 0.74821956\n",
      "Iteration 271, loss = 0.74849474\n",
      "Iteration 272, loss = 0.74757115\n",
      "Iteration 273, loss = 0.74687651\n",
      "Iteration 274, loss = 0.74750393\n",
      "Iteration 275, loss = 0.74732806\n",
      "Iteration 276, loss = 0.74687698\n",
      "Iteration 277, loss = 0.74791370\n",
      "Iteration 278, loss = 0.74735909\n",
      "Iteration 279, loss = 0.74496028\n",
      "Iteration 280, loss = 0.74632053\n",
      "Iteration 281, loss = 0.74639038\n",
      "Iteration 282, loss = 0.74585911\n",
      "Iteration 283, loss = 0.74800354\n",
      "Iteration 284, loss = 0.74586714\n",
      "Iteration 285, loss = 0.74523357\n",
      "Iteration 286, loss = 0.74581076\n",
      "Iteration 287, loss = 0.74459645\n",
      "Iteration 288, loss = 0.74447289\n",
      "Iteration 289, loss = 0.74462239\n",
      "Iteration 290, loss = 0.74409414\n",
      "Iteration 291, loss = 0.74383364\n",
      "Iteration 292, loss = 0.74529320\n",
      "Iteration 293, loss = 0.74403278\n",
      "Iteration 294, loss = 0.74299173\n",
      "Iteration 295, loss = 0.74370410\n",
      "Iteration 296, loss = 0.74348560\n",
      "Iteration 297, loss = 0.74409454\n",
      "Iteration 298, loss = 0.74471504\n",
      "Iteration 299, loss = 0.74382639\n",
      "Iteration 300, loss = 0.74195824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiergallo/miniconda3/envs/diplodatos/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31435926\n",
      "Iteration 2, loss = 1.31313205\n",
      "Iteration 3, loss = 1.15130124\n",
      "Iteration 4, loss = 1.08647985\n",
      "Iteration 5, loss = 1.04743943\n",
      "Iteration 6, loss = 1.02160805\n",
      "Iteration 7, loss = 1.00305308\n",
      "Iteration 8, loss = 0.98778206\n",
      "Iteration 9, loss = 0.97468677\n",
      "Iteration 10, loss = 0.96219650\n",
      "Iteration 11, loss = 0.95311526\n",
      "Iteration 12, loss = 0.94466233\n",
      "Iteration 13, loss = 0.93640780\n",
      "Iteration 14, loss = 0.93014133\n",
      "Iteration 15, loss = 0.92357671\n",
      "Iteration 16, loss = 0.91805317\n",
      "Iteration 17, loss = 0.91386789\n",
      "Iteration 18, loss = 0.90788682\n",
      "Iteration 19, loss = 0.90437827\n",
      "Iteration 20, loss = 0.89957109\n",
      "Iteration 21, loss = 0.89642926\n",
      "Iteration 22, loss = 0.89267368\n",
      "Iteration 23, loss = 0.88912378\n",
      "Iteration 24, loss = 0.88584359\n",
      "Iteration 25, loss = 0.88402536\n",
      "Iteration 26, loss = 0.88110722\n",
      "Iteration 27, loss = 0.87851491\n",
      "Iteration 28, loss = 0.87612745\n",
      "Iteration 29, loss = 0.87402645\n",
      "Iteration 30, loss = 0.87246637\n",
      "Iteration 31, loss = 0.86971484\n",
      "Iteration 32, loss = 0.86740357\n",
      "Iteration 33, loss = 0.86664450\n",
      "Iteration 34, loss = 0.86404364\n",
      "Iteration 35, loss = 0.86171061\n",
      "Iteration 36, loss = 0.86050012\n",
      "Iteration 37, loss = 0.85946494\n",
      "Iteration 38, loss = 0.85802684\n",
      "Iteration 39, loss = 0.85661058\n",
      "Iteration 40, loss = 0.85409265\n",
      "Iteration 41, loss = 0.85384534\n",
      "Iteration 42, loss = 0.85176612\n",
      "Iteration 43, loss = 0.85060720\n",
      "Iteration 44, loss = 0.84894901\n",
      "Iteration 45, loss = 0.84833428\n",
      "Iteration 46, loss = 0.84726146\n",
      "Iteration 47, loss = 0.84590572\n",
      "Iteration 48, loss = 0.84562993\n",
      "Iteration 49, loss = 0.84500901\n",
      "Iteration 50, loss = 0.84294517\n",
      "Iteration 51, loss = 0.84282587\n",
      "Iteration 52, loss = 0.84113742\n",
      "Iteration 53, loss = 0.84123087\n",
      "Iteration 54, loss = 0.83973619\n",
      "Iteration 55, loss = 0.83912152\n",
      "Iteration 56, loss = 0.83828438\n",
      "Iteration 57, loss = 0.83689115\n",
      "Iteration 58, loss = 0.83534682\n",
      "Iteration 59, loss = 0.83587125\n",
      "Iteration 60, loss = 0.83440667\n",
      "Iteration 61, loss = 0.83359791\n",
      "Iteration 62, loss = 0.83390957\n",
      "Iteration 63, loss = 0.83230633\n",
      "Iteration 64, loss = 0.83152282\n",
      "Iteration 65, loss = 0.83150722\n",
      "Iteration 66, loss = 0.83033233\n",
      "Iteration 67, loss = 0.82960381\n",
      "Iteration 68, loss = 0.83031793\n",
      "Iteration 69, loss = 0.82943740\n",
      "Iteration 70, loss = 0.82819785\n",
      "Iteration 71, loss = 0.82733275\n",
      "Iteration 72, loss = 0.82703492\n",
      "Iteration 73, loss = 0.82624356\n",
      "Iteration 74, loss = 0.82563932\n",
      "Iteration 75, loss = 0.82458051\n",
      "Iteration 76, loss = 0.82453695\n",
      "Iteration 77, loss = 0.82382827\n",
      "Iteration 78, loss = 0.82334098\n",
      "Iteration 79, loss = 0.82330872\n",
      "Iteration 80, loss = 0.82230114\n",
      "Iteration 81, loss = 0.82214816\n",
      "Iteration 82, loss = 0.82047296\n",
      "Iteration 83, loss = 0.82048722\n",
      "Iteration 84, loss = 0.82000732\n",
      "Iteration 85, loss = 0.82014250\n",
      "Iteration 86, loss = 0.81946771\n",
      "Iteration 87, loss = 0.81941731\n",
      "Iteration 88, loss = 0.81830986\n",
      "Iteration 89, loss = 0.81811055\n",
      "Iteration 90, loss = 0.81710808\n",
      "Iteration 91, loss = 0.81766127\n",
      "Iteration 92, loss = 0.81706536\n",
      "Iteration 93, loss = 0.81702047\n",
      "Iteration 94, loss = 0.81645473\n",
      "Iteration 95, loss = 0.81568062\n",
      "Iteration 96, loss = 0.81545027\n",
      "Iteration 97, loss = 0.81373384\n",
      "Iteration 98, loss = 0.81501758\n",
      "Iteration 99, loss = 0.81383151\n",
      "Iteration 100, loss = 0.81330816\n",
      "Iteration 101, loss = 0.81355439\n",
      "Iteration 102, loss = 0.81332264\n",
      "Iteration 103, loss = 0.81296668\n",
      "Iteration 104, loss = 0.81193672\n",
      "Iteration 105, loss = 0.81179975\n",
      "Iteration 106, loss = 0.81152665\n",
      "Iteration 107, loss = 0.81109575\n",
      "Iteration 108, loss = 0.81091472\n",
      "Iteration 109, loss = 0.81059360\n",
      "Iteration 110, loss = 0.81046723\n",
      "Iteration 111, loss = 0.80957990\n",
      "Iteration 112, loss = 0.80943488\n",
      "Iteration 113, loss = 0.80891633\n",
      "Iteration 114, loss = 0.80949003\n",
      "Iteration 115, loss = 0.80931697\n",
      "Iteration 116, loss = 0.80853681\n",
      "Iteration 117, loss = 0.80815209\n",
      "Iteration 118, loss = 0.80769672\n",
      "Iteration 119, loss = 0.80748672\n",
      "Iteration 120, loss = 0.80705522\n",
      "Iteration 121, loss = 0.80703630\n",
      "Iteration 122, loss = 0.80654944\n",
      "Iteration 123, loss = 0.80512673\n",
      "Iteration 124, loss = 0.80631688\n",
      "Iteration 125, loss = 0.80522047\n",
      "Iteration 126, loss = 0.80516211\n",
      "Iteration 127, loss = 0.80528307\n",
      "Iteration 128, loss = 0.80498739\n",
      "Iteration 129, loss = 0.80411659\n",
      "Iteration 130, loss = 0.80361918\n",
      "Iteration 131, loss = 0.80457313\n",
      "Iteration 132, loss = 0.80391214\n",
      "Iteration 133, loss = 0.80415032\n",
      "Iteration 134, loss = 0.80314119\n",
      "Iteration 135, loss = 0.80303854\n",
      "Iteration 136, loss = 0.80304709\n",
      "Iteration 137, loss = 0.80267756\n",
      "Iteration 138, loss = 0.80274606\n",
      "Iteration 139, loss = 0.80183130\n",
      "Iteration 140, loss = 0.80183924\n",
      "Iteration 141, loss = 0.80182677\n",
      "Iteration 142, loss = 0.80116701\n",
      "Iteration 143, loss = 0.80166748\n",
      "Iteration 144, loss = 0.80038209\n",
      "Iteration 145, loss = 0.80158082\n",
      "Iteration 146, loss = 0.80076352\n",
      "Iteration 147, loss = 0.80001243\n",
      "Iteration 148, loss = 0.79996596\n",
      "Iteration 149, loss = 0.79906723\n",
      "Iteration 150, loss = 0.79913380\n",
      "Iteration 151, loss = 0.79859348\n",
      "Iteration 152, loss = 0.79954707\n",
      "Iteration 153, loss = 0.79857593\n",
      "Iteration 154, loss = 0.79835825\n",
      "Iteration 155, loss = 0.79865903\n",
      "Iteration 156, loss = 0.79791069\n",
      "Iteration 157, loss = 0.79796828\n",
      "Iteration 158, loss = 0.79820269\n",
      "Iteration 159, loss = 0.79750710\n",
      "Iteration 160, loss = 0.79711882\n",
      "Iteration 161, loss = 0.79741632\n",
      "Iteration 162, loss = 0.79651493\n",
      "Iteration 163, loss = 0.79680711\n",
      "Iteration 164, loss = 0.79659041\n",
      "Iteration 165, loss = 0.79759750\n",
      "Iteration 166, loss = 0.79558618\n",
      "Iteration 167, loss = 0.79555606\n",
      "Iteration 168, loss = 0.79571176\n",
      "Iteration 169, loss = 0.79595880\n",
      "Iteration 170, loss = 0.79520464\n",
      "Iteration 171, loss = 0.79642104\n",
      "Iteration 172, loss = 0.79514839\n",
      "Iteration 173, loss = 0.79491517\n",
      "Iteration 174, loss = 0.79481414\n",
      "Iteration 175, loss = 0.79459620\n",
      "Iteration 176, loss = 0.79456828\n",
      "Iteration 177, loss = 0.79393737\n",
      "Iteration 178, loss = 0.79375184\n",
      "Iteration 179, loss = 0.79437785\n",
      "Iteration 180, loss = 0.79348478\n",
      "Iteration 181, loss = 0.79315916\n",
      "Iteration 182, loss = 0.79452309\n",
      "Iteration 183, loss = 0.79288793\n",
      "Iteration 184, loss = 0.79183470\n",
      "Iteration 185, loss = 0.79277098\n",
      "Iteration 186, loss = 0.79130687\n",
      "Iteration 187, loss = 0.79335679\n",
      "Iteration 188, loss = 0.79242342\n",
      "Iteration 189, loss = 0.79194999\n",
      "Iteration 190, loss = 0.79216214\n",
      "Iteration 191, loss = 0.79193087\n",
      "Iteration 192, loss = 0.79182301\n",
      "Iteration 193, loss = 0.79094411\n",
      "Iteration 194, loss = 0.79182920\n",
      "Iteration 195, loss = 0.79184910\n",
      "Iteration 196, loss = 0.79122650\n",
      "Iteration 197, loss = 0.79071952\n",
      "Iteration 198, loss = 0.79094472\n",
      "Iteration 199, loss = 0.79078083\n",
      "Iteration 200, loss = 0.79188023\n",
      "Iteration 201, loss = 0.79050711\n",
      "Iteration 202, loss = 0.79097276\n",
      "Iteration 203, loss = 0.79003434\n",
      "Iteration 204, loss = 0.79018292\n",
      "Iteration 205, loss = 0.79035355\n",
      "Iteration 206, loss = 0.78897492\n",
      "Iteration 207, loss = 0.78947460\n",
      "Iteration 208, loss = 0.78883699\n",
      "Iteration 209, loss = 0.78932912\n",
      "Iteration 210, loss = 0.78944127\n",
      "Iteration 211, loss = 0.78934844\n",
      "Iteration 212, loss = 0.78836057\n",
      "Iteration 213, loss = 0.78914888\n",
      "Iteration 214, loss = 0.78869114\n",
      "Iteration 215, loss = 0.78835925\n",
      "Iteration 216, loss = 0.78840179\n",
      "Iteration 217, loss = 0.78910684\n",
      "Iteration 218, loss = 0.78843543\n",
      "Iteration 219, loss = 0.78849653\n",
      "Iteration 220, loss = 0.78850736\n",
      "Iteration 221, loss = 0.78721688\n",
      "Iteration 222, loss = 0.78799607\n",
      "Iteration 223, loss = 0.78761758\n",
      "Iteration 224, loss = 0.78735084\n",
      "Iteration 225, loss = 0.78788440\n",
      "Iteration 226, loss = 0.78766178\n",
      "Iteration 227, loss = 0.78671792\n",
      "Iteration 228, loss = 0.78686137\n",
      "Iteration 229, loss = 0.78621121\n",
      "Iteration 230, loss = 0.78659752\n",
      "Iteration 231, loss = 0.78611283\n",
      "Iteration 232, loss = 0.78753024\n",
      "Iteration 233, loss = 0.78565256\n",
      "Iteration 234, loss = 0.78584278\n",
      "Iteration 235, loss = 0.78584302\n",
      "Iteration 236, loss = 0.78532245\n",
      "Iteration 237, loss = 0.78580483\n",
      "Iteration 238, loss = 0.78586585\n",
      "Iteration 239, loss = 0.78614295\n",
      "Iteration 240, loss = 0.78532241\n",
      "Iteration 241, loss = 0.78494360\n",
      "Iteration 242, loss = 0.78493627\n",
      "Iteration 243, loss = 0.78535981\n",
      "Iteration 244, loss = 0.78555053\n",
      "Iteration 245, loss = 0.78484011\n",
      "Iteration 246, loss = 0.78477563\n",
      "Iteration 247, loss = 0.78420869\n",
      "Iteration 248, loss = 0.78478715\n",
      "Iteration 249, loss = 0.78436948\n",
      "Iteration 250, loss = 0.78568718\n",
      "Iteration 251, loss = 0.78391118\n",
      "Iteration 252, loss = 0.78428161\n",
      "Iteration 253, loss = 0.78383360\n",
      "Iteration 254, loss = 0.78402463\n",
      "Iteration 255, loss = 0.78515438\n",
      "Iteration 256, loss = 0.78412466\n",
      "Iteration 257, loss = 0.78344176\n",
      "Iteration 258, loss = 0.78522648\n",
      "Iteration 259, loss = 0.78359893\n",
      "Iteration 260, loss = 0.78362425\n",
      "Iteration 261, loss = 0.78377786\n",
      "Iteration 262, loss = 0.78307401\n",
      "Iteration 263, loss = 0.78384885\n",
      "Iteration 264, loss = 0.78268694\n",
      "Iteration 265, loss = 0.78278064\n",
      "Iteration 266, loss = 0.78237896\n",
      "Iteration 267, loss = 0.78203176\n",
      "Iteration 268, loss = 0.78191337\n",
      "Iteration 269, loss = 0.78264807\n",
      "Iteration 270, loss = 0.78133841\n",
      "Iteration 271, loss = 0.78220749\n",
      "Iteration 272, loss = 0.78186466\n",
      "Iteration 273, loss = 0.78197547\n",
      "Iteration 274, loss = 0.78271044\n",
      "Iteration 275, loss = 0.78155730\n",
      "Iteration 276, loss = 0.78185852\n",
      "Iteration 277, loss = 0.78203264\n",
      "Iteration 278, loss = 0.78111287\n",
      "Iteration 279, loss = 0.78205981\n",
      "Iteration 280, loss = 0.78067646\n",
      "Iteration 281, loss = 0.78155141\n",
      "Iteration 282, loss = 0.78117207\n",
      "Iteration 283, loss = 0.78041139\n",
      "Iteration 284, loss = 0.78129454\n",
      "Iteration 285, loss = 0.78183383\n",
      "Iteration 286, loss = 0.78205070\n",
      "Iteration 287, loss = 0.78109711\n",
      "Iteration 288, loss = 0.77995457\n",
      "Iteration 289, loss = 0.78032468\n",
      "Iteration 290, loss = 0.78040092\n",
      "Iteration 291, loss = 0.77987777\n",
      "Iteration 292, loss = 0.78013046\n",
      "Iteration 293, loss = 0.78045948\n",
      "Iteration 294, loss = 0.78093282\n",
      "Iteration 295, loss = 0.77964588\n",
      "Iteration 296, loss = 0.77990559\n",
      "Iteration 297, loss = 0.77966327\n",
      "Iteration 298, loss = 0.78077439\n",
      "Iteration 299, loss = 0.77937075\n",
      "Iteration 300, loss = 0.77996720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiergallo/miniconda3/envs/diplodatos/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Original:\n",
    "tree_param = {\n",
    "    'criterion': ('gini', 'entropy'),\n",
    "    'min_samples_leaf': (1, 2, 5),\n",
    "    'min_samples_split': (2, 3, 5, 10, 50, 100)\n",
    "}\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Best models so far (from better to worse):\n",
    "MLPClassifier(\n",
    "    random_state=42,\n",
    "    max_iter=300,\n",
    "    alpha=0.02,\n",
    "    hidden_layer_sizes=(64,),\n",
    ")\n",
    "DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=101,\n",
    "    max_depth=59,\n",
    "    class_weight={999: 0.49}\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "model_param = {\n",
    "    'alpha': (0.02,),\n",
    "    # 'learning_rate_init': (0.004,)\n",
    "}\n",
    "model = MLPClassifier(random_state=42,\n",
    "                      verbose=True,\n",
    "                      max_iter=300,\n",
    "                      hidden_layer_sizes=(96,))\n",
    "\n",
    "search = GridSearchCV(model, model_param, cv=3, scoring='accuracy') #scoring='balanced_accuracy')\n",
    "\n",
    "# NOTICE we exclude visit number; all it does is confuse our model.\n",
    "search.fit(X_train.drop(columns=['VisitNumber']), y_train)\n",
    "\n",
    "best_model_clf = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Decision Tree accuracy:  0.6879795396419436\n",
      "MLPClassifier(alpha=0.02, hidden_layer_sizes=(96,), max_iter=300,\n",
      "              random_state=42, verbose=True)\n",
      "The best classifier so far is: \n",
      "MLPClassifier(alpha=0.02, hidden_layer_sizes=(64,), max_iter=300,\n",
      "              random_state=42, verbose=True)\n"
     ]
    }
   ],
   "source": [
    "print('Best Decision Tree accuracy: ', search.best_score_)\n",
    "print(best_model_clf)\n",
    "results = results.append({'clf': best_model_clf, 'best_acc': search.best_score_}, ignore_index=True)\n",
    "\n",
    "print('The best classifier so far is: ')\n",
    "best_clf = results.loc[results['best_acc'].idxmax()]['clf']\n",
    "print(best_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clf</th>\n",
       "      <th>best_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier(class_weight={999: 0.49...</td>\n",
       "      <td>0.634420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLPClassifier(alpha=0.01, hidden_layer_sizes=(...</td>\n",
       "      <td>0.687127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLPClassifier(alpha=0.01, hidden_layer_sizes=(...</td>\n",
       "      <td>0.673934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(64,), learni...</td>\n",
       "      <td>0.665196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLPClassifier(alpha=0.01, hidden_layer_sizes=(...</td>\n",
       "      <td>0.685060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLPClassifier(alpha=0.01, hidden_layer_sizes=(...</td>\n",
       "      <td>0.670716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(64,), max_it...</td>\n",
       "      <td>0.671057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(64,), max_it...</td>\n",
       "      <td>0.680733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(64,), learni...</td>\n",
       "      <td>0.672101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(64,), learni...</td>\n",
       "      <td>0.673039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MLPClassifier(alpha=0.01, hidden_layer_sizes=(...</td>\n",
       "      <td>0.681436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MLPClassifier(alpha=0.02, hidden_layer_sizes=(...</td>\n",
       "      <td>0.688107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLPClassifier(alpha=0.02, hidden_layer_sizes=(...</td>\n",
       "      <td>0.687980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  clf  best_acc\n",
       "0   DecisionTreeClassifier(class_weight={999: 0.49...  0.634420\n",
       "1   MLPClassifier(alpha=0.01, hidden_layer_sizes=(...  0.687127\n",
       "2   MLPClassifier(alpha=0.01, hidden_layer_sizes=(...  0.673934\n",
       "3   MLPClassifier(hidden_layer_sizes=(64,), learni...  0.665196\n",
       "4   MLPClassifier(alpha=0.01, hidden_layer_sizes=(...  0.685060\n",
       "5   MLPClassifier(alpha=0.01, hidden_layer_sizes=(...  0.670716\n",
       "6   MLPClassifier(hidden_layer_sizes=(64,), max_it...  0.671057\n",
       "7   MLPClassifier(hidden_layer_sizes=(64,), max_it...  0.680733\n",
       "8   MLPClassifier(hidden_layer_sizes=(64,), learni...  0.672101\n",
       "9   MLPClassifier(hidden_layer_sizes=(64,), learni...  0.673039\n",
       "10  MLPClassifier(alpha=0.01, hidden_layer_sizes=(...  0.681436\n",
       "11  MLPClassifier(alpha=0.02, hidden_layer_sizes=(...  0.688107\n",
       "12  MLPClassifier(alpha=0.02, hidden_layer_sizes=(...  0.687980"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And finally**, we predict the unknown label for the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67029, 79), (28645, 79))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTICE we exclude visit number becuase we didn't use it for training.\n",
    "yy = best_clf.predict(XX.drop(columns=['VisitNumber']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we do is generating a file that should be *submitted* on kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(list(zip(XX.VisitNumber, yy)), columns=[\"VisitNumber\", \"TripType\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"./data/submission.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
