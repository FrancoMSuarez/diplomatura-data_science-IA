{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diplodatos Kaggle Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import CategoricalNB, GaussianNB, MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(train_data_fname, test_data_fname):\n",
    "    df_train = pd.read_csv(train_data_fname)\n",
    "    df_train['is_train_set'] = 1\n",
    "    df_test = pd.read_csv(test_data_fname)\n",
    "    df_test['is_train_set'] = 0\n",
    "\n",
    "    # we  get the TripType for the train set. To do that, we group by VisitNumber and\n",
    "    # then we get the max (or min or avg)\n",
    "    y = df_train.groupby([\"VisitNumber\", \"Weekday\"], as_index=False).max().TripType\n",
    "\n",
    "    # we remove the TripType now, and concat training and testing data\n",
    "    # the concat is done so that we have the same columns for both datasets\n",
    "    # after one-hot encoding\n",
    "    df_train = df_train.drop(\"TripType\", axis=1)\n",
    "    df = pd.concat([df_train, df_test])\n",
    "    \n",
    "    # the next three operations are the ones we have just presented in the previous lines\n",
    "    \n",
    "    # drop the columns we won't use (it may be good to use them somehow)\n",
    "    df = df.drop([\"Upc\", \"FinelineNumber\"], axis=1)\n",
    "\n",
    "    # one-hot encoding for the DepartmentDescription\n",
    "    df = pd.get_dummies(df, columns=[\"DepartmentDescription\"], dummy_na=True)\n",
    "\n",
    "    # now we add the groupby values\n",
    "    df = df.groupby([\"VisitNumber\", \"Weekday\"], as_index=False).sum()\n",
    "    \n",
    "    # finally, we do one-hot encoding for the Weekday\n",
    "    df = pd.get_dummies(df, columns=[\"Weekday\"], dummy_na=True)\n",
    "\n",
    "    # get train and test back\n",
    "    df_train = df[df.is_train_set != 0]\n",
    "    df_test = df[df.is_train_set == 0]\n",
    "    \n",
    "    X = df_train.drop([\"is_train_set\"], axis=1)\n",
    "    yy = None\n",
    "    XX = df_test.drop([\"is_train_set\"], axis=1)\n",
    "\n",
    "    return X, y, XX, yy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, XX, yy = transform_data(\"./data/train.csv\", \"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VisitNumber</th>\n",
       "      <th>ScanCount</th>\n",
       "      <th>DepartmentDescription_1-HR PHOTO</th>\n",
       "      <th>DepartmentDescription_ACCESSORIES</th>\n",
       "      <th>DepartmentDescription_AUTOMOTIVE</th>\n",
       "      <th>DepartmentDescription_BAKERY</th>\n",
       "      <th>DepartmentDescription_BATH AND SHOWER</th>\n",
       "      <th>DepartmentDescription_BEAUTY</th>\n",
       "      <th>DepartmentDescription_BEDDING</th>\n",
       "      <th>DepartmentDescription_BOOKS AND MAGAZINES</th>\n",
       "      <th>...</th>\n",
       "      <th>DepartmentDescription_WIRELESS</th>\n",
       "      <th>DepartmentDescription_nan</th>\n",
       "      <th>Weekday_Friday</th>\n",
       "      <th>Weekday_Monday</th>\n",
       "      <th>Weekday_Saturday</th>\n",
       "      <th>Weekday_Sunday</th>\n",
       "      <th>Weekday_Thursday</th>\n",
       "      <th>Weekday_Tuesday</th>\n",
       "      <th>Weekday_Wednesday</th>\n",
       "      <th>Weekday_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81954</th>\n",
       "      <td>163907</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32578</th>\n",
       "      <td>65166</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86578</th>\n",
       "      <td>173052</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85079</th>\n",
       "      <td>170137</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9788</th>\n",
       "      <td>19404</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52950</th>\n",
       "      <td>106158</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8809</th>\n",
       "      <td>17442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78302</th>\n",
       "      <td>156542</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>2404</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22577</th>\n",
       "      <td>45320</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46920 rows Ã— 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       VisitNumber  ScanCount  DepartmentDescription_1-HR PHOTO  \\\n",
       "81954       163907          2                                 0   \n",
       "32578        65166          1                                 0   \n",
       "86578       173052          5                                 0   \n",
       "85079       170137          7                                 0   \n",
       "9788         19404         32                                 0   \n",
       "...            ...        ...                               ...   \n",
       "52950       106158          1                                 0   \n",
       "8809         17442          1                                 0   \n",
       "78302       156542          5                                 0   \n",
       "1229          2404          8                                 0   \n",
       "22577        45320         19                                 0   \n",
       "\n",
       "       DepartmentDescription_ACCESSORIES  DepartmentDescription_AUTOMOTIVE  \\\n",
       "81954                                  0                                 0   \n",
       "32578                                  0                                 0   \n",
       "86578                                  0                                 0   \n",
       "85079                                  0                                 0   \n",
       "9788                                   0                                 0   \n",
       "...                                  ...                               ...   \n",
       "52950                                  0                                 0   \n",
       "8809                                   0                                 0   \n",
       "78302                                  0                                 0   \n",
       "1229                                   0                                 0   \n",
       "22577                                  0                                 0   \n",
       "\n",
       "       DepartmentDescription_BAKERY  DepartmentDescription_BATH AND SHOWER  \\\n",
       "81954                             0                                      0   \n",
       "32578                             0                                      0   \n",
       "86578                             0                                      0   \n",
       "85079                             0                                      0   \n",
       "9788                              0                                      0   \n",
       "...                             ...                                    ...   \n",
       "52950                             0                                      0   \n",
       "8809                              0                                      0   \n",
       "78302                             0                                      0   \n",
       "1229                              0                                      0   \n",
       "22577                             0                                      0   \n",
       "\n",
       "       DepartmentDescription_BEAUTY  DepartmentDescription_BEDDING  \\\n",
       "81954                             0                              0   \n",
       "32578                             0                              0   \n",
       "86578                             0                              0   \n",
       "85079                             0                              0   \n",
       "9788                              0                              0   \n",
       "...                             ...                            ...   \n",
       "52950                             0                              0   \n",
       "8809                              0                              0   \n",
       "78302                             0                              0   \n",
       "1229                              0                              0   \n",
       "22577                             0                              0   \n",
       "\n",
       "       DepartmentDescription_BOOKS AND MAGAZINES  ...  \\\n",
       "81954                                          0  ...   \n",
       "32578                                          0  ...   \n",
       "86578                                          0  ...   \n",
       "85079                                          0  ...   \n",
       "9788                                           0  ...   \n",
       "...                                          ...  ...   \n",
       "52950                                          0  ...   \n",
       "8809                                           0  ...   \n",
       "78302                                          0  ...   \n",
       "1229                                           0  ...   \n",
       "22577                                          0  ...   \n",
       "\n",
       "       DepartmentDescription_WIRELESS  DepartmentDescription_nan  \\\n",
       "81954                               0                          0   \n",
       "32578                               0                          0   \n",
       "86578                               0                          0   \n",
       "85079                               0                          0   \n",
       "9788                                0                          0   \n",
       "...                               ...                        ...   \n",
       "52950                               0                          0   \n",
       "8809                                0                          0   \n",
       "78302                               0                          0   \n",
       "1229                                0                          0   \n",
       "22577                               0                          0   \n",
       "\n",
       "       Weekday_Friday  Weekday_Monday  Weekday_Saturday  Weekday_Sunday  \\\n",
       "81954               0               0                 0               0   \n",
       "32578               0               1                 0               0   \n",
       "86578               1               0                 0               0   \n",
       "85079               0               0                 0               0   \n",
       "9788                0               0                 0               1   \n",
       "...               ...             ...               ...             ...   \n",
       "52950               0               0                 0               1   \n",
       "8809                0               0                 0               1   \n",
       "78302               0               0                 0               0   \n",
       "1229                1               0                 0               0   \n",
       "22577               1               0                 0               0   \n",
       "\n",
       "       Weekday_Thursday  Weekday_Tuesday  Weekday_Wednesday  Weekday_nan  \n",
       "81954                 0                0                  1            0  \n",
       "32578                 0                0                  0            0  \n",
       "86578                 0                0                  0            0  \n",
       "85079                 1                0                  0            0  \n",
       "9788                  0                0                  0            0  \n",
       "...                 ...              ...                ...          ...  \n",
       "52950                 0                0                  0            0  \n",
       "8809                  0                0                  0            0  \n",
       "78302                 0                1                  0            0  \n",
       "1229                  0                0                  0            0  \n",
       "22577                 0                0                  0            0  \n",
       "\n",
       "[46920 rows x 79 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split training dataset into train and \"validation\" \n",
    "# (we won't be using validation set in this example, because of the cross-validation;\n",
    "# but it could be useful for you depending on your approach)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results dataframe is used to store the computed results\n",
    "results = pd.DataFrame(columns=('clf', 'best_acc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.65077244\n",
      "Iteration 2, loss = 1.52906854\n",
      "Iteration 3, loss = 1.25866649\n",
      "Iteration 4, loss = 1.16246583\n",
      "Iteration 5, loss = 1.11252099\n",
      "Iteration 6, loss = 1.07847494\n",
      "Iteration 7, loss = 1.05305404\n",
      "Iteration 8, loss = 1.03589541\n",
      "Iteration 9, loss = 1.01901410\n",
      "Iteration 10, loss = 1.00652433\n",
      "Iteration 11, loss = 0.99476784\n",
      "Iteration 12, loss = 0.98538098\n",
      "Iteration 13, loss = 0.97727694\n",
      "Iteration 14, loss = 0.96879138\n",
      "Iteration 15, loss = 0.96259563\n",
      "Iteration 16, loss = 0.95645002\n",
      "Iteration 17, loss = 0.94899170\n",
      "Iteration 18, loss = 0.94361123\n",
      "Iteration 19, loss = 0.93912256\n",
      "Iteration 20, loss = 0.93422943\n",
      "Iteration 21, loss = 0.92975527\n",
      "Iteration 22, loss = 0.92425646\n",
      "Iteration 23, loss = 0.92021889\n",
      "Iteration 24, loss = 0.91733927\n",
      "Iteration 25, loss = 0.91296167\n",
      "Iteration 26, loss = 0.91049729\n",
      "Iteration 27, loss = 0.90773773\n",
      "Iteration 28, loss = 0.90593885\n",
      "Iteration 29, loss = 0.90130023\n",
      "Iteration 30, loss = 0.89840073\n",
      "Iteration 31, loss = 0.89512262\n",
      "Iteration 32, loss = 0.89377350\n",
      "Iteration 33, loss = 0.89235242\n",
      "Iteration 34, loss = 0.88915585\n",
      "Iteration 35, loss = 0.88688853\n",
      "Iteration 36, loss = 0.88426535\n",
      "Iteration 37, loss = 0.88213299\n",
      "Iteration 38, loss = 0.88179122\n",
      "Iteration 39, loss = 0.87985938\n",
      "Iteration 40, loss = 0.87685793\n",
      "Iteration 41, loss = 0.87412353\n",
      "Iteration 42, loss = 0.87304500\n",
      "Iteration 43, loss = 0.86997646\n",
      "Iteration 44, loss = 0.86869993\n",
      "Iteration 45, loss = 0.86774711\n",
      "Iteration 46, loss = 0.86611830\n",
      "Iteration 47, loss = 0.86518255\n",
      "Iteration 48, loss = 0.86403464\n",
      "Iteration 49, loss = 0.86175339\n",
      "Iteration 50, loss = 0.86105518\n",
      "Iteration 51, loss = 0.86032991\n",
      "Iteration 52, loss = 0.85746356\n",
      "Iteration 53, loss = 0.85628436\n",
      "Iteration 54, loss = 0.85463718\n",
      "Iteration 55, loss = 0.85362935\n",
      "Iteration 56, loss = 0.85344839\n",
      "Iteration 57, loss = 0.85114007\n",
      "Iteration 58, loss = 0.85102901\n",
      "Iteration 59, loss = 0.84869475\n",
      "Iteration 60, loss = 0.85005106\n",
      "Iteration 61, loss = 0.84815842\n",
      "Iteration 62, loss = 0.84604245\n",
      "Iteration 63, loss = 0.84606338\n",
      "Iteration 64, loss = 0.84542419\n",
      "Iteration 65, loss = 0.84365328\n",
      "Iteration 66, loss = 0.84224219\n",
      "Iteration 67, loss = 0.84115591\n",
      "Iteration 68, loss = 0.84094316\n",
      "Iteration 69, loss = 0.83860392\n",
      "Iteration 70, loss = 0.83933333\n",
      "Iteration 71, loss = 0.83791724\n",
      "Iteration 72, loss = 0.83735474\n",
      "Iteration 73, loss = 0.83622590\n",
      "Iteration 74, loss = 0.83656980\n",
      "Iteration 75, loss = 0.83458830\n",
      "Iteration 76, loss = 0.83367320\n",
      "Iteration 77, loss = 0.83309761\n",
      "Iteration 78, loss = 0.83256574\n",
      "Iteration 79, loss = 0.83167385\n",
      "Iteration 80, loss = 0.82995139\n",
      "Iteration 81, loss = 0.83059783\n",
      "Iteration 82, loss = 0.82939533\n",
      "Iteration 83, loss = 0.82920338\n",
      "Iteration 84, loss = 0.82823184\n",
      "Iteration 85, loss = 0.82768580\n",
      "Iteration 86, loss = 0.82673739\n",
      "Iteration 87, loss = 0.82601247\n",
      "Iteration 88, loss = 0.82503535\n",
      "Iteration 89, loss = 0.82420311\n",
      "Iteration 90, loss = 0.82414899\n",
      "Iteration 91, loss = 0.82350257\n",
      "Iteration 92, loss = 0.82188706\n",
      "Iteration 93, loss = 0.82188663\n",
      "Iteration 94, loss = 0.82169464\n",
      "Iteration 95, loss = 0.82228642\n",
      "Iteration 96, loss = 0.82031849\n",
      "Iteration 97, loss = 0.82039370\n",
      "Iteration 98, loss = 0.82050806\n",
      "Iteration 99, loss = 0.81919572\n",
      "Iteration 100, loss = 0.81855177\n",
      "Iteration 101, loss = 0.81846991\n",
      "Iteration 102, loss = 0.81807541\n",
      "Iteration 103, loss = 0.81838955\n",
      "Iteration 104, loss = 0.81637013\n",
      "Iteration 105, loss = 0.81598284\n",
      "Iteration 106, loss = 0.81560667\n",
      "Iteration 107, loss = 0.81554008\n",
      "Iteration 108, loss = 0.81490959\n",
      "Iteration 109, loss = 0.81449984\n",
      "Iteration 110, loss = 0.81418345\n",
      "Iteration 111, loss = 0.81317053\n",
      "Iteration 112, loss = 0.81232818\n",
      "Iteration 113, loss = 0.81286680\n",
      "Iteration 114, loss = 0.81092053\n",
      "Iteration 115, loss = 0.81244592\n",
      "Iteration 116, loss = 0.81127481\n",
      "Iteration 117, loss = 0.81048270\n",
      "Iteration 118, loss = 0.81084023\n",
      "Iteration 119, loss = 0.81075220\n",
      "Iteration 120, loss = 0.80929920\n",
      "Iteration 121, loss = 0.81074794\n",
      "Iteration 122, loss = 0.81026826\n",
      "Iteration 123, loss = 0.80875850\n",
      "Iteration 124, loss = 0.80822875\n",
      "Iteration 125, loss = 0.80782195\n",
      "Iteration 126, loss = 0.80796757\n",
      "Iteration 127, loss = 0.80691146\n",
      "Iteration 128, loss = 0.80610019\n",
      "Iteration 129, loss = 0.80559212\n",
      "Iteration 130, loss = 0.80533677\n",
      "Iteration 131, loss = 0.80558040\n",
      "Iteration 132, loss = 0.80620749\n",
      "Iteration 133, loss = 0.80457631\n",
      "Iteration 134, loss = 0.80440195\n",
      "Iteration 135, loss = 0.80469421\n",
      "Iteration 136, loss = 0.80414304\n",
      "Iteration 137, loss = 0.80553437\n",
      "Iteration 138, loss = 0.80268062\n",
      "Iteration 139, loss = 0.80294491\n",
      "Iteration 140, loss = 0.80286409\n",
      "Iteration 141, loss = 0.80312749\n",
      "Iteration 142, loss = 0.80169867\n",
      "Iteration 143, loss = 0.80182093\n",
      "Iteration 144, loss = 0.80187544\n",
      "Iteration 145, loss = 0.80178360\n",
      "Iteration 146, loss = 0.80109693\n",
      "Iteration 147, loss = 0.79991703\n",
      "Iteration 148, loss = 0.79939090\n",
      "Iteration 149, loss = 0.79999294\n",
      "Iteration 150, loss = 0.79967490\n",
      "Iteration 151, loss = 0.79991644\n",
      "Iteration 152, loss = 0.79915040\n",
      "Iteration 153, loss = 0.79930085\n",
      "Iteration 154, loss = 0.79897940\n",
      "Iteration 155, loss = 0.79858416\n",
      "Iteration 156, loss = 0.79841892\n",
      "Iteration 157, loss = 0.79719683\n",
      "Iteration 158, loss = 0.79728997\n",
      "Iteration 159, loss = 0.79865589\n",
      "Iteration 160, loss = 0.79759961\n",
      "Iteration 161, loss = 0.79675295\n",
      "Iteration 162, loss = 0.79695994\n",
      "Iteration 163, loss = 0.79539414\n",
      "Iteration 164, loss = 0.79606529\n",
      "Iteration 165, loss = 0.79583932\n",
      "Iteration 166, loss = 0.79610899\n",
      "Iteration 167, loss = 0.79514917\n",
      "Iteration 168, loss = 0.79571853\n",
      "Iteration 169, loss = 0.79354736\n",
      "Iteration 170, loss = 0.79429277\n",
      "Iteration 171, loss = 0.79446654\n",
      "Iteration 172, loss = 0.79337567\n",
      "Iteration 173, loss = 0.79500902\n",
      "Iteration 174, loss = 0.79328620\n",
      "Iteration 175, loss = 0.79351932\n",
      "Iteration 176, loss = 0.79320194\n",
      "Iteration 177, loss = 0.79375225\n",
      "Iteration 178, loss = 0.79174891\n",
      "Iteration 179, loss = 0.79251884\n",
      "Iteration 180, loss = 0.79204775\n",
      "Iteration 181, loss = 0.79099582\n",
      "Iteration 182, loss = 0.79117804\n",
      "Iteration 183, loss = 0.79186548\n",
      "Iteration 184, loss = 0.79107053\n",
      "Iteration 185, loss = 0.79224402\n",
      "Iteration 186, loss = 0.79097337\n",
      "Iteration 187, loss = 0.79096069\n",
      "Iteration 188, loss = 0.79097417\n",
      "Iteration 189, loss = 0.79024330\n",
      "Iteration 190, loss = 0.78937293\n",
      "Iteration 191, loss = 0.79039096\n",
      "Iteration 192, loss = 0.78915302\n",
      "Iteration 193, loss = 0.79028195\n",
      "Iteration 194, loss = 0.78911333\n",
      "Iteration 195, loss = 0.78942100\n",
      "Iteration 196, loss = 0.78989985\n",
      "Iteration 197, loss = 0.78968039\n",
      "Iteration 198, loss = 0.78835539\n",
      "Iteration 199, loss = 0.78749424\n",
      "Iteration 200, loss = 0.78774367\n",
      "Iteration 201, loss = 0.78721962\n",
      "Iteration 202, loss = 0.78759007\n",
      "Iteration 203, loss = 0.78756098\n",
      "Iteration 204, loss = 0.78698283\n",
      "Iteration 205, loss = 0.78674443\n",
      "Iteration 206, loss = 0.78613627\n",
      "Iteration 207, loss = 0.78656606\n",
      "Iteration 208, loss = 0.78651120\n",
      "Iteration 209, loss = 0.78616558\n",
      "Iteration 210, loss = 0.78760206\n",
      "Iteration 211, loss = 0.78624458\n",
      "Iteration 212, loss = 0.78592568\n",
      "Iteration 213, loss = 0.78603761\n",
      "Iteration 214, loss = 0.78501287\n",
      "Iteration 215, loss = 0.78547025\n",
      "Iteration 216, loss = 0.78576690\n",
      "Iteration 217, loss = 0.78448308\n",
      "Iteration 218, loss = 0.78507642\n",
      "Iteration 219, loss = 0.78484117\n",
      "Iteration 220, loss = 0.78468466\n",
      "Iteration 221, loss = 0.78594466\n",
      "Iteration 222, loss = 0.78435536\n",
      "Iteration 223, loss = 0.78502503\n",
      "Iteration 224, loss = 0.78487346\n",
      "Iteration 225, loss = 0.78308404\n",
      "Iteration 226, loss = 0.78380275\n",
      "Iteration 227, loss = 0.78310370\n",
      "Iteration 228, loss = 0.78361990\n",
      "Iteration 229, loss = 0.78281889\n",
      "Iteration 230, loss = 0.78285021\n",
      "Iteration 231, loss = 0.78319712\n",
      "Iteration 232, loss = 0.78426433\n",
      "Iteration 233, loss = 0.78347403\n",
      "Iteration 234, loss = 0.78321659\n",
      "Iteration 235, loss = 0.78312220\n",
      "Iteration 236, loss = 0.78089174\n",
      "Iteration 237, loss = 0.78326046\n",
      "Iteration 238, loss = 0.78172275\n",
      "Iteration 239, loss = 0.78308631\n",
      "Iteration 240, loss = 0.78292370\n",
      "Iteration 241, loss = 0.78140025\n",
      "Iteration 242, loss = 0.78142723\n",
      "Iteration 243, loss = 0.78159668\n",
      "Iteration 244, loss = 0.78111039\n",
      "Iteration 245, loss = 0.78161752\n",
      "Iteration 246, loss = 0.78071572\n",
      "Iteration 247, loss = 0.78051262\n",
      "Iteration 248, loss = 0.78271689\n",
      "Iteration 249, loss = 0.78013195\n",
      "Iteration 250, loss = 0.77982066\n",
      "Iteration 251, loss = 0.77955623\n",
      "Iteration 252, loss = 0.77917170\n",
      "Iteration 253, loss = 0.77967988\n",
      "Iteration 254, loss = 0.77992859\n",
      "Iteration 255, loss = 0.77934855\n",
      "Iteration 256, loss = 0.77969845\n",
      "Iteration 257, loss = 0.77928333\n",
      "Iteration 258, loss = 0.77958662\n",
      "Iteration 259, loss = 0.77944683\n",
      "Iteration 260, loss = 0.77852292\n",
      "Iteration 261, loss = 0.77872076\n",
      "Iteration 262, loss = 0.77818159\n",
      "Iteration 263, loss = 0.77895601\n",
      "Iteration 264, loss = 0.77990743\n",
      "Iteration 265, loss = 0.77820750\n",
      "Iteration 266, loss = 0.77859623\n",
      "Iteration 267, loss = 0.77901277\n",
      "Iteration 268, loss = 0.77907712\n",
      "Iteration 269, loss = 0.77822992\n",
      "Iteration 270, loss = 0.77742130\n",
      "Iteration 271, loss = 0.77729975\n",
      "Iteration 272, loss = 0.77771966\n",
      "Iteration 273, loss = 0.77869309\n",
      "Iteration 274, loss = 0.77833886\n",
      "Iteration 275, loss = 0.77704390\n",
      "Iteration 276, loss = 0.77732154\n",
      "Iteration 277, loss = 0.77646501\n",
      "Iteration 278, loss = 0.77669308\n",
      "Iteration 279, loss = 0.77674829\n",
      "Iteration 280, loss = 0.77603640\n",
      "Iteration 281, loss = 0.77627634\n",
      "Iteration 282, loss = 0.77564658\n",
      "Iteration 283, loss = 0.77642285\n",
      "Iteration 284, loss = 0.77615564\n",
      "Iteration 285, loss = 0.77565151\n",
      "Iteration 286, loss = 0.77577218\n",
      "Iteration 287, loss = 0.77643385\n",
      "Iteration 288, loss = 0.77612552\n",
      "Iteration 289, loss = 0.77539968\n",
      "Iteration 290, loss = 0.77609365\n",
      "Iteration 291, loss = 0.77542025\n",
      "Iteration 292, loss = 0.77496809\n",
      "Iteration 293, loss = 0.77609014\n",
      "Iteration 294, loss = 0.77474968\n",
      "Iteration 295, loss = 0.77546059\n",
      "Iteration 296, loss = 0.77610603\n",
      "Iteration 297, loss = 0.77518662\n",
      "Iteration 298, loss = 0.77469008\n",
      "Iteration 299, loss = 0.77447271\n",
      "Iteration 300, loss = 0.77395421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiergallo/miniconda3/envs/diplodatos/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.65192348\n",
      "Iteration 2, loss = 1.53010121\n",
      "Iteration 3, loss = 1.26704637\n",
      "Iteration 4, loss = 1.16990112\n",
      "Iteration 5, loss = 1.11903721\n",
      "Iteration 6, loss = 1.08541089\n",
      "Iteration 7, loss = 1.05801043\n",
      "Iteration 8, loss = 1.03952680\n",
      "Iteration 9, loss = 1.02142677\n",
      "Iteration 10, loss = 1.00975189\n",
      "Iteration 11, loss = 0.99741099\n",
      "Iteration 12, loss = 0.98753235\n",
      "Iteration 13, loss = 0.97804482\n",
      "Iteration 14, loss = 0.96981408\n",
      "Iteration 15, loss = 0.96279314\n",
      "Iteration 16, loss = 0.95636548\n",
      "Iteration 17, loss = 0.94979144\n",
      "Iteration 18, loss = 0.94444412\n",
      "Iteration 19, loss = 0.94005714\n",
      "Iteration 20, loss = 0.93403602\n",
      "Iteration 21, loss = 0.93096842\n",
      "Iteration 22, loss = 0.92603549\n",
      "Iteration 23, loss = 0.92269823\n",
      "Iteration 24, loss = 0.92008604\n",
      "Iteration 25, loss = 0.91552814\n",
      "Iteration 26, loss = 0.91214095\n",
      "Iteration 27, loss = 0.90932991\n",
      "Iteration 28, loss = 0.90733550\n",
      "Iteration 29, loss = 0.90396280\n",
      "Iteration 30, loss = 0.90056193\n",
      "Iteration 31, loss = 0.89841398\n",
      "Iteration 32, loss = 0.89661396\n",
      "Iteration 33, loss = 0.89487474\n",
      "Iteration 34, loss = 0.89310733\n",
      "Iteration 35, loss = 0.88896569\n",
      "Iteration 36, loss = 0.88734909\n",
      "Iteration 37, loss = 0.88594282\n",
      "Iteration 38, loss = 0.88527959\n",
      "Iteration 39, loss = 0.88301975\n",
      "Iteration 40, loss = 0.88187222\n",
      "Iteration 41, loss = 0.87862634\n",
      "Iteration 42, loss = 0.87787966\n",
      "Iteration 43, loss = 0.87675548\n",
      "Iteration 44, loss = 0.87503012\n",
      "Iteration 45, loss = 0.87282985\n",
      "Iteration 46, loss = 0.87161850\n",
      "Iteration 47, loss = 0.86993093\n",
      "Iteration 48, loss = 0.86961850\n",
      "Iteration 49, loss = 0.86800648\n",
      "Iteration 50, loss = 0.86829807\n",
      "Iteration 51, loss = 0.86465477\n",
      "Iteration 52, loss = 0.86518951\n",
      "Iteration 53, loss = 0.86365082\n",
      "Iteration 54, loss = 0.86176932\n",
      "Iteration 55, loss = 0.86192070\n",
      "Iteration 56, loss = 0.86088713\n",
      "Iteration 57, loss = 0.85830942\n",
      "Iteration 58, loss = 0.85782000\n",
      "Iteration 59, loss = 0.85757342\n",
      "Iteration 60, loss = 0.85615679\n",
      "Iteration 61, loss = 0.85635734\n",
      "Iteration 62, loss = 0.85361097\n",
      "Iteration 63, loss = 0.85280295\n",
      "Iteration 64, loss = 0.85217057\n",
      "Iteration 65, loss = 0.85218792\n",
      "Iteration 66, loss = 0.85030738\n",
      "Iteration 67, loss = 0.84894936\n",
      "Iteration 68, loss = 0.84817100\n",
      "Iteration 69, loss = 0.84733586\n",
      "Iteration 70, loss = 0.84725899\n",
      "Iteration 71, loss = 0.84667245\n",
      "Iteration 72, loss = 0.84506136\n",
      "Iteration 73, loss = 0.84444338\n",
      "Iteration 74, loss = 0.84476462\n",
      "Iteration 75, loss = 0.84388401\n",
      "Iteration 76, loss = 0.84262712\n",
      "Iteration 77, loss = 0.84179905\n",
      "Iteration 78, loss = 0.84181120\n",
      "Iteration 79, loss = 0.84054374\n",
      "Iteration 80, loss = 0.84035123\n",
      "Iteration 81, loss = 0.83920920\n",
      "Iteration 82, loss = 0.83842959\n",
      "Iteration 83, loss = 0.83719471\n",
      "Iteration 84, loss = 0.83746695\n",
      "Iteration 85, loss = 0.83657667\n",
      "Iteration 86, loss = 0.83540446\n",
      "Iteration 87, loss = 0.83494859\n",
      "Iteration 88, loss = 0.83463730\n",
      "Iteration 89, loss = 0.83385309\n",
      "Iteration 90, loss = 0.83319342\n",
      "Iteration 91, loss = 0.83297551\n",
      "Iteration 92, loss = 0.83278173\n",
      "Iteration 93, loss = 0.83137647\n",
      "Iteration 94, loss = 0.83245688\n",
      "Iteration 95, loss = 0.83010605\n",
      "Iteration 96, loss = 0.83095221\n",
      "Iteration 97, loss = 0.83005889\n",
      "Iteration 98, loss = 0.83062301\n",
      "Iteration 99, loss = 0.82859759\n",
      "Iteration 100, loss = 0.82749959\n",
      "Iteration 101, loss = 0.82925427\n",
      "Iteration 102, loss = 0.82729283\n",
      "Iteration 103, loss = 0.82719823\n",
      "Iteration 104, loss = 0.82623151\n",
      "Iteration 105, loss = 0.82540642\n",
      "Iteration 106, loss = 0.82546921\n",
      "Iteration 107, loss = 0.82396219\n",
      "Iteration 108, loss = 0.82395585\n",
      "Iteration 109, loss = 0.82514260\n",
      "Iteration 110, loss = 0.82370736\n",
      "Iteration 111, loss = 0.82265245\n",
      "Iteration 112, loss = 0.82383176\n",
      "Iteration 113, loss = 0.82220752\n",
      "Iteration 114, loss = 0.82235583\n",
      "Iteration 115, loss = 0.82173163\n",
      "Iteration 116, loss = 0.82180271\n",
      "Iteration 117, loss = 0.82043003\n",
      "Iteration 118, loss = 0.81984742\n",
      "Iteration 119, loss = 0.82126428\n",
      "Iteration 120, loss = 0.81882258\n",
      "Iteration 121, loss = 0.81907872\n",
      "Iteration 122, loss = 0.82047679\n",
      "Iteration 123, loss = 0.81789857\n",
      "Iteration 124, loss = 0.81957496\n",
      "Iteration 125, loss = 0.81736680\n",
      "Iteration 126, loss = 0.81730063\n",
      "Iteration 127, loss = 0.81640022\n",
      "Iteration 128, loss = 0.81537447\n",
      "Iteration 129, loss = 0.81650600\n",
      "Iteration 130, loss = 0.81550909\n",
      "Iteration 131, loss = 0.81729855\n",
      "Iteration 132, loss = 0.81501636\n",
      "Iteration 133, loss = 0.81502941\n",
      "Iteration 134, loss = 0.81489234\n",
      "Iteration 135, loss = 0.81510864\n",
      "Iteration 136, loss = 0.81382575\n",
      "Iteration 137, loss = 0.81375668\n",
      "Iteration 138, loss = 0.81171843\n",
      "Iteration 139, loss = 0.81326200\n",
      "Iteration 140, loss = 0.81210352\n",
      "Iteration 141, loss = 0.81190402\n",
      "Iteration 142, loss = 0.81293678\n",
      "Iteration 143, loss = 0.81192452\n",
      "Iteration 144, loss = 0.81145046\n",
      "Iteration 145, loss = 0.81058254\n",
      "Iteration 146, loss = 0.80869603\n",
      "Iteration 147, loss = 0.80925076\n",
      "Iteration 148, loss = 0.80961705\n",
      "Iteration 149, loss = 0.80893128\n",
      "Iteration 150, loss = 0.80847528\n",
      "Iteration 151, loss = 0.80966478\n",
      "Iteration 152, loss = 0.80738731\n",
      "Iteration 153, loss = 0.80803218\n",
      "Iteration 154, loss = 0.80794388\n",
      "Iteration 155, loss = 0.80802643\n",
      "Iteration 156, loss = 0.80653695\n",
      "Iteration 157, loss = 0.80558710\n",
      "Iteration 158, loss = 0.80762625\n",
      "Iteration 159, loss = 0.80684977\n",
      "Iteration 160, loss = 0.80605726\n",
      "Iteration 161, loss = 0.80513950\n",
      "Iteration 162, loss = 0.80554434\n",
      "Iteration 163, loss = 0.80504855\n",
      "Iteration 164, loss = 0.80510938\n",
      "Iteration 165, loss = 0.80448003\n",
      "Iteration 166, loss = 0.80415887\n",
      "Iteration 167, loss = 0.80361398\n",
      "Iteration 168, loss = 0.80406463\n",
      "Iteration 169, loss = 0.80285311\n",
      "Iteration 170, loss = 0.80306044\n",
      "Iteration 171, loss = 0.80243184\n",
      "Iteration 172, loss = 0.80118281\n",
      "Iteration 173, loss = 0.80270913\n",
      "Iteration 174, loss = 0.80144671\n",
      "Iteration 175, loss = 0.80158404\n",
      "Iteration 176, loss = 0.80162813\n",
      "Iteration 177, loss = 0.80087703\n",
      "Iteration 178, loss = 0.80103393\n",
      "Iteration 179, loss = 0.80041083\n",
      "Iteration 180, loss = 0.80009841\n",
      "Iteration 181, loss = 0.80046370\n",
      "Iteration 182, loss = 0.79967387\n",
      "Iteration 183, loss = 0.79992313\n",
      "Iteration 184, loss = 0.80017657\n",
      "Iteration 185, loss = 0.79947761\n",
      "Iteration 186, loss = 0.79945334\n",
      "Iteration 187, loss = 0.79842083\n",
      "Iteration 188, loss = 0.79963531\n",
      "Iteration 189, loss = 0.79956727\n",
      "Iteration 190, loss = 0.79796689\n",
      "Iteration 191, loss = 0.79773251\n",
      "Iteration 192, loss = 0.79754255\n",
      "Iteration 193, loss = 0.79837388\n",
      "Iteration 194, loss = 0.79755001\n",
      "Iteration 195, loss = 0.79686152\n",
      "Iteration 196, loss = 0.79699029\n",
      "Iteration 197, loss = 0.79634443\n",
      "Iteration 198, loss = 0.79607048\n",
      "Iteration 199, loss = 0.79513546\n",
      "Iteration 200, loss = 0.79615230\n",
      "Iteration 201, loss = 0.79540240\n",
      "Iteration 202, loss = 0.79562948\n",
      "Iteration 203, loss = 0.79469854\n",
      "Iteration 204, loss = 0.79574447\n",
      "Iteration 205, loss = 0.79418262\n",
      "Iteration 206, loss = 0.79535036\n",
      "Iteration 207, loss = 0.79416531\n",
      "Iteration 208, loss = 0.79383686\n",
      "Iteration 209, loss = 0.79451679\n",
      "Iteration 210, loss = 0.79380676\n",
      "Iteration 211, loss = 0.79373794\n",
      "Iteration 212, loss = 0.79343633\n",
      "Iteration 213, loss = 0.79473627\n",
      "Iteration 214, loss = 0.79367161\n",
      "Iteration 215, loss = 0.79365036\n",
      "Iteration 216, loss = 0.79310743\n",
      "Iteration 217, loss = 0.79152644\n",
      "Iteration 218, loss = 0.79216337\n",
      "Iteration 219, loss = 0.79116772\n",
      "Iteration 220, loss = 0.79161956\n",
      "Iteration 221, loss = 0.79244555\n",
      "Iteration 222, loss = 0.79107604\n",
      "Iteration 223, loss = 0.79114972\n",
      "Iteration 224, loss = 0.79187346\n",
      "Iteration 225, loss = 0.79094811\n",
      "Iteration 226, loss = 0.79003527\n",
      "Iteration 227, loss = 0.79142032\n",
      "Iteration 228, loss = 0.79080799\n",
      "Iteration 229, loss = 0.78991996\n",
      "Iteration 230, loss = 0.79176177\n",
      "Iteration 231, loss = 0.79042955\n",
      "Iteration 232, loss = 0.79153402\n",
      "Iteration 233, loss = 0.79003893\n",
      "Iteration 234, loss = 0.78993380\n",
      "Iteration 235, loss = 0.78854663\n",
      "Iteration 236, loss = 0.78852403\n",
      "Iteration 237, loss = 0.79050831\n",
      "Iteration 238, loss = 0.78816923\n",
      "Iteration 239, loss = 0.78963006\n",
      "Iteration 240, loss = 0.78865953\n",
      "Iteration 241, loss = 0.78872252\n",
      "Iteration 242, loss = 0.78942047\n",
      "Iteration 243, loss = 0.78895188\n",
      "Iteration 244, loss = 0.78915573\n",
      "Iteration 245, loss = 0.78746254\n",
      "Iteration 246, loss = 0.78759746\n",
      "Iteration 247, loss = 0.78867621\n",
      "Iteration 248, loss = 0.78817399\n",
      "Iteration 249, loss = 0.78666008\n",
      "Iteration 250, loss = 0.78776620\n",
      "Iteration 251, loss = 0.78757784\n",
      "Iteration 252, loss = 0.78636611\n",
      "Iteration 253, loss = 0.78749484\n",
      "Iteration 254, loss = 0.78712605\n",
      "Iteration 255, loss = 0.78612939\n",
      "Iteration 256, loss = 0.78682204\n",
      "Iteration 257, loss = 0.78716966\n",
      "Iteration 258, loss = 0.78600573\n",
      "Iteration 259, loss = 0.78485377\n",
      "Iteration 260, loss = 0.78507422\n",
      "Iteration 261, loss = 0.78550387\n",
      "Iteration 262, loss = 0.78645008\n",
      "Iteration 263, loss = 0.78487377\n",
      "Iteration 264, loss = 0.78569839\n",
      "Iteration 265, loss = 0.78492571\n",
      "Iteration 266, loss = 0.78546790\n",
      "Iteration 267, loss = 0.78540090\n",
      "Iteration 268, loss = 0.78574232\n",
      "Iteration 269, loss = 0.78465398\n",
      "Iteration 270, loss = 0.78412538\n",
      "Iteration 271, loss = 0.78487091\n",
      "Iteration 272, loss = 0.78382287\n",
      "Iteration 273, loss = 0.78450392\n",
      "Iteration 274, loss = 0.78392468\n",
      "Iteration 275, loss = 0.78336353\n",
      "Iteration 276, loss = 0.78284802\n",
      "Iteration 277, loss = 0.78315497\n",
      "Iteration 278, loss = 0.78325814\n",
      "Iteration 279, loss = 0.78338680\n",
      "Iteration 280, loss = 0.78246197\n",
      "Iteration 281, loss = 0.78284226\n",
      "Iteration 282, loss = 0.78316201\n",
      "Iteration 283, loss = 0.78290571\n",
      "Iteration 284, loss = 0.78270845\n",
      "Iteration 285, loss = 0.78180176\n",
      "Iteration 286, loss = 0.78252556\n",
      "Iteration 287, loss = 0.78324494\n",
      "Iteration 288, loss = 0.78188673\n",
      "Iteration 289, loss = 0.78163836\n",
      "Iteration 290, loss = 0.78205317\n",
      "Iteration 291, loss = 0.78149849\n",
      "Iteration 292, loss = 0.78208522\n",
      "Iteration 293, loss = 0.78189306\n",
      "Iteration 294, loss = 0.78109729\n",
      "Iteration 295, loss = 0.78138601\n",
      "Iteration 296, loss = 0.78056199\n",
      "Iteration 297, loss = 0.78087675\n",
      "Iteration 298, loss = 0.78074942\n",
      "Iteration 299, loss = 0.78123412\n",
      "Iteration 300, loss = 0.77893651\n",
      "Iteration 1, loss = 2.64632425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiergallo/miniconda3/envs/diplodatos/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.52961685\n",
      "Iteration 3, loss = 1.26150181\n",
      "Iteration 4, loss = 1.16491556\n",
      "Iteration 5, loss = 1.10943807\n",
      "Iteration 6, loss = 1.07438193\n",
      "Iteration 7, loss = 1.04933488\n",
      "Iteration 8, loss = 1.02972078\n",
      "Iteration 9, loss = 1.01312534\n",
      "Iteration 10, loss = 1.00120592\n",
      "Iteration 11, loss = 0.98916749\n",
      "Iteration 12, loss = 0.98121889\n",
      "Iteration 13, loss = 0.97133273\n",
      "Iteration 14, loss = 0.96338130\n",
      "Iteration 15, loss = 0.95643144\n",
      "Iteration 16, loss = 0.94946519\n",
      "Iteration 17, loss = 0.94294302\n",
      "Iteration 18, loss = 0.93770478\n",
      "Iteration 19, loss = 0.93298880\n",
      "Iteration 20, loss = 0.92850879\n",
      "Iteration 21, loss = 0.92349849\n",
      "Iteration 22, loss = 0.91904488\n",
      "Iteration 23, loss = 0.91528045\n",
      "Iteration 24, loss = 0.91143937\n",
      "Iteration 25, loss = 0.90853880\n",
      "Iteration 26, loss = 0.90675774\n",
      "Iteration 27, loss = 0.90287850\n",
      "Iteration 28, loss = 0.89899336\n",
      "Iteration 29, loss = 0.89837907\n",
      "Iteration 30, loss = 0.89402717\n",
      "Iteration 31, loss = 0.89202771\n",
      "Iteration 32, loss = 0.88921351\n",
      "Iteration 33, loss = 0.88611923\n",
      "Iteration 34, loss = 0.88405112\n",
      "Iteration 35, loss = 0.88266681\n",
      "Iteration 36, loss = 0.87985492\n",
      "Iteration 37, loss = 0.88045202\n",
      "Iteration 38, loss = 0.87735432\n",
      "Iteration 39, loss = 0.87502833\n",
      "Iteration 40, loss = 0.87390456\n",
      "Iteration 41, loss = 0.87237901\n",
      "Iteration 42, loss = 0.87072265\n",
      "Iteration 43, loss = 0.86850949\n",
      "Iteration 44, loss = 0.86789097\n",
      "Iteration 45, loss = 0.86589727\n",
      "Iteration 46, loss = 0.86512171\n",
      "Iteration 47, loss = 0.86412258\n",
      "Iteration 48, loss = 0.86360263\n",
      "Iteration 49, loss = 0.86192112\n",
      "Iteration 50, loss = 0.86070017\n",
      "Iteration 51, loss = 0.85885650\n",
      "Iteration 52, loss = 0.85719383\n",
      "Iteration 53, loss = 0.85658757\n",
      "Iteration 54, loss = 0.85661069\n",
      "Iteration 55, loss = 0.85605206\n",
      "Iteration 56, loss = 0.85310748\n",
      "Iteration 57, loss = 0.85275810\n",
      "Iteration 58, loss = 0.85168594\n",
      "Iteration 59, loss = 0.84959601\n",
      "Iteration 60, loss = 0.85113548\n",
      "Iteration 61, loss = 0.84904059\n",
      "Iteration 62, loss = 0.84725856\n",
      "Iteration 63, loss = 0.84809918\n",
      "Iteration 64, loss = 0.84719480\n",
      "Iteration 65, loss = 0.84552930\n",
      "Iteration 66, loss = 0.84441376\n",
      "Iteration 67, loss = 0.84328413\n",
      "Iteration 68, loss = 0.84373164\n",
      "Iteration 69, loss = 0.84299911\n",
      "Iteration 70, loss = 0.84237534\n",
      "Iteration 71, loss = 0.84187447\n",
      "Iteration 72, loss = 0.84048108\n",
      "Iteration 73, loss = 0.83976458\n",
      "Iteration 74, loss = 0.83816930\n",
      "Iteration 75, loss = 0.83878451\n",
      "Iteration 76, loss = 0.83789844\n",
      "Iteration 77, loss = 0.83740700\n",
      "Iteration 78, loss = 0.83618403\n",
      "Iteration 79, loss = 0.83648167\n",
      "Iteration 80, loss = 0.83527383\n",
      "Iteration 81, loss = 0.83447938\n",
      "Iteration 82, loss = 0.83315238\n",
      "Iteration 83, loss = 0.83452628\n",
      "Iteration 84, loss = 0.83292675\n",
      "Iteration 85, loss = 0.83323952\n",
      "Iteration 86, loss = 0.83249805\n",
      "Iteration 87, loss = 0.83095042\n",
      "Iteration 88, loss = 0.83151150\n",
      "Iteration 89, loss = 0.83150416\n",
      "Iteration 90, loss = 0.83036825\n",
      "Iteration 91, loss = 0.82886268\n",
      "Iteration 92, loss = 0.82919467\n",
      "Iteration 93, loss = 0.82815940\n",
      "Iteration 94, loss = 0.82943721\n",
      "Iteration 95, loss = 0.82713959\n",
      "Iteration 96, loss = 0.82799785\n",
      "Iteration 97, loss = 0.82678429\n",
      "Iteration 98, loss = 0.82556217\n",
      "Iteration 99, loss = 0.82644962\n",
      "Iteration 100, loss = 0.82465747\n",
      "Iteration 101, loss = 0.82474740\n",
      "Iteration 102, loss = 0.82396427\n",
      "Iteration 103, loss = 0.82374751\n",
      "Iteration 104, loss = 0.82402201\n",
      "Iteration 105, loss = 0.82328326\n",
      "Iteration 106, loss = 0.82320028\n",
      "Iteration 107, loss = 0.82210189\n",
      "Iteration 108, loss = 0.82183115\n",
      "Iteration 109, loss = 0.82177708\n",
      "Iteration 110, loss = 0.82088881\n",
      "Iteration 111, loss = 0.82009935\n",
      "Iteration 112, loss = 0.82071926\n",
      "Iteration 113, loss = 0.81996272\n",
      "Iteration 114, loss = 0.82027003\n",
      "Iteration 115, loss = 0.81948049\n",
      "Iteration 116, loss = 0.81910158\n",
      "Iteration 117, loss = 0.81816714\n",
      "Iteration 118, loss = 0.81796623\n",
      "Iteration 119, loss = 0.81710983\n",
      "Iteration 120, loss = 0.81709289\n",
      "Iteration 121, loss = 0.81730904\n",
      "Iteration 122, loss = 0.81598484\n",
      "Iteration 123, loss = 0.81582539\n",
      "Iteration 124, loss = 0.81766522\n",
      "Iteration 125, loss = 0.81561949\n",
      "Iteration 126, loss = 0.81554127\n",
      "Iteration 127, loss = 0.81470941\n",
      "Iteration 128, loss = 0.81316937\n",
      "Iteration 129, loss = 0.81467963\n",
      "Iteration 130, loss = 0.81320158\n",
      "Iteration 131, loss = 0.81372379\n",
      "Iteration 132, loss = 0.81226535\n",
      "Iteration 133, loss = 0.81229918\n",
      "Iteration 134, loss = 0.81285042\n",
      "Iteration 135, loss = 0.81330429\n",
      "Iteration 136, loss = 0.81218514\n",
      "Iteration 137, loss = 0.81119231\n",
      "Iteration 138, loss = 0.81015667\n",
      "Iteration 139, loss = 0.81100099\n",
      "Iteration 140, loss = 0.81029682\n",
      "Iteration 141, loss = 0.81052193\n",
      "Iteration 142, loss = 0.81048064\n",
      "Iteration 143, loss = 0.80892851\n",
      "Iteration 144, loss = 0.81076412\n",
      "Iteration 145, loss = 0.80942288\n",
      "Iteration 146, loss = 0.80845253\n",
      "Iteration 147, loss = 0.80822320\n",
      "Iteration 148, loss = 0.80805246\n",
      "Iteration 149, loss = 0.80866729\n",
      "Iteration 150, loss = 0.80701729\n",
      "Iteration 151, loss = 0.80683291\n",
      "Iteration 152, loss = 0.80667867\n",
      "Iteration 153, loss = 0.80650253\n",
      "Iteration 154, loss = 0.80639089\n",
      "Iteration 155, loss = 0.80623059\n",
      "Iteration 156, loss = 0.80617203\n",
      "Iteration 157, loss = 0.80426192\n",
      "Iteration 158, loss = 0.80490046\n",
      "Iteration 159, loss = 0.80499162\n",
      "Iteration 160, loss = 0.80542366\n",
      "Iteration 161, loss = 0.80446433\n",
      "Iteration 162, loss = 0.80439304\n",
      "Iteration 163, loss = 0.80274942\n",
      "Iteration 164, loss = 0.80329128\n",
      "Iteration 165, loss = 0.80322829\n",
      "Iteration 166, loss = 0.80285092\n",
      "Iteration 167, loss = 0.80287923\n",
      "Iteration 168, loss = 0.80276438\n",
      "Iteration 169, loss = 0.80272525\n",
      "Iteration 170, loss = 0.80251839\n",
      "Iteration 171, loss = 0.80168807\n",
      "Iteration 172, loss = 0.80151631\n",
      "Iteration 173, loss = 0.80191354\n",
      "Iteration 174, loss = 0.80123130\n",
      "Iteration 175, loss = 0.80017593\n",
      "Iteration 176, loss = 0.80016676\n",
      "Iteration 177, loss = 0.80044076\n",
      "Iteration 178, loss = 0.79921177\n",
      "Iteration 179, loss = 0.80003641\n",
      "Iteration 180, loss = 0.79933519\n",
      "Iteration 181, loss = 0.79887311\n",
      "Iteration 182, loss = 0.79822347\n",
      "Iteration 183, loss = 0.79828365\n",
      "Iteration 184, loss = 0.79877410\n",
      "Iteration 185, loss = 0.79720669\n",
      "Iteration 186, loss = 0.79782426\n",
      "Iteration 187, loss = 0.79761680\n",
      "Iteration 188, loss = 0.79899385\n",
      "Iteration 189, loss = 0.79696724\n",
      "Iteration 190, loss = 0.79723944\n",
      "Iteration 191, loss = 0.79516539\n",
      "Iteration 192, loss = 0.79606770\n",
      "Iteration 193, loss = 0.79680853\n",
      "Iteration 194, loss = 0.79671211\n",
      "Iteration 195, loss = 0.79557339\n",
      "Iteration 196, loss = 0.79481472\n",
      "Iteration 197, loss = 0.79588176\n",
      "Iteration 198, loss = 0.79488275\n",
      "Iteration 199, loss = 0.79373509\n",
      "Iteration 200, loss = 0.79461468\n",
      "Iteration 201, loss = 0.79421095\n",
      "Iteration 202, loss = 0.79405638\n",
      "Iteration 203, loss = 0.79339972\n",
      "Iteration 204, loss = 0.79414499\n",
      "Iteration 205, loss = 0.79409663\n",
      "Iteration 206, loss = 0.79326546\n",
      "Iteration 207, loss = 0.79326140\n",
      "Iteration 208, loss = 0.79260826\n",
      "Iteration 209, loss = 0.79343099\n",
      "Iteration 210, loss = 0.79314432\n",
      "Iteration 211, loss = 0.79270119\n",
      "Iteration 212, loss = 0.79176845\n",
      "Iteration 213, loss = 0.79199016\n",
      "Iteration 214, loss = 0.79159734\n",
      "Iteration 215, loss = 0.79172051\n",
      "Iteration 216, loss = 0.79221405\n",
      "Iteration 217, loss = 0.79074422\n",
      "Iteration 218, loss = 0.79068713\n",
      "Iteration 219, loss = 0.78969437\n",
      "Iteration 220, loss = 0.79026098\n",
      "Iteration 221, loss = 0.79031055\n",
      "Iteration 222, loss = 0.78969027\n",
      "Iteration 223, loss = 0.78918557\n",
      "Iteration 224, loss = 0.78958185\n",
      "Iteration 225, loss = 0.78960368\n",
      "Iteration 226, loss = 0.78906300\n",
      "Iteration 227, loss = 0.78930238\n",
      "Iteration 228, loss = 0.78899567\n",
      "Iteration 229, loss = 0.78858684\n",
      "Iteration 230, loss = 0.78836645\n",
      "Iteration 231, loss = 0.78855387\n",
      "Iteration 232, loss = 0.78749589\n",
      "Iteration 233, loss = 0.78821479\n",
      "Iteration 234, loss = 0.78870278\n",
      "Iteration 235, loss = 0.78732327\n",
      "Iteration 236, loss = 0.78693785\n",
      "Iteration 237, loss = 0.78732857\n",
      "Iteration 238, loss = 0.78817966\n",
      "Iteration 239, loss = 0.78828861\n",
      "Iteration 240, loss = 0.78687006\n",
      "Iteration 241, loss = 0.78769975\n",
      "Iteration 242, loss = 0.78735948\n",
      "Iteration 243, loss = 0.78661168\n",
      "Iteration 244, loss = 0.78644203\n",
      "Iteration 245, loss = 0.78527302\n",
      "Iteration 246, loss = 0.78572791\n",
      "Iteration 247, loss = 0.78566206\n",
      "Iteration 248, loss = 0.78528272\n",
      "Iteration 249, loss = 0.78596036\n",
      "Iteration 250, loss = 0.78539070\n",
      "Iteration 251, loss = 0.78602482\n",
      "Iteration 252, loss = 0.78459640\n",
      "Iteration 253, loss = 0.78428393\n",
      "Iteration 254, loss = 0.78424044\n",
      "Iteration 255, loss = 0.78398626\n",
      "Iteration 256, loss = 0.78435490\n",
      "Iteration 257, loss = 0.78394224\n",
      "Iteration 258, loss = 0.78436728\n",
      "Iteration 259, loss = 0.78381336\n",
      "Iteration 260, loss = 0.78329720\n",
      "Iteration 261, loss = 0.78335770\n",
      "Iteration 262, loss = 0.78369558\n",
      "Iteration 263, loss = 0.78334613\n",
      "Iteration 264, loss = 0.78387018\n",
      "Iteration 265, loss = 0.78301919\n",
      "Iteration 266, loss = 0.78288905\n",
      "Iteration 267, loss = 0.78298241\n",
      "Iteration 268, loss = 0.78392210\n",
      "Iteration 269, loss = 0.78231666\n",
      "Iteration 270, loss = 0.78264311\n",
      "Iteration 271, loss = 0.78259023\n",
      "Iteration 272, loss = 0.78197109\n",
      "Iteration 273, loss = 0.78118274\n",
      "Iteration 274, loss = 0.78206909\n",
      "Iteration 275, loss = 0.78140448\n",
      "Iteration 276, loss = 0.78151131\n",
      "Iteration 277, loss = 0.78195453\n",
      "Iteration 278, loss = 0.78198801\n",
      "Iteration 279, loss = 0.77968152\n",
      "Iteration 280, loss = 0.78120443\n",
      "Iteration 281, loss = 0.78104378\n",
      "Iteration 282, loss = 0.78043258\n",
      "Iteration 283, loss = 0.78284232\n",
      "Iteration 284, loss = 0.78042947\n",
      "Iteration 285, loss = 0.77942544\n",
      "Iteration 286, loss = 0.78034088\n",
      "Iteration 287, loss = 0.77968111\n",
      "Iteration 288, loss = 0.77965931\n",
      "Iteration 289, loss = 0.77940481\n",
      "Iteration 290, loss = 0.77891122\n",
      "Iteration 291, loss = 0.77885655\n",
      "Iteration 292, loss = 0.78001718\n",
      "Iteration 293, loss = 0.77894735\n",
      "Iteration 294, loss = 0.77812128\n",
      "Iteration 295, loss = 0.77847367\n",
      "Iteration 296, loss = 0.77858626\n",
      "Iteration 297, loss = 0.77868768\n",
      "Iteration 298, loss = 0.77975117\n",
      "Iteration 299, loss = 0.77890909\n",
      "Iteration 300, loss = 0.77717475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiergallo/miniconda3/envs/diplodatos/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31898690\n",
      "Iteration 2, loss = 1.32116981\n",
      "Iteration 3, loss = 1.16118608\n",
      "Iteration 4, loss = 1.09785834\n",
      "Iteration 5, loss = 1.05998432\n",
      "Iteration 6, loss = 1.03507629\n",
      "Iteration 7, loss = 1.01734657\n",
      "Iteration 8, loss = 1.00281886\n",
      "Iteration 9, loss = 0.99041811\n",
      "Iteration 10, loss = 0.97850160\n",
      "Iteration 11, loss = 0.96989282\n",
      "Iteration 12, loss = 0.96188902\n",
      "Iteration 13, loss = 0.95402727\n",
      "Iteration 14, loss = 0.94829042\n",
      "Iteration 15, loss = 0.94195380\n",
      "Iteration 16, loss = 0.93682163\n",
      "Iteration 17, loss = 0.93275311\n",
      "Iteration 18, loss = 0.92706803\n",
      "Iteration 19, loss = 0.92364380\n",
      "Iteration 20, loss = 0.91928078\n",
      "Iteration 21, loss = 0.91635307\n",
      "Iteration 22, loss = 0.91278000\n",
      "Iteration 23, loss = 0.90928386\n",
      "Iteration 24, loss = 0.90627275\n",
      "Iteration 25, loss = 0.90468188\n",
      "Iteration 26, loss = 0.90150412\n",
      "Iteration 27, loss = 0.89934738\n",
      "Iteration 28, loss = 0.89718506\n",
      "Iteration 29, loss = 0.89520978\n",
      "Iteration 30, loss = 0.89381426\n",
      "Iteration 31, loss = 0.89120649\n",
      "Iteration 32, loss = 0.88902928\n",
      "Iteration 33, loss = 0.88848005\n",
      "Iteration 34, loss = 0.88597966\n",
      "Iteration 35, loss = 0.88382675\n",
      "Iteration 36, loss = 0.88278192\n",
      "Iteration 37, loss = 0.88172298\n",
      "Iteration 38, loss = 0.88047302\n",
      "Iteration 39, loss = 0.87921357\n",
      "Iteration 40, loss = 0.87688949\n",
      "Iteration 41, loss = 0.87679860\n",
      "Iteration 42, loss = 0.87486840\n",
      "Iteration 43, loss = 0.87371184\n",
      "Iteration 44, loss = 0.87201809\n",
      "Iteration 45, loss = 0.87161622\n",
      "Iteration 46, loss = 0.87044608\n",
      "Iteration 47, loss = 0.86948000\n",
      "Iteration 48, loss = 0.86928924\n",
      "Iteration 49, loss = 0.86867295\n",
      "Iteration 50, loss = 0.86644505\n",
      "Iteration 51, loss = 0.86649922\n",
      "Iteration 52, loss = 0.86524314\n",
      "Iteration 53, loss = 0.86502089\n",
      "Iteration 54, loss = 0.86362706\n",
      "Iteration 55, loss = 0.86320451\n",
      "Iteration 56, loss = 0.86250114\n",
      "Iteration 57, loss = 0.86113203\n",
      "Iteration 58, loss = 0.85973801\n",
      "Iteration 59, loss = 0.86010018\n",
      "Iteration 60, loss = 0.85907253\n",
      "Iteration 61, loss = 0.85804040\n",
      "Iteration 62, loss = 0.85860461\n",
      "Iteration 63, loss = 0.85714361\n",
      "Iteration 64, loss = 0.85621991\n",
      "Iteration 65, loss = 0.85641203\n",
      "Iteration 66, loss = 0.85528149\n",
      "Iteration 67, loss = 0.85441130\n",
      "Iteration 68, loss = 0.85526625\n",
      "Iteration 69, loss = 0.85446531\n",
      "Iteration 70, loss = 0.85326568\n",
      "Iteration 71, loss = 0.85253899\n",
      "Iteration 72, loss = 0.85235205\n",
      "Iteration 73, loss = 0.85149523\n",
      "Iteration 74, loss = 0.85103548\n",
      "Iteration 75, loss = 0.84988563\n",
      "Iteration 76, loss = 0.85000910\n",
      "Iteration 77, loss = 0.84905549\n",
      "Iteration 78, loss = 0.84881952\n",
      "Iteration 79, loss = 0.84895997\n",
      "Iteration 80, loss = 0.84803245\n",
      "Iteration 81, loss = 0.84768473\n",
      "Iteration 82, loss = 0.84615667\n",
      "Iteration 83, loss = 0.84597890\n",
      "Iteration 84, loss = 0.84530378\n",
      "Iteration 85, loss = 0.84592718\n",
      "Iteration 86, loss = 0.84526959\n",
      "Iteration 87, loss = 0.84521337\n",
      "Iteration 88, loss = 0.84415769\n",
      "Iteration 89, loss = 0.84415930\n",
      "Iteration 90, loss = 0.84287821\n",
      "Iteration 91, loss = 0.84370215\n",
      "Iteration 92, loss = 0.84326262\n",
      "Iteration 93, loss = 0.84309519\n",
      "Iteration 94, loss = 0.84228410\n",
      "Iteration 95, loss = 0.84184566\n",
      "Iteration 96, loss = 0.84171887\n",
      "Iteration 97, loss = 0.83998043\n",
      "Iteration 98, loss = 0.84107969\n",
      "Iteration 99, loss = 0.84001085\n",
      "Iteration 100, loss = 0.83948889\n",
      "Iteration 101, loss = 0.83996680\n",
      "Iteration 102, loss = 0.83965998\n",
      "Iteration 103, loss = 0.83944227\n",
      "Iteration 104, loss = 0.83830576\n",
      "Iteration 105, loss = 0.83831717\n",
      "Iteration 106, loss = 0.83778327\n",
      "Iteration 107, loss = 0.83743743\n",
      "Iteration 108, loss = 0.83729099\n",
      "Iteration 109, loss = 0.83718747\n",
      "Iteration 110, loss = 0.83667624\n",
      "Iteration 111, loss = 0.83585764\n",
      "Iteration 112, loss = 0.83586965\n",
      "Iteration 113, loss = 0.83546035\n",
      "Iteration 114, loss = 0.83599116\n",
      "Iteration 115, loss = 0.83577475\n",
      "Iteration 116, loss = 0.83501034\n",
      "Iteration 117, loss = 0.83464931\n",
      "Iteration 118, loss = 0.83422929\n",
      "Iteration 119, loss = 0.83444608\n",
      "Iteration 120, loss = 0.83365190\n",
      "Iteration 121, loss = 0.83369736\n",
      "Iteration 122, loss = 0.83320927\n",
      "Iteration 123, loss = 0.83191218\n",
      "Iteration 124, loss = 0.83320624\n",
      "Iteration 125, loss = 0.83189025\n",
      "Iteration 126, loss = 0.83206715\n",
      "Iteration 127, loss = 0.83186825\n",
      "Iteration 128, loss = 0.83156363\n",
      "Iteration 129, loss = 0.83104738\n",
      "Iteration 130, loss = 0.83048840\n",
      "Iteration 131, loss = 0.83150462\n",
      "Iteration 132, loss = 0.83090323\n",
      "Iteration 133, loss = 0.83131387\n",
      "Iteration 134, loss = 0.83041295\n",
      "Iteration 135, loss = 0.83004513\n",
      "Iteration 136, loss = 0.82990586\n",
      "Iteration 137, loss = 0.82975986\n",
      "Iteration 138, loss = 0.82964232\n",
      "Iteration 139, loss = 0.82922652\n",
      "Iteration 140, loss = 0.82898037\n",
      "Iteration 141, loss = 0.82898871\n",
      "Iteration 142, loss = 0.82822537\n",
      "Iteration 143, loss = 0.82877265\n",
      "Iteration 144, loss = 0.82768145\n",
      "Iteration 145, loss = 0.82857187\n",
      "Iteration 146, loss = 0.82848813\n",
      "Iteration 147, loss = 0.82719356\n",
      "Iteration 148, loss = 0.82736830\n",
      "Iteration 149, loss = 0.82652916\n",
      "Iteration 150, loss = 0.82649171\n",
      "Iteration 151, loss = 0.82612146\n",
      "Iteration 152, loss = 0.82684882\n",
      "Iteration 153, loss = 0.82613750\n",
      "Iteration 154, loss = 0.82560941\n",
      "Iteration 155, loss = 0.82613930\n",
      "Iteration 156, loss = 0.82542625\n",
      "Iteration 157, loss = 0.82550281\n",
      "Iteration 158, loss = 0.82540018\n",
      "Iteration 159, loss = 0.82496065\n",
      "Iteration 160, loss = 0.82478371\n",
      "Iteration 161, loss = 0.82509389\n",
      "Iteration 162, loss = 0.82428758\n",
      "Iteration 163, loss = 0.82445581\n",
      "Iteration 164, loss = 0.82397629\n",
      "Iteration 165, loss = 0.82546441\n",
      "Iteration 166, loss = 0.82323108\n",
      "Iteration 167, loss = 0.82301311\n",
      "Iteration 168, loss = 0.82313905\n",
      "Iteration 169, loss = 0.82359429\n",
      "Iteration 170, loss = 0.82283549\n",
      "Iteration 171, loss = 0.82345528\n",
      "Iteration 172, loss = 0.82269056\n",
      "Iteration 173, loss = 0.82254967\n",
      "Iteration 174, loss = 0.82261096\n",
      "Iteration 175, loss = 0.82226486\n",
      "Iteration 176, loss = 0.82210661\n",
      "Iteration 177, loss = 0.82157205\n",
      "Iteration 178, loss = 0.82137012\n",
      "Iteration 179, loss = 0.82207242\n",
      "Iteration 180, loss = 0.82101431\n",
      "Iteration 181, loss = 0.82123081\n",
      "Iteration 182, loss = 0.82230566\n",
      "Iteration 183, loss = 0.82078237\n",
      "Iteration 184, loss = 0.81920066\n",
      "Iteration 185, loss = 0.82083124\n",
      "Iteration 186, loss = 0.81909210\n",
      "Iteration 187, loss = 0.82142735\n",
      "Iteration 188, loss = 0.81971121\n",
      "Iteration 189, loss = 0.81963264\n",
      "Iteration 190, loss = 0.82006549\n",
      "Iteration 191, loss = 0.81979452\n",
      "Iteration 192, loss = 0.81973535\n",
      "Iteration 193, loss = 0.81858775\n",
      "Iteration 194, loss = 0.81946778\n",
      "Iteration 195, loss = 0.81942391\n",
      "Iteration 196, loss = 0.81904705\n",
      "Iteration 197, loss = 0.81866661\n",
      "Iteration 198, loss = 0.81862898\n",
      "Iteration 199, loss = 0.81838137\n",
      "Iteration 200, loss = 0.81996769\n",
      "Iteration 201, loss = 0.81860186\n",
      "Iteration 202, loss = 0.81880214\n",
      "Iteration 203, loss = 0.81762511\n",
      "Iteration 204, loss = 0.81783402\n",
      "Iteration 205, loss = 0.81827381\n",
      "Iteration 206, loss = 0.81678149\n",
      "Iteration 207, loss = 0.81729558\n",
      "Iteration 208, loss = 0.81681904\n",
      "Iteration 209, loss = 0.81720647\n",
      "Iteration 210, loss = 0.81718990\n",
      "Iteration 211, loss = 0.81742894\n",
      "Iteration 212, loss = 0.81633674\n",
      "Iteration 213, loss = 0.81689428\n",
      "Iteration 214, loss = 0.81648936\n",
      "Iteration 215, loss = 0.81676656\n",
      "Iteration 216, loss = 0.81605244\n",
      "Iteration 217, loss = 0.81694269\n",
      "Iteration 218, loss = 0.81630507\n",
      "Iteration 219, loss = 0.81645240\n",
      "Iteration 220, loss = 0.81637973\n",
      "Iteration 221, loss = 0.81522853\n",
      "Iteration 222, loss = 0.81622973\n",
      "Iteration 223, loss = 0.81577041\n",
      "Iteration 224, loss = 0.81524578\n",
      "Iteration 225, loss = 0.81596972\n",
      "Iteration 226, loss = 0.81609173\n",
      "Iteration 227, loss = 0.81470676\n",
      "Iteration 228, loss = 0.81516653\n",
      "Iteration 229, loss = 0.81428759\n",
      "Iteration 230, loss = 0.81459955\n",
      "Iteration 231, loss = 0.81440882\n",
      "Iteration 232, loss = 0.81535295\n",
      "Iteration 233, loss = 0.81354240\n",
      "Iteration 234, loss = 0.81417334\n",
      "Iteration 235, loss = 0.81394426\n",
      "Iteration 236, loss = 0.81361254\n",
      "Iteration 237, loss = 0.81386942\n",
      "Iteration 238, loss = 0.81430468\n",
      "Iteration 239, loss = 0.81397775\n",
      "Iteration 240, loss = 0.81369920\n",
      "Iteration 241, loss = 0.81341181\n",
      "Iteration 242, loss = 0.81353774\n",
      "Iteration 243, loss = 0.81352567\n",
      "Iteration 244, loss = 0.81368152\n",
      "Iteration 245, loss = 0.81317830\n",
      "Iteration 246, loss = 0.81286204\n",
      "Iteration 247, loss = 0.81236186\n",
      "Iteration 248, loss = 0.81283016\n",
      "Iteration 249, loss = 0.81229377\n",
      "Iteration 250, loss = 0.81382805\n",
      "Iteration 251, loss = 0.81199187\n",
      "Iteration 252, loss = 0.81261705\n",
      "Iteration 253, loss = 0.81177242\n",
      "Iteration 254, loss = 0.81213494\n",
      "Iteration 255, loss = 0.81303222\n",
      "Iteration 256, loss = 0.81237029\n",
      "Iteration 257, loss = 0.81193058\n",
      "Iteration 258, loss = 0.81334638\n",
      "Iteration 259, loss = 0.81186408\n",
      "Iteration 260, loss = 0.81164992\n",
      "Iteration 261, loss = 0.81214470\n",
      "Iteration 262, loss = 0.81156858\n",
      "Iteration 263, loss = 0.81197700\n",
      "Iteration 264, loss = 0.81111901\n",
      "Iteration 265, loss = 0.81101153\n",
      "Iteration 266, loss = 0.81069957\n",
      "Iteration 267, loss = 0.81041962\n",
      "Iteration 268, loss = 0.80967581\n",
      "Iteration 269, loss = 0.81109444\n",
      "Iteration 270, loss = 0.81001461\n",
      "Iteration 271, loss = 0.81066251\n",
      "Iteration 272, loss = 0.80947034\n",
      "Iteration 273, loss = 0.81065979\n",
      "Iteration 274, loss = 0.81075108\n",
      "Iteration 275, loss = 0.81007015\n",
      "Iteration 276, loss = 0.81011657\n",
      "Iteration 277, loss = 0.81010392\n",
      "Iteration 278, loss = 0.80956424\n",
      "Iteration 279, loss = 0.81024439\n",
      "Iteration 280, loss = 0.80910045\n",
      "Iteration 281, loss = 0.80936157\n",
      "Iteration 282, loss = 0.80980798\n",
      "Iteration 283, loss = 0.80879342\n",
      "Iteration 284, loss = 0.80958838\n",
      "Iteration 285, loss = 0.81059989\n",
      "Iteration 286, loss = 0.81037898\n",
      "Iteration 287, loss = 0.80940497\n",
      "Iteration 288, loss = 0.80881805\n",
      "Iteration 289, loss = 0.80870214\n",
      "Iteration 290, loss = 0.80866680\n",
      "Iteration 291, loss = 0.80850087\n",
      "Iteration 292, loss = 0.80861494\n",
      "Iteration 293, loss = 0.80870514\n",
      "Iteration 294, loss = 0.80968364\n",
      "Iteration 295, loss = 0.80824292\n",
      "Iteration 296, loss = 0.80840915\n",
      "Iteration 297, loss = 0.80810818\n",
      "Iteration 298, loss = 0.80933107\n",
      "Iteration 299, loss = 0.80807230\n",
      "Iteration 300, loss = 0.80851451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiergallo/miniconda3/envs/diplodatos/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Original:\n",
    "model_param = {\n",
    "    'criterion': ('gini', 'entropy'),\n",
    "    'min_samples_leaf': (1, 2, 5),\n",
    "    'min_samples_split': (2, 3, 5, 10, 50, 100)\n",
    "}\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Best models so far (from better to worse):\n",
    "MLPClassifier(\n",
    "    alpha=0.03,\n",
    "    hidden_layer_sizes=(96,),\n",
    "    max_iter=300,\n",
    "    n_iter_no_change=30,\n",
    "    random_state=42\n",
    ")\n",
    "MLPClassifier(\n",
    "    alpha=0.02,\n",
    "    hidden_layer_sizes=(64,),\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=101,\n",
    "    max_depth=59,\n",
    "    class_weight={999: 0.49}\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "model_param = {}\n",
    "model = MLPClassifier(random_state=42,\n",
    "                      verbose=True,\n",
    "                      max_iter=300,\n",
    "                      alpha=0.03,\n",
    "                      # learning_rate_init=0.002,\n",
    "                      n_iter_no_change=30,\n",
    "                      hidden_layer_sizes=(96,))\n",
    "\n",
    "search = GridSearchCV(model, model_param, cv=3, scoring='accuracy') # scoring='balanced_accuracy'\n",
    "\n",
    "# NOTICE we exclude visit number; all it does is confuse our model.\n",
    "search.fit(X_train.drop(columns=['VisitNumber']), y_train)\n",
    "\n",
    "best_model_clf = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Decision Tree accuracy:  0.6926257459505542\n",
      "MLPClassifier(alpha=0.03, hidden_layer_sizes=(96,), max_iter=300,\n",
      "              n_iter_no_change=30, random_state=42, verbose=True)\n",
      "The best classifier so far is: \n",
      "MLPClassifier(alpha=0.03, hidden_layer_sizes=(96,), max_iter=300,\n",
      "              n_iter_no_change=30, random_state=42, verbose=True)\n"
     ]
    }
   ],
   "source": [
    "print('Best Decision Tree accuracy: ', search.best_score_)\n",
    "print(best_model_clf)\n",
    "results = results.append({'clf': best_model_clf, 'best_acc': search.best_score_}, ignore_index=True)\n",
    "\n",
    "print('The best classifier so far is: ')\n",
    "best_clf = results.loc[results['best_acc'].idxmax()]['clf']\n",
    "print(best_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clf</th>\n",
       "      <th>best_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier(class_weight={999: 0.49...</td>\n",
       "      <td>0.634420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLPClassifier(alpha=0.01, hidden_layer_sizes=(...</td>\n",
       "      <td>0.687127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLPClassifier(alpha=0.01, hidden_layer_sizes=(...</td>\n",
       "      <td>0.673934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(64,), learni...</td>\n",
       "      <td>0.665196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLPClassifier(alpha=0.01, hidden_layer_sizes=(...</td>\n",
       "      <td>0.685060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLPClassifier(alpha=0.01, hidden_layer_sizes=(...</td>\n",
       "      <td>0.670716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(64,), max_it...</td>\n",
       "      <td>0.671057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(64,), max_it...</td>\n",
       "      <td>0.680733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(64,), learni...</td>\n",
       "      <td>0.672101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(64,), learni...</td>\n",
       "      <td>0.673039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MLPClassifier(alpha=0.01, hidden_layer_sizes=(...</td>\n",
       "      <td>0.681436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MLPClassifier(alpha=0.02, hidden_layer_sizes=(...</td>\n",
       "      <td>0.688107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLPClassifier(alpha=0.02, hidden_layer_sizes=(...</td>\n",
       "      <td>0.687980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MLPClassifier(alpha=0.02, hidden_layer_sizes=(...</td>\n",
       "      <td>0.677217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MLPClassifier(alpha=0.02, hidden_layer_sizes=(...</td>\n",
       "      <td>0.683717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MLPClassifier(alpha=0.02, hidden_layer_sizes=(...</td>\n",
       "      <td>0.674638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MLPClassifier(alpha=0.0002, hidden_layer_sizes...</td>\n",
       "      <td>0.656245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MLPClassifier(alpha=0.003, hidden_layer_sizes=...</td>\n",
       "      <td>0.667711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MLPClassifier(alpha=0.03, hidden_layer_sizes=(...</td>\n",
       "      <td>0.692626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  clf  best_acc\n",
       "0   DecisionTreeClassifier(class_weight={999: 0.49...  0.634420\n",
       "1   MLPClassifier(alpha=0.01, hidden_layer_sizes=(...  0.687127\n",
       "2   MLPClassifier(alpha=0.01, hidden_layer_sizes=(...  0.673934\n",
       "3   MLPClassifier(hidden_layer_sizes=(64,), learni...  0.665196\n",
       "4   MLPClassifier(alpha=0.01, hidden_layer_sizes=(...  0.685060\n",
       "5   MLPClassifier(alpha=0.01, hidden_layer_sizes=(...  0.670716\n",
       "6   MLPClassifier(hidden_layer_sizes=(64,), max_it...  0.671057\n",
       "7   MLPClassifier(hidden_layer_sizes=(64,), max_it...  0.680733\n",
       "8   MLPClassifier(hidden_layer_sizes=(64,), learni...  0.672101\n",
       "9   MLPClassifier(hidden_layer_sizes=(64,), learni...  0.673039\n",
       "10  MLPClassifier(alpha=0.01, hidden_layer_sizes=(...  0.681436\n",
       "11  MLPClassifier(alpha=0.02, hidden_layer_sizes=(...  0.688107\n",
       "12  MLPClassifier(alpha=0.02, hidden_layer_sizes=(...  0.687980\n",
       "13  MLPClassifier(alpha=0.02, hidden_layer_sizes=(...  0.677217\n",
       "14  MLPClassifier(alpha=0.02, hidden_layer_sizes=(...  0.683717\n",
       "15  MLPClassifier(alpha=0.02, hidden_layer_sizes=(...  0.674638\n",
       "16  MLPClassifier(alpha=0.0002, hidden_layer_sizes...  0.656245\n",
       "17  MLPClassifier(alpha=0.003, hidden_layer_sizes=...  0.667711\n",
       "18  MLPClassifier(alpha=0.03, hidden_layer_sizes=(...  0.692626"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And finally**, we predict the unknown label for the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67029, 79), (28645, 79))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTICE we exclude visit number becuase we didn't use it for training.\n",
    "yy = best_clf.predict(XX.drop(columns=['VisitNumber']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we do is generating a file that should be *submitted* on kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(list(zip(XX.VisitNumber, yy)), columns=[\"VisitNumber\", \"TripType\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"./data/submission.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
