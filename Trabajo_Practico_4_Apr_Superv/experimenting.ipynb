{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diplodatos Kaggle Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import CategoricalNB, GaussianNB, MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from utils import display_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(train_data_fname, test_data_fname):\n",
    "    df_train = pd.read_csv(train_data_fname)\n",
    "    df_train['is_train_set'] = 1\n",
    "    df_test = pd.read_csv(test_data_fname)\n",
    "    df_test['is_train_set'] = 0\n",
    "\n",
    "    # we  get the TripType for the train set. To do that, we group by VisitNumber and\n",
    "    # then we get the max (or min or avg)\n",
    "    y = df_train.groupby([\"VisitNumber\", \"Weekday\"], as_index=False).max().TripType\n",
    "\n",
    "    # we remove the TripType now, and concat training and testing data\n",
    "    # the concat is done so that we have the same columns for both datasets\n",
    "    # after one-hot encoding\n",
    "    df_train = df_train.drop(\"TripType\", axis=1)\n",
    "    df = pd.concat([df_train, df_test])\n",
    "    \n",
    "    # split Upc in two and add left part to dataset. Add ranks as well (added by Javi).\n",
    "    df['upc_left'] = df_train.Upc.dropna().astype('int').astype('str').str.zfill(15).str.slice(1, 7)\n",
    "    df['upc_left_rank'] = df.upc_left.rank(method='max', pct=True, na_option='bottom')\n",
    "    df['fineline_rank'] = df.FinelineNumber.rank(method='max', pct=True, na_option='bottom')    \n",
    "    \n",
    "    # the next three operations are the ones we have just presented in the previous lines\n",
    "    \n",
    "    # drop the columns we won't use (it may be good to use them somehow)\n",
    "    df = df.drop(columns=[\"FinelineNumber\", \"Upc\"])\n",
    "\n",
    "    # one-hot encoding for the DepartmentDescription\n",
    "    df = pd.get_dummies(df, columns=[\"DepartmentDescription\"], dummy_na=True)\n",
    "\n",
    "    # now we add the groupby values\n",
    "    agg_dict = {}\n",
    "    for col_name in df.columns:\n",
    "        if col_name.startswith(\"DepartmentDescription\") or col_name in ('ScanCount', 'is_train_set'):\n",
    "            agg_dict[col_name] = 'sum'\n",
    "    agg_dict['upc_left_rank'] = 'mean'\n",
    "    agg_dict['fineline_rank'] = 'mean'\n",
    "    df = df.groupby([\"VisitNumber\", \"Weekday\"], as_index=False).agg(agg_dict)\n",
    "    \n",
    "    # we do one-hot encoding for the Weekday\n",
    "    df = pd.get_dummies(df, columns=[\"Weekday\"], dummy_na=True)\n",
    "\n",
    "    # get train and test back\n",
    "    df_train = df[df.is_train_set != 0]\n",
    "    df_test = df[df.is_train_set == 0]\n",
    "    \n",
    "    X = df_train.drop([\"is_train_set\"], axis=1)\n",
    "    yy = None\n",
    "    XX = df_test.drop([\"is_train_set\"], axis=1)\n",
    "    \n",
    "    return X, y, XX, yy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, XX, yy = transform_data(\"./data/train.csv\", \"./data/test.csv\")\n",
    "\n",
    "# We drop visit number; it's just an index, it messes up all the training.\n",
    "visit_n_test = XX.VisitNumber\n",
    "X = X.drop(columns=['VisitNumber'])\n",
    "XX = XX.drop(columns=['VisitNumber'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ScanCount</th>\n",
       "      <th>DepartmentDescription_1-HR PHOTO</th>\n",
       "      <th>DepartmentDescription_ACCESSORIES</th>\n",
       "      <th>DepartmentDescription_AUTOMOTIVE</th>\n",
       "      <th>DepartmentDescription_BAKERY</th>\n",
       "      <th>DepartmentDescription_BATH AND SHOWER</th>\n",
       "      <th>DepartmentDescription_BEAUTY</th>\n",
       "      <th>DepartmentDescription_BEDDING</th>\n",
       "      <th>DepartmentDescription_BOOKS AND MAGAZINES</th>\n",
       "      <th>DepartmentDescription_BOYS WEAR</th>\n",
       "      <th>...</th>\n",
       "      <th>upc_left_rank</th>\n",
       "      <th>fineline_rank</th>\n",
       "      <th>Weekday_Friday</th>\n",
       "      <th>Weekday_Monday</th>\n",
       "      <th>Weekday_Saturday</th>\n",
       "      <th>Weekday_Sunday</th>\n",
       "      <th>Weekday_Thursday</th>\n",
       "      <th>Weekday_Tuesday</th>\n",
       "      <th>Weekday_Wednesday</th>\n",
       "      <th>Weekday_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81954</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.822490</td>\n",
       "      <td>0.098829</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32578</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.855976</td>\n",
       "      <td>0.005930</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86578</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258283</td>\n",
       "      <td>0.723924</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85079</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811328</td>\n",
       "      <td>0.067863</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9788</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398473</td>\n",
       "      <td>0.497582</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52950</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084868</td>\n",
       "      <td>0.229788</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8809</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.916814</td>\n",
       "      <td>0.047560</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78302</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498850</td>\n",
       "      <td>0.250545</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750218</td>\n",
       "      <td>0.186140</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22577</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474745</td>\n",
       "      <td>0.392573</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46920 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ScanCount  DepartmentDescription_1-HR PHOTO  \\\n",
       "81954          2                                 0   \n",
       "32578          1                                 0   \n",
       "86578          5                                 0   \n",
       "85079          7                                 0   \n",
       "9788          32                                 0   \n",
       "...          ...                               ...   \n",
       "52950          1                                 0   \n",
       "8809           1                                 0   \n",
       "78302          5                                 0   \n",
       "1229           8                                 0   \n",
       "22577         19                                 0   \n",
       "\n",
       "       DepartmentDescription_ACCESSORIES  DepartmentDescription_AUTOMOTIVE  \\\n",
       "81954                                  0                                 0   \n",
       "32578                                  0                                 0   \n",
       "86578                                  0                                 0   \n",
       "85079                                  0                                 0   \n",
       "9788                                   0                                 0   \n",
       "...                                  ...                               ...   \n",
       "52950                                  0                                 0   \n",
       "8809                                   0                                 0   \n",
       "78302                                  0                                 0   \n",
       "1229                                   0                                 0   \n",
       "22577                                  0                                 0   \n",
       "\n",
       "       DepartmentDescription_BAKERY  DepartmentDescription_BATH AND SHOWER  \\\n",
       "81954                             0                                      0   \n",
       "32578                             0                                      0   \n",
       "86578                             0                                      0   \n",
       "85079                             0                                      0   \n",
       "9788                              0                                      0   \n",
       "...                             ...                                    ...   \n",
       "52950                             0                                      0   \n",
       "8809                              0                                      0   \n",
       "78302                             0                                      0   \n",
       "1229                              0                                      0   \n",
       "22577                             0                                      0   \n",
       "\n",
       "       DepartmentDescription_BEAUTY  DepartmentDescription_BEDDING  \\\n",
       "81954                             0                              0   \n",
       "32578                             0                              0   \n",
       "86578                             0                              0   \n",
       "85079                             0                              0   \n",
       "9788                              0                              0   \n",
       "...                             ...                            ...   \n",
       "52950                             0                              0   \n",
       "8809                              0                              0   \n",
       "78302                             0                              0   \n",
       "1229                              0                              0   \n",
       "22577                             0                              0   \n",
       "\n",
       "       DepartmentDescription_BOOKS AND MAGAZINES  \\\n",
       "81954                                          0   \n",
       "32578                                          0   \n",
       "86578                                          0   \n",
       "85079                                          0   \n",
       "9788                                           0   \n",
       "...                                          ...   \n",
       "52950                                          0   \n",
       "8809                                           0   \n",
       "78302                                          0   \n",
       "1229                                           0   \n",
       "22577                                          0   \n",
       "\n",
       "       DepartmentDescription_BOYS WEAR  ...  upc_left_rank  fineline_rank  \\\n",
       "81954                                0  ...       0.822490       0.098829   \n",
       "32578                                0  ...       0.855976       0.005930   \n",
       "86578                                0  ...       0.258283       0.723924   \n",
       "85079                                0  ...       0.811328       0.067863   \n",
       "9788                                 0  ...       0.398473       0.497582   \n",
       "...                                ...  ...            ...            ...   \n",
       "52950                                0  ...       0.084868       0.229788   \n",
       "8809                                 0  ...       0.916814       0.047560   \n",
       "78302                                0  ...       0.498850       0.250545   \n",
       "1229                                 0  ...       0.750218       0.186140   \n",
       "22577                                0  ...       0.474745       0.392573   \n",
       "\n",
       "       Weekday_Friday  Weekday_Monday  Weekday_Saturday  Weekday_Sunday  \\\n",
       "81954               0               0                 0               0   \n",
       "32578               0               1                 0               0   \n",
       "86578               1               0                 0               0   \n",
       "85079               0               0                 0               0   \n",
       "9788                0               0                 0               1   \n",
       "...               ...             ...               ...             ...   \n",
       "52950               0               0                 0               1   \n",
       "8809                0               0                 0               1   \n",
       "78302               0               0                 0               0   \n",
       "1229                1               0                 0               0   \n",
       "22577               1               0                 0               0   \n",
       "\n",
       "       Weekday_Thursday  Weekday_Tuesday  Weekday_Wednesday  Weekday_nan  \n",
       "81954                 0                0                  1            0  \n",
       "32578                 0                0                  0            0  \n",
       "86578                 0                0                  0            0  \n",
       "85079                 1                0                  0            0  \n",
       "9788                  0                0                  0            0  \n",
       "...                 ...              ...                ...          ...  \n",
       "52950                 0                0                  0            0  \n",
       "8809                  0                0                  0            0  \n",
       "78302                 0                1                  0            0  \n",
       "1229                  0                0                  0            0  \n",
       "22577                 0                0                  0            0  \n",
       "\n",
       "[46920 rows x 80 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split training dataset into train and \"validation\" \n",
    "# (we won't be using validation set in this example, because of the cross-validation;\n",
    "# but it could be useful for you depending on your approach)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results dataframe is used to store the computed results\n",
    "results = pd.DataFrame(columns=('clf', 'best_acc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.25931040\n",
      "Iteration 2, loss = 1.27441125\n",
      "Iteration 3, loss = 1.12383377\n",
      "Iteration 4, loss = 1.06558437\n",
      "Iteration 5, loss = 1.03322479\n",
      "Iteration 6, loss = 1.01183024\n",
      "Iteration 7, loss = 0.99413408\n",
      "Iteration 8, loss = 0.97986490\n",
      "Iteration 9, loss = 0.96807404\n",
      "Iteration 10, loss = 0.95752456\n",
      "Iteration 11, loss = 0.94764898\n",
      "Iteration 12, loss = 0.94011437\n",
      "Iteration 13, loss = 0.93230650\n",
      "Iteration 14, loss = 0.92563558\n",
      "Iteration 15, loss = 0.91966303\n",
      "Iteration 16, loss = 0.91416081\n",
      "Iteration 17, loss = 0.91048629\n",
      "Iteration 18, loss = 0.90445022\n",
      "Iteration 19, loss = 0.90096387\n",
      "Iteration 20, loss = 0.89698443\n",
      "Iteration 21, loss = 0.89335974\n",
      "Iteration 22, loss = 0.88986402\n",
      "Iteration 23, loss = 0.88819839\n",
      "Iteration 24, loss = 0.88479681\n",
      "Iteration 25, loss = 0.88195417\n",
      "Iteration 26, loss = 0.87909271\n",
      "Iteration 27, loss = 0.87723010\n",
      "Iteration 28, loss = 0.87473939\n",
      "Iteration 29, loss = 0.87248053\n",
      "Iteration 30, loss = 0.86944825\n",
      "Iteration 31, loss = 0.86857281\n",
      "Iteration 32, loss = 0.86576652\n",
      "Iteration 33, loss = 0.86493154\n",
      "Iteration 34, loss = 0.86162097\n",
      "Iteration 35, loss = 0.86066500\n",
      "Iteration 36, loss = 0.85948973\n",
      "Iteration 37, loss = 0.85705068\n",
      "Iteration 38, loss = 0.85631810\n",
      "Iteration 39, loss = 0.85379566\n",
      "Iteration 40, loss = 0.85442961\n",
      "Iteration 41, loss = 0.85243592\n",
      "Iteration 42, loss = 0.85045599\n",
      "Iteration 43, loss = 0.84957883\n",
      "Iteration 44, loss = 0.84804892\n",
      "Iteration 45, loss = 0.84590050\n",
      "Iteration 46, loss = 0.84588576\n",
      "Iteration 47, loss = 0.84586837\n",
      "Iteration 48, loss = 0.84380243\n",
      "Iteration 49, loss = 0.84328688\n",
      "Iteration 50, loss = 0.84163146\n",
      "Iteration 51, loss = 0.84187689\n",
      "Iteration 52, loss = 0.83974210\n",
      "Iteration 53, loss = 0.83829153\n",
      "Iteration 54, loss = 0.83884781\n",
      "Iteration 55, loss = 0.83777359\n",
      "Iteration 56, loss = 0.83658726\n",
      "Iteration 57, loss = 0.83591407\n",
      "Iteration 58, loss = 0.83553764\n",
      "Iteration 59, loss = 0.83363238\n",
      "Iteration 60, loss = 0.83416470\n",
      "Iteration 61, loss = 0.83334381\n",
      "Iteration 62, loss = 0.83182353\n",
      "Iteration 63, loss = 0.83185820\n",
      "Iteration 64, loss = 0.83096094\n",
      "Iteration 65, loss = 0.83037517\n",
      "Iteration 66, loss = 0.82971142\n",
      "Iteration 67, loss = 0.82949379\n",
      "Iteration 68, loss = 0.82803654\n",
      "Iteration 69, loss = 0.82810452\n",
      "Iteration 70, loss = 0.82594302\n",
      "Iteration 71, loss = 0.82609468\n",
      "Iteration 72, loss = 0.82535860\n",
      "Iteration 73, loss = 0.82649044\n",
      "Iteration 74, loss = 0.82475926\n",
      "Iteration 75, loss = 0.82451010\n",
      "Iteration 76, loss = 0.82392385\n",
      "Iteration 77, loss = 0.82329013\n",
      "Iteration 78, loss = 0.82314276\n",
      "Iteration 79, loss = 0.82259493\n",
      "Iteration 80, loss = 0.82273440\n",
      "Iteration 81, loss = 0.82165010\n",
      "Iteration 82, loss = 0.82027482\n",
      "Iteration 83, loss = 0.82098802\n",
      "Iteration 84, loss = 0.82010337\n",
      "Iteration 85, loss = 0.81949606\n",
      "Iteration 86, loss = 0.81888890\n",
      "Iteration 87, loss = 0.81857246\n",
      "Iteration 88, loss = 0.81843974\n",
      "Iteration 89, loss = 0.81757735\n",
      "Iteration 90, loss = 0.81801586\n",
      "Iteration 91, loss = 0.81788066\n",
      "Iteration 92, loss = 0.81656581\n",
      "Iteration 93, loss = 0.81541576\n",
      "Iteration 94, loss = 0.81560357\n",
      "Iteration 95, loss = 0.81594160\n",
      "Iteration 96, loss = 0.81513912\n",
      "Iteration 97, loss = 0.81406892\n",
      "Iteration 98, loss = 0.81433213\n",
      "Iteration 99, loss = 0.81438520\n",
      "Iteration 100, loss = 0.81404579\n",
      "Iteration 101, loss = 0.81317855\n",
      "Iteration 102, loss = 0.81216820\n",
      "Iteration 103, loss = 0.81327370\n",
      "Iteration 104, loss = 0.81153534\n",
      "Iteration 105, loss = 0.81256473\n",
      "Iteration 106, loss = 0.81062940\n",
      "Iteration 107, loss = 0.81208761\n",
      "Iteration 108, loss = 0.81002579\n",
      "Iteration 109, loss = 0.81009382\n",
      "Iteration 110, loss = 0.81032746\n",
      "Iteration 111, loss = 0.80958305\n",
      "Iteration 112, loss = 0.80871047\n",
      "Iteration 113, loss = 0.80921499\n",
      "Iteration 114, loss = 0.80877954\n",
      "Iteration 115, loss = 0.80775307\n",
      "Iteration 116, loss = 0.80860473\n",
      "Iteration 117, loss = 0.80781742\n",
      "Iteration 118, loss = 0.80741614\n",
      "Iteration 119, loss = 0.80756770\n",
      "Iteration 120, loss = 0.80698653\n",
      "Iteration 121, loss = 0.80605718\n",
      "Iteration 122, loss = 0.80569564\n",
      "Iteration 123, loss = 0.80528858\n",
      "Iteration 124, loss = 0.80574065\n",
      "Iteration 125, loss = 0.80595452\n",
      "Iteration 126, loss = 0.80633018\n",
      "Iteration 127, loss = 0.80488974\n",
      "Iteration 128, loss = 0.80402205\n",
      "Iteration 129, loss = 0.80385126\n",
      "Iteration 130, loss = 0.80426342\n",
      "Iteration 131, loss = 0.80275904\n",
      "Iteration 132, loss = 0.80301240\n",
      "Iteration 133, loss = 0.80302491\n",
      "Iteration 134, loss = 0.80328697\n",
      "Iteration 135, loss = 0.80281223\n",
      "Iteration 136, loss = 0.80229501\n",
      "Iteration 137, loss = 0.80201377\n",
      "Iteration 138, loss = 0.80233932\n",
      "Iteration 139, loss = 0.80218821\n",
      "Iteration 140, loss = 0.80217029\n",
      "Iteration 141, loss = 0.80148789\n",
      "Iteration 142, loss = 0.80074431\n",
      "Iteration 143, loss = 0.80107136\n",
      "Iteration 144, loss = 0.80082983\n",
      "Iteration 145, loss = 0.80077130\n",
      "Iteration 146, loss = 0.79989874\n",
      "Iteration 147, loss = 0.80067799\n",
      "Iteration 148, loss = 0.79975169\n",
      "Iteration 149, loss = 0.79935648\n",
      "Iteration 150, loss = 0.80003020\n",
      "Iteration 151, loss = 0.80017325\n",
      "Iteration 152, loss = 0.79888500\n",
      "Iteration 153, loss = 0.79833465\n",
      "Iteration 154, loss = 0.79851545\n",
      "Iteration 155, loss = 0.79812847\n",
      "Iteration 156, loss = 0.79813385\n",
      "Iteration 157, loss = 0.79712885\n",
      "Iteration 158, loss = 0.79762870\n",
      "Iteration 159, loss = 0.79783199\n",
      "Iteration 160, loss = 0.79700464\n",
      "Iteration 161, loss = 0.79749331\n",
      "Iteration 162, loss = 0.79783533\n",
      "Iteration 163, loss = 0.79623334\n",
      "Iteration 164, loss = 0.79702299\n",
      "Iteration 165, loss = 0.79631954\n",
      "Iteration 166, loss = 0.79594230\n",
      "Iteration 167, loss = 0.79641121\n",
      "Iteration 168, loss = 0.79554073\n",
      "Iteration 169, loss = 0.79501271\n",
      "Iteration 170, loss = 0.79554522\n",
      "Iteration 171, loss = 0.79542624\n",
      "Iteration 172, loss = 0.79506955\n",
      "Iteration 173, loss = 0.79550609\n",
      "Iteration 174, loss = 0.79426871\n",
      "Iteration 175, loss = 0.79505621\n",
      "Iteration 176, loss = 0.79505339\n",
      "Iteration 177, loss = 0.79398238\n",
      "Iteration 178, loss = 0.79411026\n",
      "Iteration 179, loss = 0.79259440\n",
      "Iteration 180, loss = 0.79361726\n",
      "Iteration 181, loss = 0.79258507\n",
      "Iteration 182, loss = 0.79445136\n",
      "Iteration 183, loss = 0.79306472\n",
      "Iteration 184, loss = 0.79337698\n",
      "Iteration 185, loss = 0.79251851\n",
      "Iteration 186, loss = 0.79277136\n",
      "Iteration 187, loss = 0.79222715\n",
      "Iteration 188, loss = 0.79186901\n",
      "Iteration 189, loss = 0.79194741\n",
      "Iteration 190, loss = 0.79209456\n",
      "Iteration 191, loss = 0.79107073\n",
      "Iteration 192, loss = 0.79194840\n",
      "Iteration 193, loss = 0.79006956\n",
      "Iteration 194, loss = 0.79141399\n",
      "Iteration 195, loss = 0.79109791\n",
      "Iteration 196, loss = 0.79124339\n",
      "Iteration 197, loss = 0.79138886\n",
      "Iteration 198, loss = 0.79077525\n",
      "Iteration 199, loss = 0.79132028\n",
      "Iteration 200, loss = 0.78994550\n",
      "Iteration 201, loss = 0.79027468\n",
      "Iteration 202, loss = 0.79001989\n",
      "Iteration 203, loss = 0.78960153\n",
      "Iteration 204, loss = 0.78958838\n",
      "Iteration 205, loss = 0.78980214\n",
      "Iteration 206, loss = 0.78905847\n",
      "Iteration 207, loss = 0.78961752\n",
      "Iteration 208, loss = 0.78963528\n",
      "Iteration 209, loss = 0.78887439\n",
      "Iteration 210, loss = 0.78955557\n",
      "Iteration 211, loss = 0.78912125\n",
      "Iteration 212, loss = 0.78805124\n",
      "Iteration 213, loss = 0.78847828\n",
      "Iteration 214, loss = 0.78911359\n",
      "Iteration 215, loss = 0.78851623\n",
      "Iteration 216, loss = 0.78905519\n",
      "Iteration 217, loss = 0.78911778\n",
      "Iteration 218, loss = 0.78810464\n",
      "Iteration 219, loss = 0.78789057\n",
      "Iteration 220, loss = 0.78783247\n",
      "Iteration 221, loss = 0.78680795\n",
      "Iteration 222, loss = 0.78755459\n",
      "Iteration 223, loss = 0.78860231\n",
      "Iteration 224, loss = 0.78722939\n",
      "Iteration 225, loss = 0.78791255\n",
      "Iteration 226, loss = 0.78733304\n",
      "Iteration 227, loss = 0.78681458\n",
      "Iteration 228, loss = 0.78645168\n",
      "Iteration 229, loss = 0.78674839\n",
      "Iteration 230, loss = 0.78668474\n",
      "Iteration 231, loss = 0.78639910\n",
      "Iteration 232, loss = 0.78696834\n",
      "Iteration 233, loss = 0.78578657\n",
      "Iteration 234, loss = 0.78658437\n",
      "Iteration 235, loss = 0.78548823\n",
      "Iteration 236, loss = 0.78518047\n",
      "Iteration 237, loss = 0.78589202\n",
      "Iteration 238, loss = 0.78544472\n",
      "Iteration 239, loss = 0.78538360\n",
      "Iteration 240, loss = 0.78510089\n",
      "Iteration 241, loss = 0.78557257\n",
      "Iteration 242, loss = 0.78407015\n",
      "Iteration 243, loss = 0.78505321\n",
      "Iteration 244, loss = 0.78495488\n",
      "Iteration 245, loss = 0.78462143\n",
      "Iteration 246, loss = 0.78447675\n",
      "Iteration 247, loss = 0.78548833\n",
      "Iteration 248, loss = 0.78437639\n",
      "Iteration 249, loss = 0.78624837\n",
      "Iteration 250, loss = 0.78466090\n",
      "Iteration 251, loss = 0.78340599\n",
      "Iteration 252, loss = 0.78461345\n",
      "Iteration 253, loss = 0.78507753\n",
      "Iteration 254, loss = 0.78454800\n",
      "Iteration 255, loss = 0.78296288\n",
      "Iteration 256, loss = 0.78425614\n",
      "Iteration 257, loss = 0.78470855\n",
      "Iteration 258, loss = 0.78455733\n",
      "Iteration 259, loss = 0.78235808\n",
      "Iteration 260, loss = 0.78374069\n",
      "Iteration 261, loss = 0.78374633\n",
      "Iteration 262, loss = 0.78443197\n",
      "Iteration 263, loss = 0.78328825\n",
      "Iteration 264, loss = 0.78322262\n",
      "Iteration 265, loss = 0.78379111\n",
      "Iteration 266, loss = 0.78302272\n",
      "Iteration 267, loss = 0.78238319\n",
      "Iteration 268, loss = 0.78193923\n",
      "Iteration 269, loss = 0.78301281\n",
      "Iteration 270, loss = 0.78398390\n",
      "Iteration 271, loss = 0.78305907\n",
      "Iteration 272, loss = 0.78228422\n",
      "Iteration 273, loss = 0.78269393\n",
      "Iteration 274, loss = 0.78229345\n",
      "Iteration 275, loss = 0.78220630\n",
      "Iteration 276, loss = 0.78251277\n",
      "Iteration 277, loss = 0.78230221\n",
      "Iteration 278, loss = 0.78215967\n",
      "Iteration 279, loss = 0.78176678\n",
      "Iteration 280, loss = 0.78224916\n",
      "Iteration 281, loss = 0.78181440\n",
      "Iteration 282, loss = 0.78068692\n",
      "Iteration 283, loss = 0.78131629\n",
      "Iteration 284, loss = 0.78037209\n",
      "Iteration 285, loss = 0.78090313\n",
      "Iteration 286, loss = 0.78099428\n",
      "Iteration 287, loss = 0.78119644\n",
      "Iteration 288, loss = 0.78139234\n",
      "Iteration 289, loss = 0.78154098\n",
      "Iteration 290, loss = 0.78099662\n",
      "Iteration 291, loss = 0.78007713\n",
      "Iteration 292, loss = 0.78057381\n",
      "Iteration 293, loss = 0.78117315\n",
      "Iteration 294, loss = 0.77932306\n",
      "Iteration 295, loss = 0.77957620\n",
      "Iteration 296, loss = 0.78069067\n",
      "Iteration 297, loss = 0.78043049\n",
      "Iteration 298, loss = 0.78107781\n",
      "Iteration 299, loss = 0.77970018\n",
      "Iteration 300, loss = 0.77949119\n",
      "Iteration 301, loss = 0.77983261\n",
      "Iteration 302, loss = 0.77965189\n",
      "Iteration 303, loss = 0.77988552\n",
      "Iteration 304, loss = 0.78099468\n",
      "Iteration 305, loss = 0.78026257\n",
      "Iteration 306, loss = 0.77868165\n",
      "Iteration 307, loss = 0.77931266\n",
      "Iteration 308, loss = 0.77870913\n",
      "Iteration 309, loss = 0.78009716\n",
      "Iteration 310, loss = 0.77895057\n",
      "Iteration 311, loss = 0.78001053\n",
      "Iteration 312, loss = 0.77914287\n",
      "Iteration 313, loss = 0.77866168\n",
      "Iteration 314, loss = 0.77873443\n",
      "Iteration 315, loss = 0.77850613\n",
      "Iteration 316, loss = 0.77801349\n",
      "Iteration 317, loss = 0.77843426\n",
      "Iteration 318, loss = 0.77857140\n",
      "Iteration 319, loss = 0.77767230\n",
      "Iteration 320, loss = 0.77911431\n",
      "Iteration 321, loss = 0.77813436\n",
      "Iteration 322, loss = 0.77883844\n",
      "Iteration 323, loss = 0.77884288\n",
      "Iteration 324, loss = 0.77867577\n",
      "Iteration 325, loss = 0.77839025\n",
      "Iteration 326, loss = 0.77815115\n",
      "Iteration 327, loss = 0.77905254\n",
      "Iteration 328, loss = 0.77793387\n",
      "Iteration 329, loss = 0.77781120\n",
      "Iteration 330, loss = 0.77830364\n",
      "Iteration 331, loss = 0.77805166\n",
      "Iteration 332, loss = 0.77715412\n",
      "Iteration 333, loss = 0.77768242\n",
      "Iteration 334, loss = 0.77722761\n",
      "Iteration 335, loss = 0.77768910\n",
      "Iteration 336, loss = 0.77668294\n",
      "Iteration 337, loss = 0.77756294\n",
      "Iteration 338, loss = 0.77785336\n",
      "Iteration 339, loss = 0.77748178\n",
      "Iteration 340, loss = 0.77594776\n",
      "Iteration 341, loss = 0.77673370\n",
      "Iteration 342, loss = 0.77669988\n",
      "Iteration 343, loss = 0.77703882\n",
      "Iteration 344, loss = 0.77640429\n",
      "Iteration 345, loss = 0.77711594\n",
      "Iteration 346, loss = 0.77677205\n",
      "Iteration 347, loss = 0.77632650\n",
      "Iteration 348, loss = 0.77675763\n",
      "Iteration 349, loss = 0.77585264\n",
      "Iteration 350, loss = 0.77718384\n",
      "Iteration 351, loss = 0.77661798\n",
      "Iteration 352, loss = 0.77591129\n",
      "Iteration 353, loss = 0.77540875\n",
      "Iteration 354, loss = 0.77698539\n",
      "Iteration 355, loss = 0.77612712\n",
      "Iteration 356, loss = 0.77577068\n",
      "Iteration 357, loss = 0.77609084\n",
      "Iteration 358, loss = 0.77611328\n",
      "Iteration 359, loss = 0.77548451\n",
      "Iteration 360, loss = 0.77590774\n",
      "Iteration 361, loss = 0.77584904\n",
      "Iteration 362, loss = 0.77599799\n",
      "Iteration 363, loss = 0.77590492\n",
      "Iteration 364, loss = 0.77527512\n",
      "Iteration 365, loss = 0.77639917\n",
      "Iteration 366, loss = 0.77525464\n",
      "Iteration 367, loss = 0.77554720\n",
      "Iteration 368, loss = 0.77566293\n",
      "Iteration 369, loss = 0.77526793\n",
      "Iteration 370, loss = 0.77539944\n",
      "Iteration 371, loss = 0.77544450\n",
      "Iteration 372, loss = 0.77656411\n",
      "Iteration 373, loss = 0.77483771\n",
      "Iteration 374, loss = 0.77499301\n",
      "Iteration 375, loss = 0.77419895\n",
      "Iteration 376, loss = 0.77336975\n",
      "Iteration 377, loss = 0.77475803\n",
      "Iteration 378, loss = 0.77505933\n",
      "Iteration 379, loss = 0.77406276\n",
      "Iteration 380, loss = 0.77390618\n",
      "Iteration 381, loss = 0.77476259\n",
      "Iteration 382, loss = 0.77482335\n",
      "Iteration 383, loss = 0.77490374\n",
      "Iteration 384, loss = 0.77363272\n",
      "Iteration 385, loss = 0.77460602\n",
      "Iteration 386, loss = 0.77452765\n",
      "Iteration 387, loss = 0.77353370\n",
      "Iteration 388, loss = 0.77499091\n",
      "Iteration 389, loss = 0.77521797\n",
      "Iteration 390, loss = 0.77493499\n",
      "Iteration 391, loss = 0.77386587\n",
      "Iteration 392, loss = 0.77402968\n",
      "Iteration 393, loss = 0.77374901\n",
      "Iteration 394, loss = 0.77422900\n",
      "Iteration 395, loss = 0.77501520\n",
      "Iteration 396, loss = 0.77468632\n",
      "Iteration 397, loss = 0.77379135\n",
      "Iteration 398, loss = 0.77410238\n",
      "Iteration 399, loss = 0.77326516\n",
      "Iteration 400, loss = 0.77325488\n",
      "Iteration 401, loss = 0.77369273\n",
      "Iteration 402, loss = 0.77391372\n",
      "Iteration 403, loss = 0.77411598\n",
      "Iteration 404, loss = 0.77367770\n",
      "Iteration 405, loss = 0.77410931\n",
      "Iteration 406, loss = 0.77352336\n",
      "Iteration 407, loss = 0.77364477\n",
      "Iteration 408, loss = 0.77255599\n",
      "Iteration 409, loss = 0.77231041\n",
      "Iteration 410, loss = 0.77336846\n",
      "Iteration 411, loss = 0.77303288\n",
      "Iteration 412, loss = 0.77317048\n",
      "Iteration 413, loss = 0.77269095\n",
      "Iteration 414, loss = 0.77267868\n",
      "Iteration 415, loss = 0.77371245\n",
      "Iteration 416, loss = 0.77262789\n",
      "Iteration 417, loss = 0.77203577\n",
      "Iteration 418, loss = 0.77351982\n",
      "Iteration 419, loss = 0.77241800\n",
      "Iteration 420, loss = 0.77185355\n",
      "Iteration 421, loss = 0.77240807\n",
      "Iteration 422, loss = 0.77200108\n",
      "Iteration 423, loss = 0.77164894\n",
      "Iteration 424, loss = 0.77288130\n",
      "Iteration 425, loss = 0.77183442\n",
      "Iteration 426, loss = 0.77286750\n",
      "Iteration 427, loss = 0.77179464\n",
      "Iteration 428, loss = 0.77197258\n",
      "Iteration 429, loss = 0.77143634\n",
      "Iteration 430, loss = 0.77126585\n",
      "Iteration 431, loss = 0.77284823\n",
      "Iteration 432, loss = 0.77173714\n",
      "Iteration 433, loss = 0.77132719\n",
      "Iteration 434, loss = 0.77194817\n",
      "Iteration 435, loss = 0.77228030\n",
      "Iteration 436, loss = 0.77283290\n",
      "Iteration 437, loss = 0.77112517\n",
      "Iteration 438, loss = 0.77081796\n",
      "Iteration 439, loss = 0.77200408\n",
      "Iteration 440, loss = 0.77128231\n",
      "Iteration 441, loss = 0.77182003\n",
      "Iteration 442, loss = 0.77210009\n",
      "Iteration 443, loss = 0.77073711\n",
      "Iteration 444, loss = 0.77047367\n",
      "Iteration 445, loss = 0.77218787\n",
      "Iteration 446, loss = 0.77125635\n",
      "Iteration 447, loss = 0.77113846\n",
      "Iteration 448, loss = 0.77093777\n",
      "Iteration 449, loss = 0.77161262\n",
      "Iteration 450, loss = 0.77141046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiergallo/miniconda3/envs/diplodatos/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (450) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Best models so far (from better to worse):\n",
    "\n",
    "MLPClassifier(\n",
    "    alpha=0.03,\n",
    "    beta_1=0.8,\n",
    "    beta_2=0.99999,\n",
    "    hidden_layer_sizes=(96,),\n",
    "    max_iter=300,\n",
    "    n_iter_no_change=30,\n",
    "    random_state=42\n",
    ")\n",
    "MLPClassifier(\n",
    "    alpha=0.03,\n",
    "    hidden_layer_sizes=(96,),\n",
    "    max_iter=300,\n",
    "    n_iter_no_change=30,\n",
    "    random_state=42\n",
    ")\n",
    "MLPClassifier(\n",
    "    alpha=0.02,\n",
    "    hidden_layer_sizes=(64,),\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=101,\n",
    "    max_depth=59,\n",
    "    class_weight={999: 0.49}\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "model_params = {\n",
    "    'beta_1': (0.8, 0.9),\n",
    "    'beta_2': (0.99999, 0.999),\n",
    "}\n",
    "model = MLPClassifier(random_state=42,\n",
    "                      verbose=True,\n",
    "                      max_iter=450,\n",
    "                      alpha=0.03,\n",
    "                      n_iter_no_change=30,\n",
    "                      hidden_layer_sizes=(96,))\n",
    "\n",
    "search = GridSearchCV(model,\n",
    "                      model_params,\n",
    "                      cv=3,\n",
    "                      n_jobs=-1,  # Use all processors.\n",
    "                      scoring='accuracy',\n",
    "                      verbose=1)  # scoring='balanced_accuracy'\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "best_found_clf = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(best_found_clf.loss_curve_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Best classifier found**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(alpha=0.03, hidden_layer_sizes=(96,), max_iter=450,\n",
      "              n_iter_no_change=30, random_state=42, verbose=True)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Accuracy**: 0.6960358056265985"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown('**Best classifier found**:')\n",
    "print(best_found_clf)\n",
    "display_markdown('**Accuracy**: {}'.format(search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.append(\n",
    "    {'clf': best_found_clf, 'best_acc': search.best_score_},\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clf</th>\n",
       "      <th>best_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLPClassifier(alpha=0.03, beta_1=0.8, beta_2=0...</td>\n",
       "      <td>0.699339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLPClassifier(alpha=0.03, beta_1=0.8, beta_2=0...</td>\n",
       "      <td>0.699339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLPClassifier(alpha=0.03, hidden_layer_sizes=(...</td>\n",
       "      <td>0.700810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLPClassifier(alpha=0.03, hidden_layer_sizes=(...</td>\n",
       "      <td>0.696036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 clf  best_acc\n",
       "0  MLPClassifier(alpha=0.03, beta_1=0.8, beta_2=0...  0.699339\n",
       "1  MLPClassifier(alpha=0.03, beta_1=0.8, beta_2=0...  0.699339\n",
       "2  MLPClassifier(alpha=0.03, hidden_layer_sizes=(...  0.700810\n",
       "3  MLPClassifier(alpha=0.03, hidden_layer_sizes=(...  0.696036"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**The best classifier so far is:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(alpha=0.03, hidden_layer_sizes=(96,), max_iter=450,\n",
      "              n_iter_no_change=30, random_state=42, verbose=True)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Best accuracy from cross validation**: 0.6960358056265985"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Accuracy over validation set**: 0.7095330449052663"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_clf_idx = results['best_acc'].idxmax()\n",
    "best_clf = results.loc[best_clf_idx]['clf']\n",
    "best_acc = results.loc[best_clf_idx]['best_acc']\n",
    "display_markdown('**The best classifier so far is:**')\n",
    "print(best_clf)\n",
    "display_markdown('**Best accuracy from cross validation**: {}'.format(search.best_score_))\n",
    "validation_accuracy = accuracy_score(y_valid, best_clf.predict(X_valid))\n",
    "display_markdown('**Accuracy over validation set**: {}'.format(validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And finally**, we predict the unknown label for the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67029, 80), (28645, 80))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7095330449052663\n"
     ]
    }
   ],
   "source": [
    "yy = best_clf.predict(XX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we do is generating a file that should be *submitted* on kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(list(zip(visit_n_test, yy)), columns=[\"VisitNumber\", \"TripType\"])\n",
    "submission.to_csv('./data/submission.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
